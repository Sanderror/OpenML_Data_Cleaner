{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f99b57a",
   "metadata": {},
   "source": [
    "# Notebook for the dataset quality improvement experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8896a",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "### Set up\n",
    "* [Imports](#imports)\n",
    "* [Load in data](#load_data)\n",
    "* [Initializations](#initializations)\n",
    "### Errors\n",
    "* [Missing values](#mvs)\n",
    "* [Duplicate rows](#duplicate_rows)\n",
    "* [Duplicate attributes](#duplicate_cols)\n",
    "* [Outlier values](#outlier_vals)\n",
    "* [Outlier instances](#outlier_rows)\n",
    "* [Cryptic attribute names](#cryptic)\n",
    "* [Single value attributes](#svs)\n",
    "* [Mixed data types](#mixed)\n",
    "* [Mislabeled labels](#mislabels)\n",
    "* [String mismatches](#mismatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09008aa8",
   "metadata": {},
   "source": [
    "## All imports necessary to run the notebook <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe0f240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "#All imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import wordninja\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import copy\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pFAHES.common as common\n",
    "import pFAHES.patterns as patterns\n",
    "import pFAHES.DV_Detector as DV_Detector\n",
    "import pFAHES.RandDMVD as RandDMVD\n",
    "import pFAHES.OD as OD\n",
    "from statistics import mean, median, mode\n",
    "import copy\n",
    "from sortinghatinf import get_sortinghat_types\n",
    "from numpy import percentile\n",
    "import tiktoken\n",
    "from deepchecks.tabular.checks import MixedDataTypes\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import StringMismatch\n",
    "from deepchecks.tabular.checks import SpecialCharacters\n",
    "from deepchecks.tabular.checks import OutlierSampleDetection\n",
    "from deepchecks.tabular.checks import MixedNulls\n",
    "import math\n",
    "from PyNomaly import loop\n",
    "import cleanlab\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import openml\n",
    "import math\n",
    "from sklearn.metrics import (f1_score, accuracy_score, mean_squared_error, mean_absolute_error,\n",
    "                             roc_auc_score, precision_score, recall_score, r2_score)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcbfcc6",
   "metadata": {},
   "source": [
    "## Load in the synthetic test data <a class=\"anchor\" id=\"load_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ddc596b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "      <th>high_label_correlation</th>\n",
       "      <th>redundant SV column</th>\n",
       "      <th>duplicate column</th>\n",
       "      <th>mixed_data_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>121321</td>\n",
       "      <td>?</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>average salary</td>\n",
       "      <td>this_is_the_same</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>Private</td>\n",
       "      <td>73023</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Male</td>\n",
       "      <td>$$$$$</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>average salary</td>\n",
       "      <td>this_is_the_same</td>\n",
       "      <td>36</td>\n",
       "      <td>MD2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>Private</td>\n",
       "      <td>194686</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>average salary</td>\n",
       "      <td>this_is_the_same</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>Private</td>\n",
       "      <td>72887</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>average salary</td>\n",
       "      <td>this_is_the_same</td>\n",
       "      <td>24</td>\n",
       "      <td>MD2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>VARiation!</td>\n",
       "      <td>282192</td>\n",
       "      <td>?</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>CHANGED VALUE</td>\n",
       "      <td>this_is_the_same</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   workclass  fnlwgt     education gender native-country income  \\\n",
       "0   31     Private  121321             ?   Male  United-States  <=50K   \n",
       "1   36     Private   73023       HS-grad   Male          $$$$$  <=50K   \n",
       "2   20     Private  194686  Some-college   Male  United-States  <=50K   \n",
       "3   24     Private   72887       HS-grad   Male  United-States  <=50K   \n",
       "4   44  VARiation!  282192             ?   Male  United-States  <=50K   \n",
       "\n",
       "  high_label_correlation redundant SV column  duplicate column  \\\n",
       "0         average salary    this_is_the_same                31   \n",
       "1         average salary    this_is_the_same                36   \n",
       "2         average salary    this_is_the_same                20   \n",
       "3         average salary    this_is_the_same                24   \n",
       "4          CHANGED VALUE    this_is_the_same                44   \n",
       "\n",
       "  mixed_data_types  \n",
       "0                3  \n",
       "1              MD2  \n",
       "2                2  \n",
       "3              MD2  \n",
       "4                3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Synthetic test data\n",
    "df_synthetic = pd.read_excel(\"data/synthetic_data.xlsx\")\n",
    "df_synthetic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1aa66b",
   "metadata": {},
   "source": [
    "## Initializations <a class=\"anchor\" id=\"initializations\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "330fbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = get_sortinghat_types(df_synthetic)\n",
    "feature_types_dct = {col:feat for col, feat in zip(df_synthetic.columns, feature_types)}\n",
    "token_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "target = \"income\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ddb96e",
   "metadata": {},
   "source": [
    "## Missing values <a class=\"anchor\" id=\"mvs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad13ec",
   "metadata": {},
   "source": [
    "### Classes and help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76771dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_to_label(df, encode_nans=True):\n",
    "    \"\"\" Converts categorical columns in a dataframe to labels (numerical)\n",
    "    input:\n",
    "        df: Pandas DataFrame in which the categorical features will be converted to labels\n",
    "        encode_nans: True, if the NaNs should be encoded as a label as well (needed for most\n",
    "        imputation techniques because do not work if there are NaNs in the non-target columns). False, \n",
    "        if the NaNs should not be encoded as a label (e.g. for KNN imputation)\n",
    "    output:\n",
    "        df_copy: Copy of the original df, but now containing labels for the categorical columns\n",
    "        dct: A dictionary containing the column names (of categorical columns) as keys and the \n",
    "        LabelEncoder used for that column as value. This is needed to convert the labels back to \n",
    "        the original categorical values\n",
    "    \"\"\"\n",
    "    dct = dict()\n",
    "    df_copy = copy.deepcopy(df)\n",
    "    if encode_nans:\n",
    "        for col in df_copy.columns:\n",
    "            if df_copy[col].dtype == 'object' or df_copy[col].dtype == 'category' or df_copy[col].dtype == 'bool':\n",
    "                df_copy[col] = df_copy[col].astype(str)\n",
    "                encoder = LabelEncoder()\n",
    "                df_copy[col] = encoder.fit_transform(df_copy[col])\n",
    "                dct[col] = encoder\n",
    "    else:\n",
    "        for col in df_copy.columns:\n",
    "            if df_copy[col].dtype == 'object':\n",
    "                nan_mask = df_copy[col].isnull()\n",
    "                nan_rows = df_copy[col][nan_mask]\n",
    "                cat_rows = df_copy[col][~nan_mask].astype(str)\n",
    "                encoder = LabelEncoder()\n",
    "                cat_as_label_rows = encoder.fit_transform(cat_rows)\n",
    "                df_copy[col][~nan_mask] = cat_as_label_rows\n",
    "                dct[col] = encoder\n",
    "    return df_copy, dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3fac9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_imputation(df, col, method):\n",
    "    \"\"\" Imputation of a column with a Machine Learning (ML) technique\n",
    "    input:\n",
    "        df: Pandas DataFrame containing a column on which ML-imputation will be applied\n",
    "        col: String of the column name on which the ML-imputation will be applied\n",
    "        method: String of the ML-imputation method that is selected to impute the MVs\n",
    "        in the selected column. Could be either: MLP, CART or RF.\n",
    "    output:\n",
    "        curr_col: Pandas Series of the ML-imputed column\n",
    "    \"\"\"\n",
    "    if df[col].isna().any():\n",
    "        curr_col = df[col]\n",
    "        # Transform categorical string attributes in dataframe to categorical labels\n",
    "        temp_df, encoder = categorical_to_label(df)        \n",
    "\n",
    "        # Mask over the NaN values in the to be imputed column\n",
    "        nan_mask = curr_col.isnull()\n",
    "\n",
    "        # We use all other attributes (X) to predict the missing values of the to be imputed column (y)\n",
    "        X = temp_df.loc[:, temp_df.columns != col]\n",
    "        y = curr_col\n",
    "\n",
    "        # We split the data based on the indices for which y is a NaN\n",
    "        X_test = X[nan_mask]\n",
    "        X_train = X[~nan_mask]\n",
    "        y_test = y[nan_mask]\n",
    "        y_train = y[~nan_mask]\n",
    "\n",
    "        if method == 'MLP':\n",
    "            model = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                         hidden_layer_sizes=(100,), random_state=1)\n",
    "        elif method == 'CART':\n",
    "            model = DecisionTreeClassifier()\n",
    "        elif method == 'RF':\n",
    "            model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "        elif method == 'KNN_ALT':\n",
    "            model = KNeighborsClassifier()\n",
    "        else:\n",
    "            print(f\"Method {method} is not included as a ML-imputation technique\")\n",
    "        model.fit(X_train, y_train)\n",
    "        pred_values = model.predict(X_test)\n",
    "\n",
    "        # Map the indices to the predicted values in order to impute the values into the column\n",
    "        indices = [idx for idx in X_test.index]\n",
    "        mapping = {indices[i]:pred_values[i] for i in range(len(indices))}\n",
    "        imputed_col = copy.deepcopy(curr_col)\n",
    "        for i, j in mapping.items():\n",
    "            imputed_col.loc[i] = j\n",
    "        return imputed_col\n",
    "    else:\n",
    "        return df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dda708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mv_method(df, col, method):\n",
    "    \"\"\" Applies the selected imputation method on the selected column\n",
    "    input:\n",
    "        df: Pandas DataFrame containing a column on which imputation will be applied\n",
    "        col: String of the column name on which the imputation will be applied\n",
    "        method: String of the imputation method that is selected to impute the MVs\n",
    "        in the selected column. Could be either: Mode, MLP, CART, RF, KNN, Delete record\n",
    "        and Do not impute (and Mean and Median too for numerical columns)\n",
    "    output:\n",
    "        curr_col: Pandas Series of the imputed column\n",
    "    \"\"\"\n",
    "    curr_col = df[col]\n",
    "    if method == \"mode\":\n",
    "        col_mode = df[col].value_counts().idxmax()\n",
    "        imputed_col = curr_col.fillna(col_mode)\n",
    "    elif method == 'mean':\n",
    "        imputed_col = curr_col.fillna(mean(curr_col))\n",
    "    elif method == 'median':\n",
    "        imputed_col = curr_col.fillna(median(curr_col))\n",
    "    elif method == 'RF' or method == \"MLP\" or method == 'CART' or method == 'KNN_ALT':\n",
    "        imputed_col = ml_imputation(df, col, method)\n",
    "    elif method == 'Do not impute':\n",
    "        imputed_col = curr_col\n",
    "    elif method == 'Delete record':\n",
    "        mv_records = df[df[col].isna() == True].index\n",
    "        cleaned_df = df.drop(mv_records)\n",
    "        return cleaned_df\n",
    "    else:\n",
    "        print(f\"Method {method} is not known\")\n",
    "        \n",
    "    return imputed_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434b57c",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66388618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_mv(df):\n",
    "    \"\"\" Detects whether there are missing values in a column\n",
    "    input: \n",
    "        series: Pandas Series of a column in a dataframe\n",
    "    output: \n",
    "        boolean: True if missing values in the column, False if not\n",
    "    \"\"\"\n",
    "    boolean = df.isnull().any()\n",
    "    nan_cols = df.columns[boolean].to_list()\n",
    "    mv_list = []\n",
    "    for col in nan_cols:\n",
    "        freq = len(df[df[col].isna()])\n",
    "        mv_list.append([col, 'nan', freq])\n",
    "    return mv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48aa5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dmv(df, tool_id=\"5\"):\n",
    "    \"\"\" Detects disguised missing values (DMVs) in a dataframe\n",
    "    input: \n",
    "        df: Pandas DataFrame that will be checked on DMVs\n",
    "        tool_id: String of 1, 2, 3, 4 or 5, which determines what type of DMVs\n",
    "        will be checked for. Default=5; pattern discovery + numerical outlier detection\n",
    "    output:\n",
    "        output_list: List containing the DMVs detected stored as lists within that list\n",
    "        in the format: Column name, DMV, Frequency (of DMV), Detection tool (that found the DMV)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.astype(str)\n",
    "    sus_dis_values = []\n",
    "\n",
    "    if tool_id == '1':\n",
    "        sus_dis_values, ptrns = patterns.find_all_patterns(df, sus_dis_values)\n",
    "        sus_dis_values = DV_Detector.check_non_conforming_patterns(df, sus_dis_values)\n",
    "    elif tool_id == '2':\n",
    "        sus_dis_values = RandDMVD.find_disguised_values(df, sus_dis_values)\n",
    "    elif tool_id == '3':\n",
    "        sus_dis_values = OD.detect_outliers(df, sus_dis_values)\n",
    "    elif tool_id == '4':\n",
    "        sus_dis_values, ptrns = patterns.find_all_patterns(df, sus_dis_values)\n",
    "        sus_dis_values = DV_Detector.check_non_conforming_patterns(df, sus_dis_values)\n",
    "        sus_dis_values = RandDMVD.find_disguised_values(df, sus_dis_values)\n",
    "        sus_dis_values = OD.detect_outliers(df, sus_dis_values)\n",
    "    elif tool_id == '5':\n",
    "        sus_dis_values, ptrns = patterns.find_all_patterns(df, sus_dis_values)\n",
    "        sus_dis_values = DV_Detector.check_non_conforming_patterns(df, sus_dis_values)\n",
    "        sus_dis_values = OD.detect_outliers(df, sus_dis_values)\n",
    "    else:\n",
    "        print(\"Unkown option ..\",tool_id)\n",
    "    \n",
    "    output_str = \"Attribute Name, Value, Frequency, Detection Tool \\n\"\n",
    "    output_list = []\n",
    "    for sus_dis in sus_dis_values:\n",
    "        output_str = output_str + f\"{sus_dis.attr_name}, {sus_dis.value}, {sus_dis.frequency}, {sus_dis.tool_name}\\n\"\n",
    "        output_list.append([sus_dis.attr_name,sus_dis.value,sus_dis.frequency])\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a446884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_spec_chars(df):\n",
    "    ''' Detects special characters using Deepchecks'''\n",
    "    spec_mvs = []\n",
    "    check = SpecialCharacters(n_most_common=10000, n_top_columns=10000).run(df)\n",
    "    if check.display:\n",
    "        result = check.display[1]\n",
    "        print(result)\n",
    "        for idx, specs in zip(result.index, result['Most Common Special-Only Samples']):\n",
    "            for spec in specs:\n",
    "                freq = len(df[df[idx] == spec])\n",
    "                spec_mvs.append([idx, spec, freq])\n",
    "    return spec_mvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c518dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_mixed_nulls(df):\n",
    "    ''' Dects mixed nulls using deepchecks'''\n",
    "    mixed_nulls = []\n",
    "    check = MixedNulls(n_top_columns=10000).run(df)\n",
    "    if check.display:\n",
    "        result = check.display[1].reset_index()\n",
    "        for i in range(len(result)):\n",
    "            if not result['Value'][i].endswith('.nan') and result['Value'][i] != 'None':\n",
    "                col = result.loc[i, 'Column Name']\n",
    "                val = result.loc[i, 'Value']\n",
    "                freq = result.loc[i, 'Count']\n",
    "                mixed_nulls.append([col, val, freq])\n",
    "    return mixed_nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f79ff0b",
   "metadata": {},
   "source": [
    "### Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad5e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_mv(df, feature_types):\n",
    "    ''' Imputes missing values according to the feature types, dataset size and missingness ratio\n",
    "    '''\n",
    "    df_copy = copy.deepcopy(df).astype(str)\n",
    "    \n",
    "    # Determine imputation method\n",
    "    feature_type_set = set([typ for typ in feature_types if typ == 'numeric' or typ == 'categorical'])\n",
    "    dataset_size = df_copy.size\n",
    "    dtypes_dct = {col:df.dtypes[col] for col in df}\n",
    "    dmvs = detect_dmv(df_copy)\n",
    "    mvs = detect_mv(df)\n",
    "    spec_chars = detect_spec_chars(df)\n",
    "    mixed_nulls = detect_mixed_nulls(df)\n",
    "    all_mvs = dmvs + mvs + spec_chars + mixed_nulls\n",
    "    unique_mvs = []\n",
    "    nan_cols = []\n",
    "    for mv in all_mvs:\n",
    "        if mv not in unique_mvs:\n",
    "            unique_mvs.append(mv)\n",
    "        if mv[0] not in nan_cols:\n",
    "            nan_cols.append(mv[0])\n",
    "    print(\"Detected Missing Values: \", unique_mvs)\n",
    "    for mv in unique_mvs:\n",
    "        indices = df_copy[df_copy[mv[0]] == mv[1]].index\n",
    "        df_copy.loc[indices, mv[0]] = np.nan\n",
    "    \n",
    "    print(f\"Number of feature types: {len(feature_type_set)}\")\n",
    "    if len(feature_type_set) == 1:\n",
    "        #not mixed data\n",
    "        data_type = list(feature_type_set)[0]\n",
    "        print(f\"Data type: {data_type}\")\n",
    "        print(f\"Dataset size: {dataset_size}\")\n",
    "        if data_type == 'numeric':\n",
    "            if dataset_size < 10000:\n",
    "                method = \"MLP\"\n",
    "            else:\n",
    "                method = \"CART\"\n",
    "        else:\n",
    "            if dataset_size < 10000:\n",
    "                method = \"RF\"\n",
    "            else:\n",
    "                method = \"CART\"\n",
    "    else:\n",
    "        #mixed data\n",
    "        nan_values = df_copy.isnull().sum().sum()\n",
    "        missingness_ratio = nan_values / dataset_size\n",
    "        print(f\"MR: {missingness_ratio}\")\n",
    "        if missingness_ratio <= 0.1:\n",
    "            method = \"KNN\"\n",
    "        else:\n",
    "            print(f\"Dataset size: {dataset_size}\")\n",
    "            if dataset_size < 10000:\n",
    "                method = \"RF\"\n",
    "            else:\n",
    "                method = \"CART\"\n",
    "    print(f\"Method: {method}\")\n",
    "            \n",
    "    if method == \"KNN\":\n",
    "        labeled_df, dct_encoder = categorical_to_label(df_copy, encode_nans=False)\n",
    "        imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "        output = imputer.fit_transform(labeled_df)\n",
    "        df_imputed = pd.DataFrame(output, columns=labeled_df.columns)\n",
    "        df_final = copy.deepcopy(df_copy)\n",
    "        for col in df_final:\n",
    "            if df_final[col].isnull().any(): # only take the imputed column for columns that got missing values\n",
    "                if col in dct_encoder:\n",
    "                    # object columns\n",
    "                    col_encoding = dct_encoder[col]\n",
    "                    imputed_col = col_encoding.inverse_transform(list(round(df_imputed[col]).values.astype(int)))\n",
    "                else:\n",
    "                    if dtypes_dct[col] == pd.Int64Dtype():\n",
    "                        # integer columns should not contain floats\n",
    "                        imputed_col = pd.Series(list(round(df_imputed[col]).values.astype(int)))\n",
    "                    else:\n",
    "                        # float columns\n",
    "                        imputed_col = df_imputed[col]\n",
    "                df_final[col] = imputed_col\n",
    "        for col in df_final:\n",
    "            if pd.api.types.is_categorical_dtype(dtypes_dct[col]):\n",
    "                df_final[col] = df_final[col].astype('category')\n",
    "            else:\n",
    "                df_final[col] = df_final[col].astype(dtypes_dct[col])\n",
    "        return df_final\n",
    "    else:\n",
    "        for col in nan_cols:\n",
    "            df_copy[col] = mv_method(df_copy, col, method)\n",
    "    for col in df_copy:\n",
    "        if pd.api.types.is_categorical_dtype(dtypes_dct[col]):\n",
    "            df_copy[col] = df_copy[col].astype('category')\n",
    "        else:\n",
    "            df_copy[col] = df_copy[col].astype(dtypes_dct[col])\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8f7a7",
   "metadata": {},
   "source": [
    "## Duplicate instances <a class=\"anchor\" id=\"duplicate_rows\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fb5e5",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e48a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dup_row(df):\n",
    "    ''' Detects duplicate rows'''\n",
    "    dup_row_mask = df.duplicated()\n",
    "    dup_row_idxs = df.index[dup_row_mask].to_list()\n",
    "    print(\"Detected duplicate rows: \", dup_row_idxs)\n",
    "    return dup_row_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf721a2",
   "metadata": {},
   "source": [
    "### Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e36105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dup_row(df, remove=True):\n",
    "    ''' Corrects duplicate rows by dropping the duplictes except the first one'''\n",
    "    if remove == True:\n",
    "        duplicate_rows = detect_dup_row(df)\n",
    "        new_df = df.drop(index=duplicate_rows)\n",
    "    else:\n",
    "        new_df = df\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54137d8c",
   "metadata": {},
   "source": [
    "## Duplicate attributes <a class=\"anchor\" id=\"duplicate_cols\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a8c01",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cafe4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dup_col(df):\n",
    "    ''' Detects duplicate columns'''\n",
    "    dup_col_mask = df.T.duplicated()\n",
    "    dup_col_names = df.columns[dup_col_mask].to_list()\n",
    "    print(\"Detected duplicate columns: \", dup_col_names)\n",
    "    return dup_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf7833",
   "metadata": {},
   "source": [
    "### Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7638147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dup_col(df):\n",
    "    ''' Corrects duplicate cols by dropping the duplicates except the first one'''\n",
    "    duplicate_columns = detect_dup_col(df)\n",
    "    new_df = df.drop(columns=duplicate_columns)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14040812",
   "metadata": {},
   "source": [
    "## Outliers values <a class=\"anchor\" id=\"outlier_vals\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f784582",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69292ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outlier_val(series, k=3):\n",
    "    ''' Detects outlier values (only k=3 away cause the recommendation is to correct those and keep k=2)'''\n",
    "    q25, q75 = percentile(series, 25), percentile(series,75)\n",
    "    k_IQR = k * (q75 - q25)\n",
    "    lower_bound, upper_bound = q25 - k_IQR, q75 + k_IQR\n",
    "    outliers = set([val for val in series if val < lower_bound or val > upper_bound])\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccf32a",
   "metadata": {},
   "source": [
    "### Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "194d2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_outlier_val(df, feature_types, feature_types_dct, method=\"impute\"):\n",
    "    ''' Corrects outlier values by default using imputation'''\n",
    "    outlier_dct = dict()\n",
    "    int_cols = []\n",
    "    for col, typ in df.dtypes.items(): # to convert int columns after imputation from float back to int\n",
    "        if typ == 'int64':\n",
    "            int_cols.append(col)\n",
    "    for col, ft_type in feature_types_dct.items():\n",
    "        if ft_type == 'numeric' and col in df.columns:\n",
    "            outlier_vals = detect_outlier_val(df[col])\n",
    "            if len(outlier_vals) != 0:\n",
    "                outlier_dct[col] = outlier_vals\n",
    "    print(\"Detected outlier values: \", outlier_dct)\n",
    "    if method == \"remove\":\n",
    "        idx_to_remove = []\n",
    "        for col, outliers in outlier_dct.items():\n",
    "            for idx, val in df[col].items():\n",
    "                if val in outliers:\n",
    "                    idx_to_remove.append(idx)\n",
    "        df_imputed = df.drop(idx_to_remove)\n",
    "    else:\n",
    "        df_with_nans = copy.deepcopy(df)\n",
    "        for col, vals in outlier_dct.items():\n",
    "            for val in vals:\n",
    "                indices = df_with_nans[df_with_nans[col] == val].index\n",
    "                for idx in indices:\n",
    "                    df_with_nans.loc[idx, col] = np.nan\n",
    "        print(feature_types)\n",
    "        df_imputed = impute_mv(df_with_nans, feature_types)\n",
    "        for col in int_cols:\n",
    "            df_imputed[col] = df_imputed[col].astype(int)\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8187eb0e",
   "metadata": {},
   "source": [
    "## Outlier instances <a class=\"anchor\" id=\"outlier_rows\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd1911",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29977073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outlier_row(df, threshold=0.8):\n",
    "    ''' Detect outlier instances using Deepchecks'''\n",
    "    if math.sqrt(len(df)) < 5: #minimum number of nearest neighbors\n",
    "        nearest_neighbors_percent = 5/len(df)\n",
    "        if nearest_neighbors_percent >= 1:\n",
    "            nearest_neighbors_percent = 0.99\n",
    "        if nearest_neighbors_percent == 0:\n",
    "            nearest_neighbors_percent = 0.01\n",
    "    else:\n",
    "        nearest_neighbors_percent = round(((math.sqrt(len(df))) / len(df)), 2)\n",
    "        if nearest_neighbors_percent == 0:\n",
    "            nearest_neighbors_percent = 0.01\n",
    "        if nearest_neighbors_percent >= 1:\n",
    "            nearest_neighbors_percent = 0.99\n",
    "    check = OutlierSampleDetection(n_to_show=10000, nearest_neighbors_percent=nearest_neighbors_percent, extent_parameter=3,  outlier_score_threshold=0.8, timeout=300)\n",
    "    result = check.run(df)\n",
    "    if result.display:\n",
    "        output = result.display[1]\n",
    "        outlier_indices = output[output['Outlier Probability Score'] > 0.8].index\n",
    "        print(\"Detected outlier instances: \", outlier_indices)\n",
    "    return outlier_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c20034",
   "metadata": {},
   "source": [
    "### Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7542d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_outlier_row(df):\n",
    "    ''' Correct Outlier instances by dropping them from the dataset'''\n",
    "    outlier_indices = detect_outlier_row(df)\n",
    "    new_df = df.drop(outlier_indices)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164619b",
   "metadata": {},
   "source": [
    "## Cryptic attribute names <a class=\"anchor\" id=\"cryptic\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c929764",
   "metadata": {},
   "source": [
    "### Classes and help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5f1efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrypticIdentifier:\n",
    "    \"\"\"Module to identify any cryptic forms in a column header.\n",
    "    Example usage: \n",
    "        identifier = CrypticIdentifier(vocab_file)\n",
    "        identifier.iscryptic(\"newyorkcitytotalpopulation\") --> False\n",
    "        identifier.iscryptic(\"tot_revq4\") --> True\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_file=None, word_rank_file=None, k_whole=4, k_split=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_file (str, optional): json file containing the vocabulary. Defaults to None.\n",
    "            k_whole (int, optional): length threshold for a whole string to be considered non-cryptic if it fails the first round of check (i.e.\n",
    "            _iscryptic returns True). Defaults to 4.\n",
    "            k_split (int, optional): length threshold for each word split (wordninja.split()) from the string to be considered non-cryptic, if the pre-split string fails the first round of check (i.e.\n",
    "            _iscryptic returns True). Defaults to 2.\n",
    "        \"\"\"\n",
    "        if vocab_file is not None:\n",
    "            with open(vocab_file, \"r\") as fi:\n",
    "                self.vocab = json.load(fi)\n",
    "#                 print(\"#vocab={}\".format(len(self.vocab)))\n",
    "        else:\n",
    "            self.vocab = None\n",
    "\n",
    "        self.k_whole = k_whole\n",
    "        self.k_split = k_split\n",
    "        if word_rank_file is None:\n",
    "            self.splitter = wordninja\n",
    "        else:\n",
    "            self.splitter = wordninja.LanguageModel(word_rank_file)\n",
    "        self.lem = WordNetLemmatizer()\n",
    "        \n",
    "\n",
    "    def split_rm_punc(self, text: str) -> list:\n",
    "        return re.sub(r'[^\\w\\s]', ' ', text).split()\n",
    "\n",
    "    def separate_camel_case(self, text: str) -> list:\n",
    "        return re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', text))\n",
    "\n",
    "    def convert2base(self, text: str) -> str:\n",
    "        return self.lem.lemmatize(text)\n",
    "\n",
    "    def _split(self, text: str) -> list:\n",
    "        text = text.replace('_', ' ')\n",
    "        words = self.split_rm_punc(self.separate_camel_case(text))\n",
    "        return words\n",
    "\n",
    "    def _iscryptic(self, text: str) -> bool:\n",
    "        words = self._split(text)\n",
    "        if all([word.isnumeric() for word in words]):\n",
    "            return True\n",
    "        if self.vocab is None:\n",
    "            self.vocab = nltk.corpus.wordnet.words('english')\n",
    "        return any([self.convert2base(w.lower()) not in self.vocab for w in words])\n",
    "\n",
    "\n",
    "\n",
    "    def doublecheck_cryptic(self, text: str) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Double-check whether a column header contains cryptic terms. For example in some cases where neither \n",
    "        delimiters between tokens nor camelcases is available\n",
    "\n",
    "        Args:\n",
    "            text (str): column header\n",
    "\n",
    "        Returns:\n",
    "            Tuple[\n",
    "                    bool: whether header is cryptic\n",
    "                    List[str]: splitted tokens from the header\n",
    "                ]\n",
    "        \"\"\"\n",
    "\n",
    "        #stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "        def split_check(words: List[str]) -> Tuple[bool, List[str]]:\n",
    "            l_cryptic = []\n",
    "            for ele in words:\n",
    "                if ele.isdigit():\n",
    "                    l_cryptic.append(False)\n",
    "                ## Cornercases includes stopwords like \"I\", \"for\", etc.\n",
    "                elif len(ele) < self.k_split: # and ele.lower() not in stopwords:\n",
    "                    l_cryptic.append(True)\n",
    "                ## Second round check\n",
    "                else:\n",
    "                    l_cryptic.append(self._iscryptic(ele))\n",
    "            return any(l_cryptic), words\n",
    "            \n",
    "        if len(text) >= self.k_whole:\n",
    "            if self._iscryptic(text):\n",
    "                split = self.splitter.split(text)\n",
    "                return split_check(split)            \n",
    "            else:\n",
    "                # return (False, self.splitter.split(text))\n",
    "                return (False, self._split(text))\n",
    "        else:\n",
    "            with open('data/vocab/words.txt', 'r') as file:\n",
    "                # Read the content of the file\n",
    "                content = file.read()\n",
    "\n",
    "                # Split the content into words based on whitespace\n",
    "                vocabulair = content.split()\n",
    "\n",
    "            # Print the list of words\n",
    "            if text in vocabulair:\n",
    "                return (False, [text])\n",
    "            return (True, [text])\n",
    "\n",
    "    def iscryptic(self, text: str) -> bool:\n",
    "        return self.doublecheck_cryptic(text)[0]\n",
    "    \n",
    "    def split_results(self, text: str) -> List[str]:\n",
    "        return self.doublecheck_cryptic(text)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94c276fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cryp_cols(cryp_list, title, description, content_df):\n",
    "    \n",
    "    col_query = \" | \".join(str(item) for item in cryp_list)\n",
    "    \n",
    "    if not content_df.empty:\n",
    "        num_instances = len(content_df)\n",
    "        content_list = [content_df.loc[idx].to_list() for idx in content_df.index]\n",
    "        contents = \"\\n\".join([\" | \".join(str(item) for item in lst) for lst in content_list])\n",
    "        content_bool = True\n",
    "    else:\n",
    "        content_bool = False\n",
    "    \n",
    "    if title == False:\n",
    "        title_query = \"\"\n",
    "    else:\n",
    "        title_query = f\"title: {title}\"\n",
    "    if description == False:\n",
    "        desc_query = \"\"\n",
    "    else:\n",
    "        desc_query = f\"description: {description}\"\n",
    "    if content_bool == False:\n",
    "        content_query = \"\"\n",
    "    else:\n",
    "        content_query = f\"contents of {num_instances} random instances:\\n{contents}\"\n",
    "    \n",
    "    if title == False and description == False and content_bool == False:\n",
    "        subquery = \"\"\n",
    "    else:\n",
    "        subquery=f\"\"\"As abbreviations of column names from a table with characteristics:\n",
    "{title_query}\n",
    "{desc_query}\n",
    "{content_query}\n",
    "        \"\"\"\n",
    "    \n",
    "    query = f\"\"\"As abbreviations of column names from a table, the column names c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
    "{subquery}\n",
    "The column names {col_query} stand for\"\"\"\n",
    "    print(query)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00a0a8",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ebb947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cryp_cols(df):\n",
    "    ''' Detect cryptic column names in dataset'''\n",
    "    identifier = CrypticIdentifier(\"./lookups/wordnet.json\", \"./lookups/wordninja_words_alpha.txt.gz\")\n",
    "    cryptic_cols = [col for col in df.columns if identifier.doublecheck_cryptic(col)[0]==True]\n",
    "    return cryptic_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f8947",
   "metadata": {},
   "source": [
    "### Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73f11124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_cryp_cols(df, title=False, description=False, n_instances=0):\n",
    "    ''' Corrects the cryptic column names using GPT-3.5'''\n",
    "    # Set API key, uncomment to test\n",
    "    client = OpenAI(api_key=\"\") # add your own API key here\n",
    "    \n",
    "    # Identify the cryptic column names\n",
    "    cryptic_cols = detect_cryp_cols(df)\n",
    "    num_cols = len(cryptic_cols)\n",
    "    \n",
    "    if n_instances == 0:\n",
    "        instances = pd.DataFrame()\n",
    "    else:\n",
    "        # Obtain n random instances of content for the cryptic columns\n",
    "        sliced_df = df[cryptic_cols]\n",
    "        indices = random.sample(range(0, len(sliced_df)), n_instances)\n",
    "        instances = sliced_df.iloc[indices]\n",
    "    \n",
    "    # Generate the OpenAI query\n",
    "    query = query_cryp_cols(cryptic_cols, title=title, description=description, content_df=instances)\n",
    "    tokens_used = len(token_encoding.encode(query))\n",
    "    print(tokens_used)\n",
    "#     # 3 word ~= 4 tokens of OpenAI. We dont want too long column names (max. 4 words), which translates to ~ max 5 tokens per column name\n",
    "#     max_token_len = num_cols * 4\n",
    "    # Call OpenAI's GPT 3.5 Turbo LLM model to generate better column names\n",
    "    completion = client.chat.completions.create(model=\"gpt-3.5-turbo\",temperature=0.0,messages=[{\"role\": \"user\", \"content\": query}])\n",
    "    message = completion.choices[0].message\n",
    "    new_col_names = message.content\n",
    "    return new_col_names, tokens_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d76923",
   "metadata": {},
   "source": [
    "## Single value columns <a class=\"anchor\" id=\"svs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dbc4b",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "895ff7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sv_col(df):\n",
    "    '''Detects single value columns'''\n",
    "    single_value_cols_list = []\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() <= 1:\n",
    "            single_value_cols_list.append(col)\n",
    "    print(\"Detected sv columns: \", single_value_cols_list)\n",
    "    return single_value_cols_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a7d06",
   "metadata": {},
   "source": [
    "### Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c84c4078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sv_col(df, keep=False):\n",
    "    ''' Corrects single value columns by removing them'''\n",
    "    if not keep:\n",
    "        single_value_cols_list = detect_sv_col(df)\n",
    "        df = df.drop(columns=single_value_cols_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5df96",
   "metadata": {},
   "source": [
    "## Mixed data type columns <a class=\"anchor\" id=\"mixed\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8accaf18",
   "metadata": {},
   "source": [
    "### Classes and Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c11fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mix_to_nan(df, mix_dct):\n",
    "    ''' Converts the minority data type in mixed data columns to NaN values'''\n",
    "    for col, dct in mix_dct.items():\n",
    "        if dct['numbers'] < dct['strings']:\n",
    "            #numbers to np.nan\n",
    "            for idx, val in df[col].items():\n",
    "                if isinstance(val, str) and val.isdigit():\n",
    "                    df.loc[idx, col] = np.nan\n",
    "                elif isinstance(val, (int, float)):\n",
    "                    df.loc[idx, col] = np.nan\n",
    "        else:\n",
    "            #strings to np.nan\n",
    "            for idx, val in df[col].items():\n",
    "                if isinstance(val, str) and not val.isdigit():\n",
    "                    df.loc[idx, col] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e592e",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffdf4820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_mixed_data(df):\n",
    "    ''' Detects mixed data types using deepchecks'''\n",
    "    result = MixedDataTypes(n_top_columns=10000).run(df)\n",
    "    mixed_dct = result.value\n",
    "    delete_cols = []\n",
    "    for col, mixed in mixed_dct.items():\n",
    "        if len(mixed) == 0:\n",
    "            delete_cols.append(col)\n",
    "    for col in delete_cols:\n",
    "        del mixed_dct[col]\n",
    "    print(\"Detected mixed data: \", mixed_dct)\n",
    "    return mixed_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aefc2f",
   "metadata": {},
   "source": [
    "### Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "349418be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_mixed_data(df, feature_types):\n",
    "    ''' Corrects mixed data using deepchecks'''\n",
    "    df_copy = copy.deepcopy(df)\n",
    "    mixed_dct = detect_mixed_data(df_copy)\n",
    "    df_with_nans = convert_mix_to_nan(df_copy, mixed_dct)\n",
    "    df_imputed = impute_mv(df_with_nans, feature_types)\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6eda4",
   "metadata": {},
   "source": [
    "## Mislabeled labels <a class=\"anchor\" id=\"mislabels\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779819a",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd2d0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_mislabels(df, target):\n",
    "    ''' Detects Incorrect labels using Cleanlab'''\n",
    "    model_XGBC = XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "    df_numeric, encoding = categorical_to_label(df)\n",
    "    X = df_numeric.drop(columns=[target])\n",
    "    y = df_numeric[target]\n",
    "    \n",
    "    stratified_splits = StratifiedKFold(n_splits=5)\n",
    "    pred_probs = cross_val_predict(model_XGBC, X, y, cv=stratified_splits, method='predict_proba')\n",
    "    cl = cleanlab.classification.CleanLearning()\n",
    "    df_label_issues = cl.find_label_issues(X=None, labels=y, pred_probs=pred_probs)\n",
    "    print(\"Detected mislabels: \", df_label_issues[df_label_issues['is_label_issue'] == True])\n",
    "    return df_label_issues, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a601815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_mislabels(df, target, ft_types_dct):\n",
    "    ''' Corrects incorrect labels and converts them to correct label'''\n",
    "    df_copy = copy.deepcopy(df)\n",
    "    if ft_types_dct[target] == 'categorical':\n",
    "        df_label_issues, encoding = detect_mislabels(df_copy, target)\n",
    "        df_label_issues['predicted_label'] = encoding[target].inverse_transform(df_label_issues['predicted_label'].to_list())\n",
    "        dicty = {idx : df_label_issues.loc[idx,'predicted_label'] for idx in df_label_issues[df_label_issues['is_label_issue'] == True].index}\n",
    "        col_idx = [col_idx for col_idx in range(len(df_copy.columns)) if df_copy.columns[col_idx] == target][0]\n",
    "        for idx in dicty.keys():\n",
    "            df_copy.iloc[idx,col_idx] = dicty[idx]\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127538d9",
   "metadata": {},
   "source": [
    "## String mismatch <a class=\"anchor\" id=\"mismatches\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fdcc19",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e06c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_mismatch(df, target):\n",
    "    ''' Detects string mismatches using deepchecks'''\n",
    "    result = StringMismatch(n_top_columns=10000).run(df)\n",
    "    dct_mismatch = result.value['columns']\n",
    "    print(dct_mismatch)\n",
    "    delete_cols = []\n",
    "    for col, mismatches in dct_mismatch.items():\n",
    "        if len(mismatches) == 0:\n",
    "            delete_cols.append(col)\n",
    "    for col in delete_cols:\n",
    "        print(col)\n",
    "        del dct_mismatch[col]\n",
    "    if target in dct_mismatch:\n",
    "        del dct_mismatch[target]\n",
    "    df_dct = dict()\n",
    "    for col, data in dct_mismatch.items():\n",
    "        for base, var in data.items():\n",
    "            new_df = pd.DataFrame(var)\n",
    "            df_dct[(col, base)] = new_df\n",
    "    print(\"Detected mismatches: \", dct_mismatch)\n",
    "    return df_dct, dct_mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306ba57",
   "metadata": {},
   "source": [
    "### Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bb2da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_mismatch(df, target):\n",
    "    ''' Corrects the string mismatches by converting them to base form'''\n",
    "    df_copy = copy.deepcopy(df)\n",
    "    print(df_copy)\n",
    "    dct_df, mismatch_dct = detect_mismatch(df_copy, target)\n",
    "    for col, mismatches in mismatch_dct.items():\n",
    "        variations = []\n",
    "        for base, var_dct_list in mismatches.items():\n",
    "            for var in var_dct_list:\n",
    "                variations.append(var['variant'])\n",
    "        variation_idx = df_copy[df_copy[col].isin(variations)].index\n",
    "        df_copy.loc[variation_idx, col] = base\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f0b12",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6461cbb",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10ba75d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['gender', 'nan', 11]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_mv(df_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f4d59df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['workclass', 'Variation@', 3],\n",
       " ['education', '?', 7],\n",
       " ['education', '0', 3],\n",
       " ['education', '-', 3]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_dmv(df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9b479",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3cb88867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected duplicate rows:  [20, 25, 34, 43, 46, 47]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20, 25, 34, 43, 46, 47]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_dup_row(df_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11cccd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected duplicate columns:  ['duplicate column']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['duplicate column']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_dup_col(df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2c432",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42ce322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "{128, 167, 137, 139, 143}\n",
      "fnlwgt\n",
      "set()\n",
      "duplicate column\n",
      "{128, 167, 137, 139, 143}\n"
     ]
    }
   ],
   "source": [
    "for col, ft_type in feature_types_dct.items():\n",
    "    if ft_type == 'numeric':\n",
    "        print(col)\n",
    "        print(detect_outlier_val(df_synthetic[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc82513c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - Received a \"pandas.DataFrame\" instance. It is recommended to pass a \"deepchecks.tabular.Dataset\" instance by initializing it with the data and metadata, for example by doing \"Dataset(dataframe, label=label, cat_features=cat_features)\"\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 6 categorical features were inferred.: gender, native-country, income, high_label_correlation, redundant SV column, mixed_data_types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected outlier instances:  Index([], dtype='int64')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([], dtype='int64')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_outlier_row(df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf0d85",
   "metadata": {},
   "source": [
    "### Cryptic column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77b444c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fnlwgt', 'redundant SV column']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_cryp_cols(df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecea1cd",
   "metadata": {},
   "source": [
    "### Single value columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e806d04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected sv columns:  ['redundant SV column']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['redundant SV column']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_sv_col(df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3e871",
   "metadata": {},
   "source": [
    "### Mixed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c5ee616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - Received a \"pandas.DataFrame\" instance. It is recommended to pass a \"deepchecks.tabular.Dataset\" instance by initializing it with the data and metadata, for example by doing \"Dataset(dataframe, label=label, cat_features=cat_features)\"\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 6 categorical features were inferred.: gender, native-country, income, high_label_correlation, redundant SV column, mixed_data_types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected mixed data:  {'education': {'strings': 0.9454545454545454, 'numbers': 0.05454545454545454, 'strings_examples': {'Bachelors', 'Masters', 'HS-grad'}, 'numbers_examples': {0}}, 'mixed_data_types': {'strings': 0.2727272727272727, 'numbers': 0.7272727272727273, 'strings_examples': {'MD2', 'MD3', 'MD1'}, 'numbers_examples': {1, 2, 3}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'education': {'strings': 0.9454545454545454,\n",
       "  'numbers': 0.05454545454545454,\n",
       "  'strings_examples': {'Bachelors', 'HS-grad', 'Masters'},\n",
       "  'numbers_examples': {0}},\n",
       " 'mixed_data_types': {'strings': 0.2727272727272727,\n",
       "  'numbers': 0.7272727272727273,\n",
       "  'strings_examples': {'MD1', 'MD2', 'MD3'},\n",
       "  'numbers_examples': {1, 2, 3}}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_mixed_data(df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e764c5",
   "metadata": {},
   "source": [
    "### Mislabeled labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a1a2e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected mislabels:      is_label_issue  label_quality  given_label  predicted_label\n",
      "12            True       0.279960            0                1\n",
      "37            True       0.168822            0                1\n",
      "45            True       0.385885            0                1\n",
      "50            True       0.009794            1                0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    is_label_issue  label_quality  given_label  predicted_label\n",
       " 0            False       0.942201            0                0\n",
       " 1            False       0.949764            0                0\n",
       " 2            False       0.996256            0                0\n",
       " 3            False       0.921297            0                0\n",
       " 4            False       0.542952            0                0\n",
       " 5            False       0.985606            0                0\n",
       " 6            False       0.982427            0                0\n",
       " 7            False       0.981859            0                0\n",
       " 8            False       0.990919            0                0\n",
       " 9            False       0.840710            0                0\n",
       " 10           False       0.959199            0                0\n",
       " 11           False       0.992264            0                0\n",
       " 12            True       0.279960            0                1\n",
       " 13           False       0.932174            0                0\n",
       " 14           False       0.891915            1                1\n",
       " 15           False       0.228430            1                0\n",
       " 16           False       0.537657            0                0\n",
       " 17           False       0.972918            0                0\n",
       " 18           False       0.986687            0                0\n",
       " 19           False       0.970640            0                0\n",
       " 20           False       0.910944            1                1\n",
       " 21           False       0.938079            1                1\n",
       " 22           False       0.958685            0                0\n",
       " 23           False       0.537909            0                0\n",
       " 24           False       0.547430            0                0\n",
       " 25           False       0.916744            0                0\n",
       " 26           False       0.025416            1                0\n",
       " 27           False       0.762660            0                0\n",
       " 28           False       0.992790            0                0\n",
       " 29           False       0.516910            0                0\n",
       " 30           False       0.986609            0                0\n",
       " 31           False       0.997276            0                0\n",
       " 32           False       0.964423            0                0\n",
       " 33           False       0.954987            0                0\n",
       " 34           False       0.771255            1                1\n",
       " 35           False       0.994930            0                0\n",
       " 36           False       0.992684            0                0\n",
       " 37            True       0.168822            0                1\n",
       " 38           False       0.579610            0                0\n",
       " 39           False       0.924917            0                0\n",
       " 40           False       0.994930            0                0\n",
       " 41           False       0.994000            0                0\n",
       " 42           False       0.984836            0                0\n",
       " 43           False       0.993666            0                0\n",
       " 44           False       0.950666            0                0\n",
       " 45            True       0.385885            0                1\n",
       " 46           False       0.730548            0                0\n",
       " 47           False       0.939759            0                0\n",
       " 48           False       0.962803            0                0\n",
       " 49           False       0.962803            0                0\n",
       " 50            True       0.009794            1                0\n",
       " 51           False       0.030034            1                0\n",
       " 52           False       0.060241            1                0\n",
       " 53           False       0.037197            1                0\n",
       " 54           False       0.037197            1                0,\n",
       " {'workclass': LabelEncoder(),\n",
       "  'education': LabelEncoder(),\n",
       "  'gender': LabelEncoder(),\n",
       "  'native-country': LabelEncoder(),\n",
       "  'income': LabelEncoder(),\n",
       "  'high_label_correlation': LabelEncoder(),\n",
       "  'redundant SV column': LabelEncoder(),\n",
       "  'mixed_data_types': LabelEncoder()})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_mislabels(df_synthetic, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8d0e1",
   "metadata": {},
   "source": [
    "### String mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "579e9b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - Received a \"pandas.DataFrame\" instance. It is recommended to pass a \"deepchecks.tabular.Dataset\" instance by initializing it with the data and metadata, for example by doing \"Dataset(dataframe, label=label, cat_features=cat_features)\"\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 6 categorical features were inferred.: gender, native-country, income, high_label_correlation, redundant SV column, mixed_data_types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'workclass': {'variation': [{'variant': 'VARIATION', 'count': 1, 'percent': 0.01818181818181818}, {'variant': 'VARiation!', 'count': 1, 'percent': 0.01818181818181818}, {'variant': 'vaRiAtIoN', 'count': 3, 'percent': 0.05454545454545454}, {'variant': 'variation', 'count': 2, 'percent': 0.03636363636363636}, {'variant': 'Variation@', 'count': 3, 'percent': 0.05454545454545454}]}, 'education': {}, 'gender': {}, 'native-country': {}, 'income': {'50k': [{'variant': '<=50K', 'count': 44, 'percent': 0.8}, {'variant': '>50K', 'count': 11, 'percent': 0.2}]}, 'high_label_correlation': {}, 'redundant SV column': {}, 'mixed_data_types': {}}\n",
      "education\n",
      "gender\n",
      "native-country\n",
      "high_label_correlation\n",
      "redundant SV column\n",
      "mixed_data_types\n",
      "Detected mismatches:  {'workclass': {'variation': [{'variant': 'VARIATION', 'count': 1, 'percent': 0.01818181818181818}, {'variant': 'VARiation!', 'count': 1, 'percent': 0.01818181818181818}, {'variant': 'vaRiAtIoN', 'count': 3, 'percent': 0.05454545454545454}, {'variant': 'variation', 'count': 2, 'percent': 0.03636363636363636}, {'variant': 'Variation@', 'count': 3, 'percent': 0.05454545454545454}]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({('workclass',\n",
       "   'variation'):       variant  count   percent\n",
       "  0   VARIATION      1  0.018182\n",
       "  1  VARiation!      1  0.018182\n",
       "  2   vaRiAtIoN      3  0.054545\n",
       "  3   variation      2  0.036364\n",
       "  4  Variation@      3  0.054545},\n",
       " {'workclass': {'variation': [{'variant': 'VARIATION',\n",
       "     'count': 1,\n",
       "     'percent': 0.01818181818181818},\n",
       "    {'variant': 'VARiation!', 'count': 1, 'percent': 0.01818181818181818},\n",
       "    {'variant': 'vaRiAtIoN', 'count': 3, 'percent': 0.05454545454545454},\n",
       "    {'variant': 'variation', 'count': 2, 'percent': 0.03636363636363636},\n",
       "    {'variant': 'Variation@', 'count': 3, 'percent': 0.05454545454545454}]}})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_mismatch(df_synthetic, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34802082",
   "metadata": {},
   "source": [
    "# Downstream model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110a1b3",
   "metadata": {},
   "source": [
    "## Gather datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8d7ac13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>uploader</th>\n",
       "      <th>status</th>\n",
       "      <th>format</th>\n",
       "      <th>MajorityClassSize</th>\n",
       "      <th>MaxNominalAttDistinctValues</th>\n",
       "      <th>MinorityClassSize</th>\n",
       "      <th>NumberOfClasses</th>\n",
       "      <th>NumberOfFeatures</th>\n",
       "      <th>NumberOfInstances</th>\n",
       "      <th>NumberOfInstancesWithMissingValues</th>\n",
       "      <th>NumberOfMissingValues</th>\n",
       "      <th>NumberOfNumericFeatures</th>\n",
       "      <th>NumberOfSymbolicFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>anneal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>684.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>898.0</td>\n",
       "      <td>898.0</td>\n",
       "      <td>22175.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>kr-vs-kp</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>1669.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3196.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>labor</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>arrhythmia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>245.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>letter</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>813.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>734.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46254</th>\n",
       "      <td>46254</td>\n",
       "      <td>Diabetes_Dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>39999</td>\n",
       "      <td>active</td>\n",
       "      <td>arff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46255</th>\n",
       "      <td>46255</td>\n",
       "      <td>Student_Performance_Dataset</td>\n",
       "      <td>1</td>\n",
       "      <td>39999</td>\n",
       "      <td>active</td>\n",
       "      <td>arff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2392.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46258</th>\n",
       "      <td>46258</td>\n",
       "      <td>sonar</td>\n",
       "      <td>2</td>\n",
       "      <td>43180</td>\n",
       "      <td>active</td>\n",
       "      <td>arff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46259</th>\n",
       "      <td>46259</td>\n",
       "      <td>Electricity-hourly</td>\n",
       "      <td>1</td>\n",
       "      <td>30703</td>\n",
       "      <td>active</td>\n",
       "      <td>arff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319.0</td>\n",
       "      <td>26305.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46260</th>\n",
       "      <td>46260</td>\n",
       "      <td>FRED-QD</td>\n",
       "      <td>1</td>\n",
       "      <td>30703</td>\n",
       "      <td>active</td>\n",
       "      <td>arff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>204.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5690 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         did                         name  version uploader  status format  \\\n",
       "2          2                       anneal        1        1  active   ARFF   \n",
       "3          3                     kr-vs-kp        1        1  active   ARFF   \n",
       "4          4                        labor        1        1  active   ARFF   \n",
       "5          5                   arrhythmia        1        1  active   ARFF   \n",
       "6          6                       letter        1        1  active   ARFF   \n",
       "...      ...                          ...      ...      ...     ...    ...   \n",
       "46254  46254             Diabetes_Dataset        2    39999  active   arff   \n",
       "46255  46255  Student_Performance_Dataset        1    39999  active   arff   \n",
       "46258  46258                        sonar        2    43180  active   arff   \n",
       "46259  46259           Electricity-hourly        1    30703  active   arff   \n",
       "46260  46260                      FRED-QD        1    30703  active   arff   \n",
       "\n",
       "       MajorityClassSize  MaxNominalAttDistinctValues  MinorityClassSize  \\\n",
       "2                  684.0                          7.0                8.0   \n",
       "3                 1669.0                          3.0             1527.0   \n",
       "4                   37.0                          3.0               20.0   \n",
       "5                  245.0                         13.0                2.0   \n",
       "6                  813.0                         26.0              734.0   \n",
       "...                  ...                          ...                ...   \n",
       "46254                NaN                          NaN                NaN   \n",
       "46255                NaN                          NaN                NaN   \n",
       "46258                NaN                          NaN                NaN   \n",
       "46259                NaN                          NaN                NaN   \n",
       "46260                NaN                          NaN                NaN   \n",
       "\n",
       "       NumberOfClasses  NumberOfFeatures  NumberOfInstances  \\\n",
       "2                  5.0              39.0              898.0   \n",
       "3                  2.0              37.0             3196.0   \n",
       "4                  2.0              17.0               57.0   \n",
       "5                 13.0             280.0              452.0   \n",
       "6                 26.0              17.0            20000.0   \n",
       "...                ...               ...                ...   \n",
       "46254              NaN               9.0              768.0   \n",
       "46255              NaN              15.0             2392.0   \n",
       "46258              NaN              61.0              207.0   \n",
       "46259              NaN             319.0            26305.0   \n",
       "46260              NaN             204.0              258.0   \n",
       "\n",
       "       NumberOfInstancesWithMissingValues  NumberOfMissingValues  \\\n",
       "2                                   898.0                22175.0   \n",
       "3                                     0.0                    0.0   \n",
       "4                                    56.0                  326.0   \n",
       "5                                   384.0                  408.0   \n",
       "6                                     0.0                    0.0   \n",
       "...                                   ...                    ...   \n",
       "46254                                 0.0                    0.0   \n",
       "46255                                 0.0                    0.0   \n",
       "46258                                 0.0                    0.0   \n",
       "46259                                 0.0                    0.0   \n",
       "46260                                 0.0                    0.0   \n",
       "\n",
       "       NumberOfNumericFeatures  NumberOfSymbolicFeatures  \n",
       "2                          6.0                      33.0  \n",
       "3                          0.0                      37.0  \n",
       "4                          8.0                       9.0  \n",
       "5                        206.0                      74.0  \n",
       "6                         16.0                       1.0  \n",
       "...                        ...                       ...  \n",
       "46254                      9.0                       0.0  \n",
       "46255                     15.0                       0.0  \n",
       "46258                     60.0                       0.0  \n",
       "46259                    317.0                       1.0  \n",
       "46260                    202.0                       1.0  \n",
       "\n",
       "[5690 rows x 16 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openml\n",
    "import sortinghatinf\n",
    "openml.datasets.list_datasets(output_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0bfb1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have chosen by hand 10 regression and 10 classification datasets\n",
    "regression_datasets = [8, 204, 210, 560, 491, 566, 189, 673, 639, 41700]\n",
    "classification_datasets = [44, 38, 458, 1053, 1050, 2, 3, 5, 31, 1464]\n",
    "complete_did_list = regression_datasets + classification_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d699ac6",
   "metadata": {},
   "source": [
    "## Create clean version of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4f73366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_cleaner(df, feature_types, feature_types_dct, target):\n",
    "    df = correct_dup_row(df).reset_index(drop=True) #dup 1\n",
    "    df = remove_dup_col(df) # dup 2\n",
    "    df = correct_outlier_row(df) # out 1\n",
    "    df = correct_sv_col(df) # sv\n",
    "    df = correct_mismatch(df, target) # mm\n",
    "    df = correct_mislabels(df, target, feature_types_dct) #mislabels\n",
    "    df = impute_mv(df, feature_types) # mv\n",
    "    df = correct_outlier_val(df, feature_types, feature_types_dct) # out 2\n",
    "    df = correct_mixed_data(df, feature_types) # mix\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5c5ff",
   "metadata": {},
   "source": [
    "## Final Data Quality Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85f03f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_datasets(did_list):\n",
    "    ''' Evaluates for every dataset the performance on the dirty and clean dataset'''\n",
    "    ds_perfs = dict()\n",
    "    \n",
    "    for did in did_list:\n",
    "        dataset = openml.datasets.get_dataset(did)\n",
    "        X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            target=dataset.default_target_attribute\n",
    "        )\n",
    "        y = pd.DataFrame(y)\n",
    "        df = pd.concat([X, y], axis=1)\n",
    "        ft_types = sortinghatinf.get_sortinghat_types(df)\n",
    "        ft_types_dct = {col: typ for col, typ in zip(df.columns, ft_types)}\n",
    "        \n",
    "        clean_df = full_cleaner(df, ft_types, ft_types_dct, df.columns[-1])\n",
    "        \n",
    "        task = 'classification' if ft_types[-1] == 'categorical' else 'regression'\n",
    "\n",
    "        new_df = df.dropna()\n",
    "        if len(new_df) < 0.2 * len(df):\n",
    "            new_df = df.dropna(axis=1)\n",
    "        \n",
    "        X_dirty = new_df.drop(columns=[new_df.columns[-1]])\n",
    "        y_dirty = new_df[new_df.columns[-1]]\n",
    "        enc_X_dirty, enc_dirty = categorical_to_label(X_dirty)\n",
    "        \n",
    "        X_clean = clean_df.drop(columns=[df.columns[-1]])\n",
    "        y_clean = clean_df[df.columns[-1]]\n",
    "        enc_X_clean, enc_clean = categorical_to_label(X_clean)\n",
    "\n",
    "        ds_perfs[did] = evaluate_performance(enc_X_dirty, y_dirty, enc_X_clean, y_clean, task)\n",
    "\n",
    "        with open('data/data/performances_datasets_dirty_clean.pkl', 'wb') as handle:\n",
    "            pickle.dump(ds_perfs, handle)\n",
    "    \n",
    "    return ds_perfs\n",
    "\n",
    "def evaluate_performance(X_dirty, y_dirty, X_clean, y_clean, task):\n",
    "    ''' Calculates the performance of the 5 different models using cross validation'''\n",
    "    models = {\n",
    "        'RF': RandomForestClassifier() if task == 'classification' else RandomForestRegressor(),\n",
    "        'LR': LogisticRegression() if task == 'classification' else LinearRegression(),\n",
    "        'KNN': KNeighborsClassifier() if task == 'classification' else KNeighborsRegressor(),\n",
    "        'DT': DecisionTreeClassifier() if task == 'classification' else DecisionTreeRegressor(),\n",
    "        'GB': GradientBoostingClassifier() if task == 'classification' else GradientBoostingRegressor()\n",
    "    }\n",
    "    \n",
    "    metrics = {}\n",
    "    for model_name, model in models.items():\n",
    "        metrics[model_name] = {\n",
    "            'dirty': calculate_performance(X_dirty, y_dirty, task, model),\n",
    "            'clean': calculate_performance(X_clean, y_clean, task, model)\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_performance(X, y, task, model):\n",
    "    ''' Function for cross validation and what performance measure to use'''\n",
    "    if task == 'classification':\n",
    "        cv = StratifiedKFold(n_splits=5)\n",
    "        scoring = ['f1_weighted', 'accuracy']\n",
    "    else:\n",
    "        cv = KFold(n_splits=5)\n",
    "        scoring = ['neg_mean_absolute_error', 'neg_root_mean_squared_error']\n",
    "    \n",
    "    scores = cross_validate(model, X, y, cv=cv, scoring=scoring)\n",
    "    \n",
    "    if task == 'classification':\n",
    "        performance = {\n",
    "            'F1': scores['test_f1_weighted'].mean(),\n",
    "            'Accuracy': scores['test_accuracy'].mean(),\n",
    "        }\n",
    "    else:\n",
    "        performance = {\n",
    "            'MAE': -scores['test_neg_mean_absolute_error'].mean(),\n",
    "            'RMSE': -scores['test_neg_root_mean_squared_error'].mean()}\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d03d35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {'RF': {'dirty': {'MAE': 2.8779565217391303, 'RMSE': 3.4075470512183026},\n",
       "   'clean': {'MAE': 2.8728582089552237, 'RMSE': 3.4040742399517443}},\n",
       "  'LR': {'dirty': {'MAE': 2.8014647951133584, 'RMSE': 3.364155310843141},\n",
       "   'clean': {'MAE': 2.7335902830502987, 'RMSE': 3.2783025527080403}},\n",
       "  'KNN': {'dirty': {'MAE': 3.0234782608695654, 'RMSE': 3.6525838429736104},\n",
       "   'clean': {'MAE': 3.103617208077261, 'RMSE': 3.7102125253265057}},\n",
       "  'DT': {'dirty': {'MAE': 3.6869565217391305, 'RMSE': 4.689735253112677},\n",
       "   'clean': {'MAE': 3.517866549604917, 'RMSE': 4.354162113760905}},\n",
       "  'GB': {'dirty': {'MAE': 2.88346309086793, 'RMSE': 3.4530471410787813},\n",
       "   'clean': {'MAE': 2.901515597717368, 'RMSE': 3.4566025860491605}}},\n",
       " 204: {'RF': {'dirty': {'MAE': 42.80966101694916, 'RMSE': 54.75768759808791},\n",
       "   'clean': {'MAE': 40.26471147540983, 'RMSE': 50.32794662629982}},\n",
       "  'LR': {'dirty': {'MAE': 40.24055142704408, 'RMSE': 52.49827100690286},\n",
       "   'clean': {'MAE': 37.92614771342156, 'RMSE': 48.11541890635404}},\n",
       "  'KNN': {'dirty': {'MAE': 43.31700564971752, 'RMSE': 55.6396903806694},\n",
       "   'clean': {'MAE': 41.37488524590164, 'RMSE': 51.804771490601}},\n",
       "  'DT': {'dirty': {'MAE': 56.38418079096046, 'RMSE': 71.97626462971405},\n",
       "   'clean': {'MAE': 56.677978142076505, 'RMSE': 69.68877035480907}},\n",
       "  'GB': {'dirty': {'MAE': 44.99525294860506, 'RMSE': 57.41045473690478},\n",
       "   'clean': {'MAE': 42.88983909796805, 'RMSE': 53.3300650323363}}},\n",
       " 210: {'RF': {'dirty': {'MAE': 0.30757220779220773,\n",
       "    'RMSE': 0.4922253146071557},\n",
       "   'clean': {'MAE': 0.25391632034632033, 'RMSE': 0.3873000264378125}},\n",
       "  'LR': {'dirty': {'MAE': 0.2616177903487995, 'RMSE': 0.3853300754372834},\n",
       "   'clean': {'MAE': 0.26713746340928507, 'RMSE': 0.415765805820389}},\n",
       "  'KNN': {'dirty': {'MAE': 0.351369696969697, 'RMSE': 0.538638320329937},\n",
       "   'clean': {'MAE': 0.31188398268398265, 'RMSE': 0.47345070241864}},\n",
       "  'DT': {'dirty': {'MAE': 0.3738831168831169, 'RMSE': 0.5901600801021886},\n",
       "   'clean': {'MAE': 0.32482251082251085, 'RMSE': 0.49018359510196435}},\n",
       "  'GB': {'dirty': {'MAE': 0.29985835626820573, 'RMSE': 0.471201240591828},\n",
       "   'clean': {'MAE': 0.2803959474567427, 'RMSE': 0.4038351537664637}}},\n",
       " 560: {'RF': {'dirty': {'MAE': 0.49474807843137236, 'RMSE': 1.484630958660591},\n",
       "   'clean': {'MAE': 0.48932800000000076, 'RMSE': 1.4604340931050301}},\n",
       "  'LR': {'dirty': {'MAE': 0.5536131362567366, 'RMSE': 1.194448377957331},\n",
       "   'clean': {'MAE': 0.5875928223395658, 'RMSE': 1.228714016625616}},\n",
       "  'KNN': {'dirty': {'MAE': 4.999643921568627, 'RMSE': 6.00316606056365},\n",
       "   'clean': {'MAE': 4.9813600000000005, 'RMSE': 5.99283719987131}},\n",
       "  'DT': {'dirty': {'MAE': 0.7838588235294118, 'RMSE': 2.232643808566758},\n",
       "   'clean': {'MAE': 0.7567999999999999, 'RMSE': 2.302188516962936}},\n",
       "  'GB': {'dirty': {'MAE': 0.4776384647228536, 'RMSE': 1.3603088518948216},\n",
       "   'clean': {'MAE': 0.4859722669798214, 'RMSE': 1.3462141267800036}}},\n",
       " 491: {'RF': {'dirty': {'MAE': 0.6569333333333334, 'RMSE': 0.9045367970433787},\n",
       "   'clean': {'MAE': 0.6585643274853801, 'RMSE': 0.9287426712846314}},\n",
       "  'LR': {'dirty': {'MAE': 0.5972415039711748, 'RMSE': 0.7734838482665822},\n",
       "   'clean': {'MAE': 0.6031776208317068, 'RMSE': 0.7931450273237649}},\n",
       "  'KNN': {'dirty': {'MAE': 0.6293333333333333, 'RMSE': 0.8579821839975953},\n",
       "   'clean': {'MAE': 0.6529824561403509, 'RMSE': 0.8979588737642231}},\n",
       "  'DT': {'dirty': {'MAE': 0.72, 'RMSE': 1.134215120230095},\n",
       "   'clean': {'MAE': 0.7116959064327485, 'RMSE': 1.137072280338551}},\n",
       "  'GB': {'dirty': {'MAE': 0.6927632514807758, 'RMSE': 0.9767324926710685},\n",
       "   'clean': {'MAE': 0.7214156409519613, 'RMSE': 1.015497975185388}}},\n",
       " 566: {'RF': {'dirty': {'MAE': 202.80155787445574, 'RMSE': 685.5083933852063},\n",
       "   'clean': {'MAE': 12.955891909120211, 'RMSE': 17.336254117788105}},\n",
       "  'LR': {'dirty': {'MAE': 244.49978656741274, 'RMSE': 624.6378068264805},\n",
       "   'clean': {'MAE': 13.77906301192409, 'RMSE': 18.89823868392528}},\n",
       "  'KNN': {'dirty': {'MAE': 256.0108125689405, 'RMSE': 748.6741395613113},\n",
       "   'clean': {'MAE': 15.454668035938903, 'RMSE': 20.801717969066843}},\n",
       "  'DT': {'dirty': {'MAE': 263.524852467344, 'RMSE': 1015.1299794308248},\n",
       "   'clean': {'MAE': 19.426517969451933, 'RMSE': 27.398236717510223}},\n",
       "  'GB': {'dirty': {'MAE': 214.32299093692532, 'RMSE': 727.844276851295},\n",
       "   'clean': {'MAE': 14.780582239888517, 'RMSE': 20.62517480232636}}},\n",
       " 189: {'RF': {'dirty': {'MAE': 0.11150637801999533,\n",
       "    'RMSE': 0.14175088027866942},\n",
       "   'clean': {'MAE': 0.111710057315673, 'RMSE': 0.14147281267814701}},\n",
       "  'LR': {'dirty': {'MAE': 0.16234827203460017, 'RMSE': 0.20217056507709522},\n",
       "   'clean': {'MAE': 0.16212927151365192, 'RMSE': 0.2018633309382883}},\n",
       "  'KNN': {'dirty': {'MAE': 0.09419607988844171, 'RMSE': 0.12052876101013983},\n",
       "   'clean': {'MAE': 0.09406398867735835, 'RMSE': 0.12028187632199179}},\n",
       "  'DT': {'dirty': {'MAE': 0.1633806145435878, 'RMSE': 0.21420002856694467},\n",
       "   'clean': {'MAE': 0.15968703175130425, 'RMSE': 0.2105913134635508}},\n",
       "  'GB': {'dirty': {'MAE': 0.1397809263981544, 'RMSE': 0.17627256057441382},\n",
       "   'clean': {'MAE': 0.13922967009974227, 'RMSE': 0.17538311466841114}}},\n",
       " 673: {'RF': {'dirty': {'MAE': 0.4152935250000002, 'RMSE': 0.5170547361579618},\n",
       "   'clean': {'MAE': 0.4137504804430384, 'RMSE': 0.5180785918572777}},\n",
       "  'LR': {'dirty': {'MAE': 0.4077110483426455, 'RMSE': 0.5122182236381942},\n",
       "   'clean': {'MAE': 0.40871868935895944, 'RMSE': 0.5140552393594947}},\n",
       "  'KNN': {'dirty': {'MAE': 0.42438325, 'RMSE': 0.5341189758496008},\n",
       "   'clean': {'MAE': 0.4246818765822784, 'RMSE': 0.5330127172515785}},\n",
       "  'DT': {'dirty': {'MAE': 0.5468234999999999, 'RMSE': 0.6903438469783213},\n",
       "   'clean': {'MAE': 0.5680274651898736, 'RMSE': 0.7115871726761884}},\n",
       "  'GB': {'dirty': {'MAE': 0.43138166609718986, 'RMSE': 0.5413809081517701},\n",
       "   'clean': {'MAE': 0.421065359171954, 'RMSE': 0.5365960053884473}}},\n",
       " 639: {'RF': {'dirty': {'MAE': 0.6662963750197999, 'RMSE': 0.8518033416598023},\n",
       "   'clean': {'MAE': 0.6693870330299999, 'RMSE': 0.856919065507863}},\n",
       "  'LR': {'dirty': {'MAE': 0.9669260349020506, 'RMSE': 1.1573080983015818},\n",
       "   'clean': {'MAE': 0.9669260349020506, 'RMSE': 1.1573080983015818}},\n",
       "  'KNN': {'dirty': {'MAE': 0.724847294808, 'RMSE': 0.915286391163944},\n",
       "   'clean': {'MAE': 0.724847294808, 'RMSE': 0.915286391163944}},\n",
       "  'DT': {'dirty': {'MAE': 0.88410993539, 'RMSE': 1.2142512138820982},\n",
       "   'clean': {'MAE': 0.87891647789, 'RMSE': 1.162012681132334}},\n",
       "  'GB': {'dirty': {'MAE': 0.5912025209966958, 'RMSE': 0.788614075997609},\n",
       "   'clean': {'MAE': 0.5951605309657222, 'RMSE': 0.7877236520507148}}},\n",
       " 41700: {'RF': {'dirty': {'MAE': 281.50970991661694,\n",
       "    'RMSE': 486.1335649991769},\n",
       "   'clean': {'MAE': 275.64082660951243, 'RMSE': 472.7973232770026}},\n",
       "  'LR': {'dirty': {'MAE': 1274.6527300007515, 'RMSE': 1467.9870240944845},\n",
       "   'clean': {'MAE': 1259.516763761364, 'RMSE': 1523.582188163416}},\n",
       "  'KNN': {'dirty': {'MAE': 1403.924856367034, 'RMSE': 1690.8854150831219},\n",
       "   'clean': {'MAE': 1345.5949520538695, 'RMSE': 1709.4839532109459}},\n",
       "  'DT': {'dirty': {'MAE': 353.5094536175434, 'RMSE': 776.5748885224269},\n",
       "   'clean': {'MAE': 388.56717442663944, 'RMSE': 805.7719837233692}},\n",
       "  'GB': {'dirty': {'MAE': 262.5468216530609, 'RMSE': 467.1512929779313},\n",
       "   'clean': {'MAE': 260.2683258763069, 'RMSE': 456.03336420904924}}},\n",
       " 44: {'RF': {'dirty': {'F1': 0.9279878569571107,\n",
       "    'Accuracy': 0.9278385969881509},\n",
       "   'clean': {'F1': 0.8964403252210127, 'Accuracy': 0.897031200756382}},\n",
       "  'LR': {'dirty': {'F1': 0.9005276194445833, 'Accuracy': 0.9002332058726337},\n",
       "   'clean': {'F1': 0.8522383742028496, 'Accuracy': 0.8536870756095464}},\n",
       "  'KNN': {'dirty': {'F1': 0.7718712391555475, 'Accuracy': 0.7724458292026626},\n",
       "   'clean': {'F1': 0.7425700193148611, 'Accuracy': 0.7437781852562818}},\n",
       "  'DT': {'dirty': {'F1': 0.882930796602855, 'Accuracy': 0.8826294670254449},\n",
       "   'clean': {'F1': 0.8423649800551634, 'Accuracy': 0.8417124602469702}},\n",
       "  'GB': {'dirty': {'F1': 0.9303120762499993, 'Accuracy': 0.9302294292593117},\n",
       "   'clean': {'F1': 0.8897095155024571, 'Accuracy': 0.8900833739220124}}},\n",
       " 38: {'RF': {'dirty': {'F1': 0.9081468657889051,\n",
       "    'Accuracy': 0.9358448539357423},\n",
       "   'clean': {'F1': 0.9843156759654121, 'Accuracy': 0.9848693259972489}},\n",
       "  'LR': {'dirty': {'F1': 0.9091064835226215, 'Accuracy': 0.938759463874787},\n",
       "   'clean': {'F1': 0.9556194532935253, 'Accuracy': 0.9592847317744153}},\n",
       "  'KNN': {'dirty': {'F1': 0.9090728146914586, 'Accuracy': 0.9339891439914277},\n",
       "   'clean': {'F1': 0.9100308907762816, 'Accuracy': 0.9303988995873453}},\n",
       "  'DT': {'dirty': {'F1': 0.9073810260514821, 'Accuracy': 0.9353143499569624},\n",
       "   'clean': {'F1': 0.98391259986163, 'Accuracy': 0.9840440165061898}},\n",
       "  'GB': {'dirty': {'F1': 0.9085447265721529, 'Accuracy': 0.9366388532682206},\n",
       "   'clean': {'F1': 0.9848207736553135, 'Accuracy': 0.9851444291609353}}},\n",
       " 458: {'RF': {'dirty': {'F1': 0.9821284768557158,\n",
       "    'Accuracy': 0.9821499013806706},\n",
       "   'clean': {'F1': 0.9382503744460877, 'Accuracy': 0.9411009306687828}},\n",
       "  'LR': {'dirty': {'F1': 0.990466401004207, 'Accuracy': 0.990483234714004},\n",
       "   'clean': {'F1': 0.9091538518438144, 'Accuracy': 0.9123223432652768}},\n",
       "  'KNN': {'dirty': {'F1': 0.9832747918895606, 'Accuracy': 0.9833474218089602},\n",
       "   'clean': {'F1': 0.9323403280691286, 'Accuracy': 0.9362744390736599}},\n",
       "  'DT': {'dirty': {'F1': 0.8188131307809362, 'Accuracy': 0.8276697661313046},\n",
       "   'clean': {'F1': 0.8937703572275856, 'Accuracy': 0.8955198037659621}},\n",
       "  'GB': {'dirty': {'F1': 0.9319869767312776, 'Accuracy': 0.9182093547478163},\n",
       "   'clean': {'F1': 0.9258218070316705, 'Accuracy': 0.9279272779741721}}},\n",
       " 1053: {'RF': {'dirty': {'F1': 0.7583147103298538,\n",
       "    'Accuracy': 0.7936580882352942},\n",
       "   'clean': {'F1': 0.8596028788965306, 'Accuracy': 0.8699958239914238}},\n",
       "  'LR': {'dirty': {'F1': 0.7300614252021966, 'Accuracy': 0.7614889705882353},\n",
       "   'clean': {'F1': 0.8102955385715017, 'Accuracy': 0.8199984976554514}},\n",
       "  'KNN': {'dirty': {'F1': 0.7357552027313892, 'Accuracy': 0.7590073529411764},\n",
       "   'clean': {'F1': 0.8214165387146716, 'Accuracy': 0.8382824764240127}},\n",
       "  'DT': {'dirty': {'F1': 0.7196106504429597, 'Accuracy': 0.7134191176470588},\n",
       "   'clean': {'F1': 0.825659541171716, 'Accuracy': 0.8233878760794917}},\n",
       "  'GB': {'dirty': {'F1': 0.7542196382721448, 'Accuracy': 0.8042279411764707},\n",
       "   'clean': {'F1': 0.8568130224939713, 'Accuracy': 0.8698837210782759}}},\n",
       " 1050: {'RF': {'dirty': {'F1': 0.8766204497594787,\n",
       "    'Accuracy': 0.9021155894159089},\n",
       "   'clean': {'F1': 0.9135246811245556, 'Accuracy': 0.9283666913763282}},\n",
       "  'LR': {'dirty': {'F1': 0.8686606726176752, 'Accuracy': 0.8950745473908415},\n",
       "   'clean': {'F1': 0.9023414972634514, 'Accuracy': 0.9164269829503336}},\n",
       "  'KNN': {'dirty': {'F1': 0.844711517573254, 'Accuracy': 0.8777975751617924},\n",
       "   'clean': {'F1': 0.8854167146628582, 'Accuracy': 0.908712626637015}},\n",
       "  'DT': {'dirty': {'F1': 0.8563030047575912, 'Accuracy': 0.851595396084214},\n",
       "   'clean': {'F1': 0.9068612741020792, 'Accuracy': 0.9072942920681987}},\n",
       "  'GB': {'dirty': {'F1': 0.8769541971064634, 'Accuracy': 0.8963504546571638},\n",
       "   'clean': {'F1': 0.9216903132770555, 'Accuracy': 0.92976772918211}}},\n",
       " 2: {'RF': {'dirty': {'F1': 0.8920120055670319,\n",
       "    'Accuracy': 0.8953258845437617},\n",
       "   'clean': {'F1': 0.996985548683238, 'Accuracy': 0.9977272727272727}},\n",
       "  'LR': {'dirty': {'F1': 0.6918796974024323, 'Accuracy': 0.7595034140285537},\n",
       "   'clean': {'F1': 0.7458563308451799, 'Accuracy': 0.8170454545454545}},\n",
       "  'KNN': {'dirty': {'F1': 0.8007172081513494, 'Accuracy': 0.8150962135319677},\n",
       "   'clean': {'F1': 0.8911677656677099, 'Accuracy': 0.9}},\n",
       "  'DT': {'dirty': {'F1': 0.8747817056599075, 'Accuracy': 0.8775046554934823},\n",
       "   'clean': {'F1': 0.9983093126385809, 'Accuracy': 0.9988636363636363}},\n",
       "  'GB': {'dirty': {'F1': 0.9048981777026578, 'Accuracy': 0.9087026691495966},\n",
       "   'clean': {'F1': 0.9969981238273921, 'Accuracy': 0.9977272727272727}}},\n",
       " 3: {'RF': {'dirty': {'F1': 0.9440280732068139, 'Accuracy': 0.944319248826291},\n",
       "   'clean': {'F1': 0.9527251330633133, 'Accuracy': 0.9529534504903964}},\n",
       "  'LR': {'dirty': {'F1': 0.9328378384318651, 'Accuracy': 0.9333788145539905},\n",
       "   'clean': {'F1': 0.9330783319748797, 'Accuracy': 0.9338110165696373}},\n",
       "  'KNN': {'dirty': {'F1': 0.7791189100321267, 'Accuracy': 0.7822368935837245},\n",
       "   'clean': {'F1': 0.7813511611464241, 'Accuracy': 0.7844534775569258}},\n",
       "  'DT': {'dirty': {'F1': 0.9768224042859567, 'Accuracy': 0.976847613458529},\n",
       "   'clean': {'F1': 0.9836716253566429, 'Accuracy': 0.9836828196434109}},\n",
       "  'GB': {'dirty': {'F1': 0.9382860184891113, 'Accuracy': 0.9386962050078248},\n",
       "   'clean': {'F1': 0.9500613819988283, 'Accuracy': 0.9504416765500509}}},\n",
       " 5: {'RF': {'dirty': {'F1': 0.6520368010602652,\n",
       "    'Accuracy': 0.7213431013431013},\n",
       "   'clean': {'F1': 0.6927699985874768, 'Accuracy': 0.7545054945054945}},\n",
       "  'LR': {'dirty': {'F1': 0.6116265607204437, 'Accuracy': 0.6152136752136752},\n",
       "   'clean': {'F1': 0.6828086933440345, 'Accuracy': 0.7035897435897436}},\n",
       "  'KNN': {'dirty': {'F1': 0.5077211583139432, 'Accuracy': 0.6173626373626373},\n",
       "   'clean': {'F1': 0.5090272165543549, 'Accuracy': 0.623956043956044}},\n",
       "  'DT': {'dirty': {'F1': 0.561830020174423, 'Accuracy': 0.5596581196581196},\n",
       "   'clean': {'F1': 0.6812682787953648, 'Accuracy': 0.6834676434676434}},\n",
       "  'GB': {'dirty': {'F1': 0.6519210133931621, 'Accuracy': 0.6947985347985348},\n",
       "   'clean': {'F1': 0.7178791906611673, 'Accuracy': 0.7321367521367522}}},\n",
       " 31: {'RF': {'dirty': {'F1': 0.7374403699301784,\n",
       "    'Accuracy': 0.7569999999999999},\n",
       "   'clean': {'F1': 0.8360545123161316, 'Accuracy': 0.844}},\n",
       "  'LR': {'dirty': {'F1': 0.689011654311023, 'Accuracy': 0.7149999999999999},\n",
       "   'clean': {'F1': 0.7298809196452336, 'Accuracy': 0.7470000000000001}},\n",
       "  'KNN': {'dirty': {'F1': 0.6198889608514138, 'Accuracy': 0.651},\n",
       "   'clean': {'F1': 0.6372229792119773, 'Accuracy': 0.668}},\n",
       "  'DT': {'dirty': {'F1': 0.686501491611497, 'Accuracy': 0.6839999999999999},\n",
       "   'clean': {'F1': 0.7520888902794258, 'Accuracy': 0.75}},\n",
       "  'GB': {'dirty': {'F1': 0.7404224772795408, 'Accuracy': 0.754},\n",
       "   'clean': {'F1': 0.8294350493973136, 'Accuracy': 0.8320000000000001}}},\n",
       " 1464: {'RF': {'dirty': {'F1': 0.6381033352240261,\n",
       "    'Accuracy': 0.6659418344519016},\n",
       "   'clean': {'F1': 0.5968595098460269, 'Accuracy': 0.6242673992673993}},\n",
       "  'LR': {'dirty': {'F1': 0.7122678138558042, 'Accuracy': 0.7727874720357942},\n",
       "   'clean': {'F1': 0.6977366512728395, 'Accuracy': 0.7272710622710623}},\n",
       "  'KNN': {'dirty': {'F1': 0.6416965949556144, 'Accuracy': 0.6673914988814318},\n",
       "   'clean': {'F1': 0.5767765585572798, 'Accuracy': 0.5956776556776557}},\n",
       "  'DT': {'dirty': {'F1': 0.6033135709680766, 'Accuracy': 0.6112125279642059},\n",
       "   'clean': {'F1': 0.5958500801274317, 'Accuracy': 0.6127655677655678}},\n",
       "  'GB': {'dirty': {'F1': 0.6719617030655909, 'Accuracy': 0.7113288590604027},\n",
       "   'clean': {'F1': 0.661914659649328, 'Accuracy': 0.6852747252747253}}}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the saved performances for all datasets\n",
    "with open(\"data/performances_datasets_dirty_clean.pkl\", 'rb') as f:\n",
    "    downstream_perfs_dct = pickle.load(f)\n",
    "downstream_perfs_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe30b6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirty_rf_f1</th>\n",
       "      <th>dirty_rf_acc</th>\n",
       "      <th>dirty_lr_f1</th>\n",
       "      <th>dirty_lr_acc</th>\n",
       "      <th>dirty_knn_f1</th>\n",
       "      <th>dirty_knn_acc</th>\n",
       "      <th>dirty_dt_f1</th>\n",
       "      <th>dirty_dt_acc</th>\n",
       "      <th>dirty_gb_f1</th>\n",
       "      <th>dirty_gb_acc</th>\n",
       "      <th>clean_rf_f1</th>\n",
       "      <th>clean_rf_acc</th>\n",
       "      <th>clean_lr_f1</th>\n",
       "      <th>clean_lr_acc</th>\n",
       "      <th>clean_knn_f1</th>\n",
       "      <th>clean_knn_acc</th>\n",
       "      <th>clean_dt_f1</th>\n",
       "      <th>clean_dt_acc</th>\n",
       "      <th>clean_gb_f1</th>\n",
       "      <th>clean_gb_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dirty_rf_f1  dirty_rf_acc  dirty_lr_f1  dirty_lr_acc  dirty_knn_f1  \\\n",
       "44           0.93          0.93         0.90          0.90          0.77   \n",
       "38           0.91          0.94         0.91          0.94          0.91   \n",
       "458          0.98          0.98         0.99          0.99          0.98   \n",
       "1053         0.76          0.79         0.73          0.76          0.74   \n",
       "1050         0.88          0.90         0.87          0.90          0.84   \n",
       "2            0.89          0.90         0.69          0.76          0.80   \n",
       "3            0.94          0.94         0.93          0.93          0.78   \n",
       "5            0.65          0.72         0.61          0.62          0.51   \n",
       "31           0.74          0.76         0.69          0.71          0.62   \n",
       "1464         0.64          0.67         0.71          0.77          0.64   \n",
       "\n",
       "      dirty_knn_acc  dirty_dt_f1  dirty_dt_acc  dirty_gb_f1  dirty_gb_acc  \\\n",
       "44             0.77         0.88          0.88         0.93          0.93   \n",
       "38             0.93         0.91          0.94         0.91          0.94   \n",
       "458            0.98         0.82          0.83         0.93          0.92   \n",
       "1053           0.76         0.72          0.71         0.75          0.80   \n",
       "1050           0.88         0.86          0.85         0.88          0.90   \n",
       "2              0.82         0.87          0.88         0.90          0.91   \n",
       "3              0.78         0.98          0.98         0.94          0.94   \n",
       "5              0.62         0.56          0.56         0.65          0.69   \n",
       "31             0.65         0.69          0.68         0.74          0.75   \n",
       "1464           0.67         0.60          0.61         0.67          0.71   \n",
       "\n",
       "      clean_rf_f1  clean_rf_acc  clean_lr_f1  clean_lr_acc  clean_knn_f1  \\\n",
       "44           0.90          0.90         0.85          0.85          0.74   \n",
       "38           0.98          0.98         0.96          0.96          0.91   \n",
       "458          0.94          0.94         0.91          0.91          0.93   \n",
       "1053         0.86          0.87         0.81          0.82          0.82   \n",
       "1050         0.91          0.93         0.90          0.92          0.89   \n",
       "2            1.00          1.00         0.75          0.82          0.89   \n",
       "3            0.95          0.95         0.93          0.93          0.78   \n",
       "5            0.69          0.75         0.68          0.70          0.51   \n",
       "31           0.84          0.84         0.73          0.75          0.64   \n",
       "1464         0.60          0.62         0.70          0.73          0.58   \n",
       "\n",
       "      clean_knn_acc  clean_dt_f1  clean_dt_acc  clean_gb_f1  clean_gb_acc  \n",
       "44             0.74         0.84          0.84         0.89          0.89  \n",
       "38             0.93         0.98          0.98         0.98          0.99  \n",
       "458            0.94         0.89          0.90         0.93          0.93  \n",
       "1053           0.84         0.83          0.82         0.86          0.87  \n",
       "1050           0.91         0.91          0.91         0.92          0.93  \n",
       "2              0.90         1.00          1.00         1.00          1.00  \n",
       "3              0.78         0.98          0.98         0.95          0.95  \n",
       "5              0.62         0.68          0.68         0.72          0.73  \n",
       "31             0.67         0.75          0.75         0.83          0.83  \n",
       "1464           0.60         0.60          0.61         0.66          0.69  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with all performances for all datasets for all models for all measures for classification task\n",
    "classification = pd.DataFrame(columns = [\n",
    "    'dirty_rf_f1', 'dirty_rf_acc',\n",
    "    'dirty_lr_f1', 'dirty_lr_acc',\n",
    "    'dirty_knn_f1', 'dirty_knn_acc',\n",
    "    'dirty_dt_f1', 'dirty_dt_acc',\n",
    "    'dirty_gb_f1', 'dirty_gb_acc',\n",
    "    'clean_rf_f1', 'clean_rf_acc',\n",
    "    'clean_lr_f1', 'clean_lr_acc',\n",
    "    'clean_knn_f1', 'clean_knn_acc',\n",
    "    'clean_dt_f1', 'clean_dt_acc',\n",
    "    'clean_gb_f1', 'clean_gb_acc'\n",
    "])\n",
    "for key, value in downstream_perfs_dct.items():\n",
    "    if key in classification_datasets:\n",
    "        dirty = []\n",
    "        clean = []\n",
    "        for model, perfs in value.items():\n",
    "            dirty_f1 = perfs['dirty']['F1']\n",
    "            dirty_acc = perfs['dirty']['Accuracy']\n",
    "            dirty.append(round(dirty_f1,2))\n",
    "            dirty.append(round(dirty_acc,2))\n",
    "            clean_f1 = perfs['clean']['F1']\n",
    "            clean_acc = perfs['clean']['Accuracy']\n",
    "            clean.append(round(clean_f1,2))\n",
    "            clean.append(round(clean_acc,2))\n",
    "        all_perfs = dirty + clean\n",
    "        classification.loc[key] = all_perfs\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a2e0fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirty_rf_f1</th>\n",
       "      <th>dirty_rf_acc</th>\n",
       "      <th>dirty_lr_f1</th>\n",
       "      <th>dirty_lr_acc</th>\n",
       "      <th>dirty_knn_f1</th>\n",
       "      <th>dirty_knn_acc</th>\n",
       "      <th>dirty_dt_f1</th>\n",
       "      <th>dirty_dt_acc</th>\n",
       "      <th>dirty_gb_f1</th>\n",
       "      <th>dirty_gb_acc</th>\n",
       "      <th>clean_rf_f1</th>\n",
       "      <th>clean_rf_acc</th>\n",
       "      <th>clean_lr_f1</th>\n",
       "      <th>clean_lr_acc</th>\n",
       "      <th>clean_knn_f1</th>\n",
       "      <th>clean_knn_acc</th>\n",
       "      <th>clean_dt_f1</th>\n",
       "      <th>clean_dt_acc</th>\n",
       "      <th>clean_gb_f1</th>\n",
       "      <th>clean_gb_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dirty_rf_f1  dirty_rf_acc  dirty_lr_f1  dirty_lr_acc  dirty_knn_f1  \\\n",
       "2            0.89          0.90         0.69          0.76          0.80   \n",
       "3            0.94          0.94         0.93          0.93          0.78   \n",
       "5            0.65          0.72         0.61          0.62          0.51   \n",
       "31           0.74          0.76         0.69          0.71          0.62   \n",
       "38           0.91          0.94         0.91          0.94          0.91   \n",
       "44           0.93          0.93         0.90          0.90          0.77   \n",
       "458          0.98          0.98         0.99          0.99          0.98   \n",
       "1050         0.88          0.90         0.87          0.90          0.84   \n",
       "1053         0.76          0.79         0.73          0.76          0.74   \n",
       "1464         0.64          0.67         0.71          0.77          0.64   \n",
       "\n",
       "      dirty_knn_acc  dirty_dt_f1  dirty_dt_acc  dirty_gb_f1  dirty_gb_acc  \\\n",
       "2              0.82         0.87          0.88         0.90          0.91   \n",
       "3              0.78         0.98          0.98         0.94          0.94   \n",
       "5              0.62         0.56          0.56         0.65          0.69   \n",
       "31             0.65         0.69          0.68         0.74          0.75   \n",
       "38             0.93         0.91          0.94         0.91          0.94   \n",
       "44             0.77         0.88          0.88         0.93          0.93   \n",
       "458            0.98         0.82          0.83         0.93          0.92   \n",
       "1050           0.88         0.86          0.85         0.88          0.90   \n",
       "1053           0.76         0.72          0.71         0.75          0.80   \n",
       "1464           0.67         0.60          0.61         0.67          0.71   \n",
       "\n",
       "      clean_rf_f1  clean_rf_acc  clean_lr_f1  clean_lr_acc  clean_knn_f1  \\\n",
       "2            1.00          1.00         0.75          0.82          0.89   \n",
       "3            0.95          0.95         0.93          0.93          0.78   \n",
       "5            0.69          0.75         0.68          0.70          0.51   \n",
       "31           0.84          0.84         0.73          0.75          0.64   \n",
       "38           0.98          0.98         0.96          0.96          0.91   \n",
       "44           0.90          0.90         0.85          0.85          0.74   \n",
       "458          0.94          0.94         0.91          0.91          0.93   \n",
       "1050         0.91          0.93         0.90          0.92          0.89   \n",
       "1053         0.86          0.87         0.81          0.82          0.82   \n",
       "1464         0.60          0.62         0.70          0.73          0.58   \n",
       "\n",
       "      clean_knn_acc  clean_dt_f1  clean_dt_acc  clean_gb_f1  clean_gb_acc  \n",
       "2              0.90         1.00          1.00         1.00          1.00  \n",
       "3              0.78         0.98          0.98         0.95          0.95  \n",
       "5              0.62         0.68          0.68         0.72          0.73  \n",
       "31             0.67         0.75          0.75         0.83          0.83  \n",
       "38             0.93         0.98          0.98         0.98          0.99  \n",
       "44             0.74         0.84          0.84         0.89          0.89  \n",
       "458            0.94         0.89          0.90         0.93          0.93  \n",
       "1050           0.91         0.91          0.91         0.92          0.93  \n",
       "1053           0.84         0.83          0.82         0.86          0.87  \n",
       "1464           0.60         0.60          0.61         0.66          0.69  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29a56223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 44:  0.88\n",
      "Acc 44:  0.88\n",
      "F1 38:  0.91\n",
      "Acc 38:  0.94\n",
      "F1 458:  0.94\n",
      "Acc 458:  0.94\n",
      "F1 1053:  0.74\n",
      "Acc 1053:  0.76\n",
      "F1 1050:  0.87\n",
      "Acc 1050:  0.89\n",
      "F1 2:  0.83\n",
      "Acc 2:  0.85\n",
      "F1 3:  0.91\n",
      "Acc 3:  0.91\n",
      "F1 5:  0.6\n",
      "Acc 5:  0.64\n",
      "F1 31:  0.7\n",
      "Acc 31:  0.71\n",
      "F1 1464:  0.65\n",
      "Acc 1464:  0.69\n"
     ]
    }
   ],
   "source": [
    "# Calculate average scores per dataset for dirty datasets\n",
    "for idx in classification.index:\n",
    "    print(f\"F1 {idx}: \", round(mean(classification.loc[idx][['dirty_rf_f1', 'dirty_lr_f1', 'dirty_knn_f1', 'dirty_dt_f1', 'dirty_gb_f1']]),2))\n",
    "    print(f\"Acc {idx}: \", round(mean(classification.loc[idx][['dirty_rf_acc', 'dirty_lr_acc', 'dirty_knn_acc', 'dirty_dt_acc', 'dirty_gb_acc']]),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f19d49ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 44:  0.84\n",
      "Acc 44:  0.84\n",
      "F1 38:  0.96\n",
      "Acc 38:  0.97\n",
      "F1 458:  0.92\n",
      "Acc 458:  0.92\n",
      "F1 1053:  0.84\n",
      "Acc 1053:  0.84\n",
      "F1 1050:  0.91\n",
      "Acc 1050:  0.92\n",
      "F1 2:  0.93\n",
      "Acc 2:  0.94\n",
      "F1 3:  0.92\n",
      "Acc 3:  0.92\n",
      "F1 5:  0.66\n",
      "Acc 5:  0.7\n",
      "F1 31:  0.76\n",
      "Acc 31:  0.77\n",
      "F1 1464:  0.63\n",
      "Acc 1464:  0.65\n"
     ]
    }
   ],
   "source": [
    "# Calculate average scores per dataset for clean datasets\n",
    "for idx in classification.index:\n",
    "    print(f\"F1 {idx}: \", round(mean(classification.loc[idx][['clean_rf_f1', 'clean_lr_f1', 'clean_knn_f1', 'clean_dt_f1', 'clean_gb_f1']]), 2))\n",
    "    print(f\"Acc {idx}: \", round(mean(classification.loc[idx][['clean_rf_acc', 'clean_lr_acc', 'clean_knn_acc', 'clean_dt_acc', 'clean_gb_acc']]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a8976c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_9812\\4218847766.py:1: UserWarning:\n",
      "\n",
      "Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Store the performances in an excel file\n",
    "classification.to_excel('data/perfs_cv_class.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "749d8017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirty_rf_mae</th>\n",
       "      <th>dirty_rf_rmse</th>\n",
       "      <th>dirty_lr_mae</th>\n",
       "      <th>dirty_lr_rmse</th>\n",
       "      <th>dirty_knn_mae</th>\n",
       "      <th>dirty_knn_rmse</th>\n",
       "      <th>dirty_dt_mae</th>\n",
       "      <th>dirty_dt_rmse</th>\n",
       "      <th>dirty_gb_mae</th>\n",
       "      <th>dirty_gb_rmse</th>\n",
       "      <th>clean_rf_mae</th>\n",
       "      <th>clean_rf_rmse</th>\n",
       "      <th>clean_lr_mae</th>\n",
       "      <th>clean_lr_rmse</th>\n",
       "      <th>clean_knn_mae</th>\n",
       "      <th>clean_knn_rmse</th>\n",
       "      <th>clean_dt_mae</th>\n",
       "      <th>clean_dt_rmse</th>\n",
       "      <th>clean_gb_mae</th>\n",
       "      <th>clean_gb_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.88</td>\n",
       "      <td>3.41</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.69</td>\n",
       "      <td>4.69</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.28</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.52</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>42.81</td>\n",
       "      <td>54.76</td>\n",
       "      <td>40.24</td>\n",
       "      <td>52.50</td>\n",
       "      <td>43.32</td>\n",
       "      <td>55.64</td>\n",
       "      <td>56.38</td>\n",
       "      <td>71.98</td>\n",
       "      <td>45.00</td>\n",
       "      <td>57.41</td>\n",
       "      <td>40.26</td>\n",
       "      <td>50.33</td>\n",
       "      <td>37.93</td>\n",
       "      <td>48.12</td>\n",
       "      <td>41.37</td>\n",
       "      <td>51.80</td>\n",
       "      <td>56.68</td>\n",
       "      <td>69.69</td>\n",
       "      <td>42.89</td>\n",
       "      <td>53.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0.49</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.19</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.23</td>\n",
       "      <td>4.98</td>\n",
       "      <td>5.99</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>202.80</td>\n",
       "      <td>685.51</td>\n",
       "      <td>244.50</td>\n",
       "      <td>624.64</td>\n",
       "      <td>256.01</td>\n",
       "      <td>748.67</td>\n",
       "      <td>263.52</td>\n",
       "      <td>1015.13</td>\n",
       "      <td>214.32</td>\n",
       "      <td>727.84</td>\n",
       "      <td>12.96</td>\n",
       "      <td>17.34</td>\n",
       "      <td>13.78</td>\n",
       "      <td>18.90</td>\n",
       "      <td>15.45</td>\n",
       "      <td>20.80</td>\n",
       "      <td>19.43</td>\n",
       "      <td>27.40</td>\n",
       "      <td>14.78</td>\n",
       "      <td>20.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41700</th>\n",
       "      <td>281.51</td>\n",
       "      <td>486.13</td>\n",
       "      <td>1274.65</td>\n",
       "      <td>1467.99</td>\n",
       "      <td>1403.92</td>\n",
       "      <td>1690.89</td>\n",
       "      <td>353.51</td>\n",
       "      <td>776.57</td>\n",
       "      <td>262.55</td>\n",
       "      <td>467.15</td>\n",
       "      <td>275.64</td>\n",
       "      <td>472.80</td>\n",
       "      <td>1259.52</td>\n",
       "      <td>1523.58</td>\n",
       "      <td>1345.59</td>\n",
       "      <td>1709.48</td>\n",
       "      <td>388.57</td>\n",
       "      <td>805.77</td>\n",
       "      <td>260.27</td>\n",
       "      <td>456.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dirty_rf_mae  dirty_rf_rmse  dirty_lr_mae  dirty_lr_rmse  \\\n",
       "8              2.88           3.41          2.80           3.36   \n",
       "204           42.81          54.76         40.24          52.50   \n",
       "210            0.31           0.49          0.26           0.39   \n",
       "560            0.49           1.48          0.55           1.19   \n",
       "491            0.66           0.90          0.60           0.77   \n",
       "566          202.80         685.51        244.50         624.64   \n",
       "189            0.11           0.14          0.16           0.20   \n",
       "673            0.42           0.52          0.41           0.51   \n",
       "639            0.67           0.85          0.97           1.16   \n",
       "41700        281.51         486.13       1274.65        1467.99   \n",
       "\n",
       "       dirty_knn_mae  dirty_knn_rmse  dirty_dt_mae  dirty_dt_rmse  \\\n",
       "8               3.02            3.65          3.69           4.69   \n",
       "204            43.32           55.64         56.38          71.98   \n",
       "210             0.35            0.54          0.37           0.59   \n",
       "560             5.00            6.00          0.78           2.23   \n",
       "491             0.63            0.86          0.72           1.13   \n",
       "566           256.01          748.67        263.52        1015.13   \n",
       "189             0.09            0.12          0.16           0.21   \n",
       "673             0.42            0.53          0.55           0.69   \n",
       "639             0.72            0.92          0.88           1.21   \n",
       "41700        1403.92         1690.89        353.51         776.57   \n",
       "\n",
       "       dirty_gb_mae  dirty_gb_rmse  clean_rf_mae  clean_rf_rmse  clean_lr_mae  \\\n",
       "8              2.88           3.45          2.87           3.40          2.73   \n",
       "204           45.00          57.41         40.26          50.33         37.93   \n",
       "210            0.30           0.47          0.25           0.39          0.27   \n",
       "560            0.48           1.36          0.49           1.46          0.59   \n",
       "491            0.69           0.98          0.66           0.93          0.60   \n",
       "566          214.32         727.84         12.96          17.34         13.78   \n",
       "189            0.14           0.18          0.11           0.14          0.16   \n",
       "673            0.43           0.54          0.41           0.52          0.41   \n",
       "639            0.59           0.79          0.67           0.86          0.97   \n",
       "41700        262.55         467.15        275.64         472.80       1259.52   \n",
       "\n",
       "       clean_lr_rmse  clean_knn_mae  clean_knn_rmse  clean_dt_mae  \\\n",
       "8               3.28           3.10            3.71          3.52   \n",
       "204            48.12          41.37           51.80         56.68   \n",
       "210             0.42           0.31            0.47          0.32   \n",
       "560             1.23           4.98            5.99          0.76   \n",
       "491             0.79           0.65            0.90          0.71   \n",
       "566            18.90          15.45           20.80         19.43   \n",
       "189             0.20           0.09            0.12          0.16   \n",
       "673             0.51           0.42            0.53          0.57   \n",
       "639             1.16           0.72            0.92          0.88   \n",
       "41700        1523.58        1345.59         1709.48        388.57   \n",
       "\n",
       "       clean_dt_rmse  clean_gb_mae  clean_gb_rmse  \n",
       "8               4.35          2.90           3.46  \n",
       "204            69.69         42.89          53.33  \n",
       "210             0.49          0.28           0.40  \n",
       "560             2.30          0.49           1.35  \n",
       "491             1.14          0.72           1.02  \n",
       "566            27.40         14.78          20.63  \n",
       "189             0.21          0.14           0.18  \n",
       "673             0.71          0.42           0.54  \n",
       "639             1.16          0.60           0.79  \n",
       "41700         805.77        260.27         456.03  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with all performances for all datasets for all models for all measures for regression task\n",
    "regression = pd.DataFrame(columns = [\n",
    "    'dirty_rf_mae', 'dirty_rf_rmse',\n",
    "    'dirty_lr_mae', 'dirty_lr_rmse',\n",
    "    'dirty_knn_mae', 'dirty_knn_rmse',\n",
    "    'dirty_dt_mae', 'dirty_dt_rmse',\n",
    "    'dirty_gb_mae', 'dirty_gb_rmse',\n",
    "    'clean_rf_mae', 'clean_rf_rmse',\n",
    "    'clean_lr_mae', 'clean_lr_rmse',\n",
    "    'clean_knn_mae', 'clean_knn_rmse',\n",
    "    'clean_dt_mae', 'clean_dt_rmse',\n",
    "    'clean_gb_mae', 'clean_gb_rmse'\n",
    "])\n",
    "\n",
    "for key, value in downstream_perfs_dct.items():\n",
    "    if key in regression_datasets:\n",
    "        dirty = []\n",
    "        clean = []\n",
    "        for model, perfs in value.items():\n",
    "            dirty_mae = perfs['dirty']['MAE']\n",
    "            dirty_rmse = perfs['dirty']['RMSE']\n",
    "            dirty.append(round(dirty_mae, 2))\n",
    "            dirty.append(round(dirty_rmse, 2))\n",
    "            clean_mae = perfs['clean']['MAE']\n",
    "            clean_rmse = perfs['clean']['RMSE']\n",
    "            clean.append(round(clean_mae, 2))\n",
    "            clean.append(round(clean_rmse, 2))\n",
    "        all_perfs = dirty + clean\n",
    "        regression.loc[key] = all_perfs\n",
    "regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7ed32ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_9812\\2389794404.py:1: UserWarning:\n",
      "\n",
      "Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.8' currently installed).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Store the performances in an excel file\n",
    "regression.to_excel(\"data/perfs_cv_regr.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fe696c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 8:  3.02\n",
      "RMSE 8:  3.64\n",
      "MAE 204:  43.83\n",
      "RMSE 204:  54.65\n",
      "MAE 210:  0.29\n",
      "RMSE 210:  0.43\n",
      "MAE 560:  1.46\n",
      "RMSE 560:  2.47\n",
      "MAE 491:  0.67\n",
      "RMSE 491:  0.96\n",
      "MAE 566:  15.28\n",
      "RMSE 566:  21.01\n",
      "MAE 189:  0.13\n",
      "RMSE 189:  0.17\n",
      "MAE 673:  0.45\n",
      "RMSE 673:  0.56\n",
      "MAE 639:  0.77\n",
      "RMSE 639:  0.98\n",
      "MAE 41700:  705.92\n",
      "RMSE 41700:  993.53\n"
     ]
    }
   ],
   "source": [
    "# Calculate average scores per dataset for clean datasets\n",
    "for idx in regression.index:\n",
    "    print(f\"MAE {idx}: \", round(mean(regression.loc[idx][['clean_rf_mae', 'clean_lr_mae', 'clean_knn_mae', 'clean_dt_mae', 'clean_gb_mae']]), 2))\n",
    "    print(f\"RMSE {idx}: \", round(mean(regression.loc[idx][['clean_rf_rmse', 'clean_lr_rmse', 'clean_knn_rmse', 'clean_dt_rmse', 'clean_gb_rmse']]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5836e00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 8:  3.05\n",
      "RMSE 8:  3.71\n",
      "MAE 204:  45.55\n",
      "RMSE 204:  58.46\n",
      "MAE 210:  0.32\n",
      "RMSE 210:  0.5\n",
      "MAE 560:  1.46\n",
      "RMSE 560:  2.45\n",
      "MAE 491:  0.66\n",
      "RMSE 491:  0.93\n",
      "MAE 566:  236.23\n",
      "RMSE 566:  760.36\n",
      "MAE 189:  0.13\n",
      "RMSE 189:  0.17\n",
      "MAE 673:  0.45\n",
      "RMSE 673:  0.56\n",
      "MAE 639:  0.77\n",
      "RMSE 639:  0.99\n",
      "MAE 41700:  715.23\n",
      "RMSE 41700:  977.75\n"
     ]
    }
   ],
   "source": [
    "# Calculate average scores per dataset for dirty datasets\n",
    "for idx in regression.index:\n",
    "    print(f\"MAE {idx}: \", round(mean(regression.loc[idx][['dirty_rf_mae', 'dirty_lr_mae', 'dirty_knn_mae', 'dirty_dt_mae', 'dirty_gb_mae']]), 2))\n",
    "    print(f\"RMSE {idx}: \", round(mean(regression.loc[idx][['dirty_rf_rmse', 'dirty_lr_rmse', 'dirty_knn_rmse', 'dirty_dt_rmse', 'dirty_gb_rmse']]), 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
