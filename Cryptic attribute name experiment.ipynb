{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f43a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import wordninja\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import copy\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pFAHES.common as common\n",
    "import pFAHES.patterns as patterns\n",
    "import pFAHES.DV_Detector as DV_Detector\n",
    "import pFAHES.RandDMVD as RandDMVD\n",
    "import pFAHES.OD as OD\n",
    "from statistics import mean, median, mode\n",
    "import copy\n",
    "from sortinghatinf import get_sortinghat_types\n",
    "from numpy import percentile\n",
    "import tiktoken\n",
    "from deepchecks.tabular.checks import MixedDataTypes\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import StringMismatch\n",
    "from PyNomaly import loop\n",
    "from nameguess.metric import BNGMetrics \n",
    "import pickle\n",
    "from evaluate import load\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9680e",
   "metadata": {},
   "source": [
    "## Classes and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eeba3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrypticNameGenerator:\n",
    "    def __init__(self, per_tok_target_len, lookup_abbreviation,\n",
    "                 p_filter_acronym, lookup_acronym,\n",
    "                 pr_keep_k, pr_remove_vowels, pr_logic,\n",
    "                 pm_as_is, pm_lookup, pm_selected_rule):\n",
    "        \"\"\"_summary_\n",
    "        Class for automatic cryptic name generation from table column headers\n",
    "\n",
    "        Args:\n",
    "            per_tok_target_len (int): the target length when abbreviating each token through rules\n",
    "            lookup_abbreviation (dict): a lookup tables containing (expansion, abbreviation) pairs\n",
    "            lookup_acronym (dict): a lookup tables containing (expansion, acronym) pairs\n",
    "            p_filter_acronym (float): probability of filtering and replacing the subsequence by an acronym from the acronym lookup dictionary.\n",
    "            pr_keep_k (float): for rules, the probability of choosing rule 1: keep the first k characters\n",
    "            pr_remove_vowels (float): for rules, the probability of choosing rule 2: remove all non-leading vowels\n",
    "            pr_logic (float): for rules, the probability of choosing rule 3: logic from https://docs.tibco.com/pub/enterprise-runtime-for-R/4.1.1/doc/html/Language_Reference/base/abbreviate.html\n",
    "            pm_as_is (float): for token-level methods, the probability of choosing token-level method 1: keep the token as-is\n",
    "            pm_lookup (float): for token-level methods, the probability of choosing token-level method 2: generate abbreviation through lookup table\n",
    "            pm_selected_rule (float): or token-level methods, the probability of choosing token-level method 3: use rules selected from (pr_keep_k, pr_remove_vowels, pr_logic)\n",
    "        \"\"\"\n",
    "        self.per_tok_target_len = per_tok_target_len\n",
    "        self.lookup_abbreviation = lookup_abbreviation\n",
    "        self.lookup_acronym = lookup_acronym\n",
    "        self.p_filter_acronym = p_filter_acronym\n",
    "        self.pr_keep_k = pr_keep_k\n",
    "        self.pr_remove_vowels = pr_remove_vowels\n",
    "        self.pr_logic = pr_logic\n",
    "        self.pm_as_is = pm_as_is\n",
    "        self.pm_lookup = pm_lookup\n",
    "        self.pm_selected_rule = pm_selected_rule\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        self.stemmer = nltk.stem.PorterStemmer()\n",
    "        \n",
    "    def rule_keep_k(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Rule 1: Keep first k characters in a word\n",
    "        \"\"\"\n",
    "        return query[:self.per_tok_target_len] if len(query) > self.per_tok_target_len else query\n",
    "\n",
    "    def rule_remove_vowels(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Rule 2: Keep removing non-leading vowels until the threshold or all non-leading vowels have been removed\n",
    "        \"\"\"\n",
    "        start, elems = query[0], list(query)[1:]\n",
    "        \n",
    "        vow_idx = [i for i, val in enumerate(elems) if val in ('a', 'e', 'i', 'o', 'u')]\n",
    "        counter_vow = len(vow_idx)\n",
    "        counter_truncate = len(query)\n",
    "        if len(query) > self.per_tok_target_len and vow_idx: \n",
    "            while counter_truncate >= self.per_tok_target_len and counter_vow > 0:\n",
    "                elems[vow_idx[counter_vow-1]] = \"\"\n",
    "                counter_vow -= 1\n",
    "                counter_truncate -= 1\n",
    "\n",
    "        return start + \"\".join(elems)\n",
    "\n",
    "    def rule_logic(self, query: str) -> str:\n",
    "        \"\"\" \n",
    "        Rule 3:\n",
    "        Code contributed by Nicholas Hespe @nahespe\n",
    "\n",
    "        The abbreviation algorithm does not simply truncate. \n",
    "        It has a threshold, according to which it will drop, in order:\n",
    "\n",
    "            1. duplicate values next to eachother\n",
    "            2. lower case vowels.\n",
    "            3. lower case consonants and punctuation.\n",
    "            4. upper case letters and special characters.  \n",
    "        \n",
    "        exits if target_len <= 2\n",
    "        \n",
    "        \"\"\"\n",
    "        start, elems = query[0], list(query)[1:]\n",
    "        \n",
    "        ## exit early if not valid\n",
    "        if len(elems) < self.per_tok_target_len: \n",
    "            return start + \"\".join(elems)\n",
    "        \n",
    "        counter = len(elems)\n",
    "        while counter >= self.per_tok_target_len:\n",
    "            counter -= 1\n",
    "            \n",
    "            ## remove duplicates next to eachother\n",
    "            candidates = [i for i in range(len(elems[:-1])) if (elems[i] and elems[i]==elems[i+1])]\n",
    "            if candidates:\n",
    "                choice = random.choice(candidates)\n",
    "                elems[choice] = \"\"\n",
    "                continue\n",
    "                \n",
    "            ## search for vowels and remove right to left\n",
    "            candidates = [i for i, val in enumerate(elems) if val in ('a', 'e', 'i', 'o', 'u')]\n",
    "            if candidates:\n",
    "                choice = random.choice(candidates)\n",
    "                elems[choice] = \"\"\n",
    "                continue\n",
    "            \n",
    "            ## Search for  lower case consonants and remove randomly\n",
    "            candidates = [i for i, val in enumerate(elems) if (val and not val in ('a', 'e', 'i', 'o', 'u'))]\n",
    "            if candidates:\n",
    "                choice = random.choice(candidates)\n",
    "                elems[choice] = \"\"\n",
    "            \n",
    "        return start + \"\".join(elems)\n",
    "\n",
    "    def select_from_probs(self, probs: list, epsilon: float=1e-8) -> int:\n",
    "        \"\"\"\n",
    "        Make random selection based on the probabilities of each index\n",
    "        \"\"\"\n",
    "        assert abs(np.sum(probs) - 1) < epsilon, 'Sampling probabilities must add up tp 1.'\n",
    "       \n",
    "        rand = random.uniform(0, 1)\n",
    "\n",
    "        def cum_sum(l):\n",
    "            sum = 0\n",
    "            new_l = [0]\n",
    "            for ele in l:\n",
    "                sum += ele\n",
    "                new_l.append(sum) \n",
    "            return new_l\n",
    "\n",
    "        probs_cum = cum_sum(probs)\n",
    "        for i, this_level in enumerate(probs_cum[:-1]):\n",
    "            next_level = probs_cum[i + 1]\n",
    "            if this_level <= rand < next_level:\n",
    "                return i\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def tokenize(self, text: str, \n",
    "                   keep_punc: bool=True, \n",
    "                   keep_stopwords: bool=True,\n",
    "                   split_camelcase: bool=True,\n",
    "                   use_stem: bool=False) -> list:\n",
    "        \"\"\"_summary_\n",
    "        Split the text into words and punctuations\n",
    "\n",
    "        Args:\n",
    "            text (str): input string\n",
    "            keep_punc (bool, optional): whether to keep non-alphanumeric symbols. Defaults to True.\n",
    "            keep_stopwords (bool, optional): whether to keep stop words. Defaults to False.\n",
    "            split_camelcase (bool, optional): whether to split camelCased words (i.e. \"camelCase\" -> \"camel Case\"). Defaults to True.\n",
    "            use_stem (bool, optional): whether to use stemmer\n",
    "        Returns:\n",
    "            list: a list of tokens\n",
    "        \"\"\"\n",
    "        def split_with_punc(text: str) -> list:\n",
    "            return re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "\n",
    "        def separate_camel_case(text: str) -> list:\n",
    "            return re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', text))\n",
    "\n",
    "        text = text.replace('_', ' ')\n",
    "        if split_camelcase:\n",
    "            text = separate_camel_case(text)\n",
    "        if keep_punc:\n",
    "            res = split_with_punc(text)\n",
    "        else:\n",
    "            res = text.split()\n",
    "        if not keep_stopwords:\n",
    "            res = [ele for ele in res if ele not in self.stopwords]\n",
    "        \n",
    "        ## Each tokenized words are stemmed\n",
    "        return [self.stemmer.stem(ele) if use_stem else ele for ele in res]\n",
    "\n",
    "    ## Methods\n",
    "    def select_rule(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Method 3: Randomly select a rule from all the pre-defined rules and apply on the string\n",
    "        \"\"\"\n",
    "        ## Rule not applied on numericals\n",
    "        if query.isdigit():\n",
    "            return query \n",
    "\n",
    "        rule_choices = [(self.pr_keep_k, self.rule_keep_k), \n",
    "                        (self.pr_remove_vowels, self.rule_remove_vowels), \n",
    "                        (self.pr_logic, self.rule_logic)]\n",
    "        ## Probabilities of choosing each of the rule when the method seleted is rule-based.\n",
    "        rule_probs = [choice[0] for choice in rule_choices]\n",
    "        selected_rule_idx = self.select_from_probs(rule_probs)\n",
    "        selected_rule = rule_choices[selected_rule_idx][-1]\n",
    "        if len(query) > 10:\n",
    "            orig_thres = self.per_tok_target_len\n",
    "            self.per_tok_target_len = len(query) // 2\n",
    "            res = selected_rule(query)\n",
    "            self.per_tok_target_len = orig_thres\n",
    "        else:\n",
    "            res = selected_rule(query)\n",
    "        return res\n",
    "\n",
    "    def as_is(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Method 1: Keep the word as is.s\n",
    "        \"\"\"\n",
    "        return query\n",
    "    \n",
    "    def lookup(self, query: str) -> str: \n",
    "        \"\"\"\n",
    "        Method 2: Find corresponding abbreviation from a lookup table\n",
    "        \"\"\"\n",
    "        ## TODO if returns multiple values, current solution is to randomly pick one, but need to later figure out a soln to cache one value for future use in the same table, or some similar tables\n",
    "        if query in self.lookup_abbreviation:\n",
    "            values_raw = self.lookup_abbreviation[query]\n",
    "            if values_raw is not None:\n",
    "                weights = [ele[\"upvotes\"] for ele in values_raw.values()]\n",
    "                if sum(weights) > 0:\n",
    "                    abbrev = random.choices(list(values_raw.keys()),\n",
    "                        weights=weights, k=1)[0]\n",
    "                    return abbrev\n",
    "                \n",
    "        return self.select_rule(query)\n",
    "\n",
    "    def select_method(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Select one of the token-level processing method\n",
    "        \"\"\"\n",
    "        method_choices = [(self.pm_as_is, self.as_is), \n",
    "                          (self.pm_lookup, self.lookup), \n",
    "                          (self.pm_selected_rule, self.select_rule)]\n",
    "        method_probs = [choice[0] for choice in method_choices]\n",
    "        selected_method_idx = self.select_from_probs(method_probs)\n",
    "        selected_method = method_choices[selected_method_idx][-1]\n",
    "        return selected_method(query)\n",
    "\n",
    "    def combine(self, toks: list, p_camel=.333, p_underscore=.333) -> str:\n",
    "        \"\"\"\n",
    "        Combine the abbreviated tokens into the cryptic name by either camelCase or underscore_name\n",
    "        \"\"\"\n",
    "        def preprocess(toks: list) -> list:\n",
    "            new_toks = []\n",
    "            for tok in toks:\n",
    "                if isinstance(tok, list):\n",
    "                    new_toks.extend(tok)\n",
    "                else:\n",
    "                    new_toks.append(tok)\n",
    "            return new_toks\n",
    "\n",
    "        def combine_underscore(toks: list) -> str:\n",
    "            res = \"\"\n",
    "            for i, tok in enumerate(toks):\n",
    "                if tok.isalnum() and i < len(toks) - 1:\n",
    "                    res += tok\n",
    "                    if toks[i+1].isalnum():\n",
    "                        res += \"_\"\n",
    "                else:\n",
    "                    res += tok\n",
    "            return res\n",
    "        \n",
    "        def combine_camel(toks: list) -> str:\n",
    "            if len(toks) > 1:\n",
    "                camel_case = \"\".join([toks[0]] + [tok[0].upper() + tok[1:] if len(tok) > 1 else tok.upper() for tok in toks[1:]])\n",
    "                return camel_case\n",
    "            else:\n",
    "                return \"\".join(toks)\n",
    "\n",
    "        def combine_simple(toks: list) -> str: \n",
    "            return \"\".join(toks)\n",
    "\n",
    "        toks = preprocess(toks)\n",
    "        rand = random.uniform(0, 1)\n",
    "        if 0 < rand < p_camel:\n",
    "            return combine_camel(toks)\n",
    "        elif p_camel <= rand < p_camel + p_underscore:\n",
    "            return combine_underscore(toks)\n",
    "        else:\n",
    "            return combine_simple(toks)\n",
    "    \n",
    "    def span2plus(self, lst):\n",
    "        res = []\n",
    "        for i in range(2, len(lst) + 1):\n",
    "            for t in range(len(lst) - i + 1):\n",
    "                res.append((lst[t:t+i], t, t+i))\n",
    "        return res\n",
    "\n",
    "    def filter_acronyms(self, words, lookup):\n",
    "    \n",
    "        combs = self.span2plus(words)\n",
    "        for comb, l_end, r_start in combs:\n",
    "            comb_string = \" \".join(comb)\n",
    "            if comb_string in lookup:\n",
    "                acronym_cands = lookup[comb_string]\n",
    "                weights = [ele[\"upvotes\"] for ele in acronym_cands.values()]\n",
    "                if sum(weights) > 0:\n",
    "                    acronym = random.choices(list(acronym_cands.keys()), weights=weights, k=1)[0]\n",
    "                    left, right = words[:l_end], words[r_start:]\n",
    "                    return [acronym], l_end, r_start\n",
    "        return [], -1, -1\n",
    "\n",
    "    def generate(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate cryptic name from column header\n",
    "        \"\"\"\n",
    "        toks = self.tokenize(text)\n",
    "        if len(toks) < 10:\n",
    "            # The time complexity for acronym matching is O(N(N-1)/2) ~ O(N^2), where N is the number of tokens in the column header.\n",
    "            # It is possible to encounter very long headers like a small paragraph and we should avoid matching acronyms for very long headers.\n",
    "            # Threshold set to 10 tokens.\n",
    "            acronym, acronym_start_idx, acronym_end_idx = self.filter_acronyms([tok.lower() for tok in toks], self.lookup_acronym)\n",
    "            rand = random.uniform(0, 1)\n",
    "\n",
    "            ## Case where there exist matching span(s) from the acronym lookup dictionary and generator selected to replace acronyms\n",
    "            if acronym_start_idx >= 0 and rand < self.p_filter_acronym:\n",
    "                left = [self.select_method(tok) for tok in toks[:acronym_start_idx]]\n",
    "                right = [self.select_method(tok) for tok in toks[acronym_end_idx:]]\n",
    "                return left + acronym + right\n",
    "\n",
    "        return [self.select_method(tok) for tok in toks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835ffd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrypticIdentifier:\n",
    "    \"\"\"Module to identify any cryptic forms in a column header.\n",
    "    Example usage: \n",
    "        identifier = CrypticIdentifier(vocab_file)\n",
    "        identifier.iscryptic(\"newyorkcitytotalpopulation\") --> False\n",
    "        identifier.iscryptic(\"tot_revq4\") --> True\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_file=None, word_rank_file=None, k_whole=4, k_split=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_file (str, optional): json file containing the vocabulary. Defaults to None.\n",
    "            k_whole (int, optional): length threshold for a whole string to be considered non-cryptic if it fails the first round of check (i.e.\n",
    "            _iscryptic returns True). Defaults to 4.\n",
    "            k_split (int, optional): length threshold for each word split (wordninja.split()) from the string to be considered non-cryptic, if the pre-split string fails the first round of check (i.e.\n",
    "            _iscryptic returns True). Defaults to 2.\n",
    "        \"\"\"\n",
    "        if vocab_file is not None:\n",
    "            with open(vocab_file, \"r\") as fi:\n",
    "                self.vocab = json.load(fi)\n",
    "#                 print(\"#vocab={}\".format(len(self.vocab)))\n",
    "        else:\n",
    "            self.vocab = None\n",
    "\n",
    "        self.k_whole = k_whole\n",
    "        self.k_split = k_split\n",
    "        if word_rank_file is None:\n",
    "            self.splitter = wordninja\n",
    "        else:\n",
    "            self.splitter = wordninja.LanguageModel(word_rank_file)\n",
    "        self.lem = WordNetLemmatizer()\n",
    "        \n",
    "\n",
    "    def split_rm_punc(self, text: str) -> list:\n",
    "        return re.sub(r'[^\\w\\s]', ' ', text).split()\n",
    "\n",
    "    def separate_camel_case(self, text: str) -> list:\n",
    "        return re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', text))\n",
    "\n",
    "    def convert2base(self, text: str) -> str:\n",
    "        return self.lem.lemmatize(text)\n",
    "\n",
    "    def _split(self, text: str) -> list:\n",
    "        text = text.replace('_', ' ')\n",
    "        words = self.split_rm_punc(self.separate_camel_case(text))\n",
    "        return words\n",
    "\n",
    "    def _iscryptic(self, text: str) -> bool:\n",
    "        words = self._split(text)\n",
    "        if all([word.isnumeric() for word in words]):\n",
    "            return True\n",
    "        if self.vocab is None:\n",
    "            self.vocab = nltk.corpus.wordnet.words('english')\n",
    "        return any([self.convert2base(w.lower()) not in self.vocab for w in words])\n",
    "\n",
    "\n",
    "\n",
    "    def doublecheck_cryptic(self, text: str) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Double-check whether a column header contains cryptic terms. For example in some cases where neither \n",
    "        delimiters between tokens nor camelcases is available\n",
    "\n",
    "        Args:\n",
    "            text (str): column header\n",
    "\n",
    "        Returns:\n",
    "            Tuple[\n",
    "                    bool: whether header is cryptic\n",
    "                    List[str]: splitted tokens from the header\n",
    "                ]\n",
    "        \"\"\"\n",
    "\n",
    "        #stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "        def split_check(words: List[str]) -> Tuple[bool, List[str]]:\n",
    "            l_cryptic = []\n",
    "            for ele in words:\n",
    "                if ele.isdigit():\n",
    "                    l_cryptic.append(False)\n",
    "                ## Cornercases includes stopwords like \"I\", \"for\", etc.\n",
    "                elif len(ele) < self.k_split: # and ele.lower() not in stopwords:\n",
    "                    l_cryptic.append(True)\n",
    "                ## Second round check\n",
    "                else:\n",
    "                    l_cryptic.append(self._iscryptic(ele))\n",
    "            return any(l_cryptic), words\n",
    "            \n",
    "        if len(text) >= self.k_whole:\n",
    "            if self._iscryptic(text):\n",
    "                split = self.splitter.split(text)\n",
    "                return split_check(split)            \n",
    "            else:\n",
    "                # return (False, self.splitter.split(text))\n",
    "                return (False, self._split(text))\n",
    "        else:\n",
    "            return (True, [text])\n",
    "\n",
    "    def iscryptic(self, text: str) -> bool:\n",
    "        return self.doublecheck_cryptic(text)[0]\n",
    "    \n",
    "    def split_results(self, text: str) -> List[str]:\n",
    "        return self.doublecheck_cryptic(text)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "935833dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cryp_cols(cryp_list, title, description, content_df):\n",
    "    ''' Function to generate the query for a dataset based on the parameters passed'''\n",
    "    col_query = \" | \".join(str(item) for item in cryp_list)\n",
    "    \n",
    "    if not content_df.empty:\n",
    "        num_instances = len(content_df)\n",
    "        content_list = [content_df.loc[idx].to_list() for idx in content_df.index]\n",
    "        contents = \"\\n\".join([\" | \".join(str(item) for item in lst) for lst in content_list])\n",
    "        content_bool = True\n",
    "    else:\n",
    "        content_bool = False\n",
    "    \n",
    "    if title == False:\n",
    "        title_query = \"\"\n",
    "    else:\n",
    "        title_query = f\"with title: {title} \"\n",
    "    if description == False:\n",
    "        desc_query = \"\"\n",
    "    else:\n",
    "        desc_query = f\"with description: {description} \"\n",
    "    if content_bool == False:\n",
    "        content_query = \"\"\n",
    "    else:\n",
    "        content_query = f\"with contents of {num_instances} random instances:\\n{contents}\"\n",
    "    \n",
    "    if title == False and description == False and content_bool == False:\n",
    "        subquery = \"\"\n",
    "    else:\n",
    "        subquery=f\"\"\"\n",
    "{title_query}\n",
    "{desc_query}\n",
    "{content_query}\n",
    "        \"\"\"\n",
    "    \n",
    "    query = f\"\"\"Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
    "From a dataset{subquery},\n",
    "the abbreviated column names: {col_query} stand for\"\"\"\n",
    "    print(query)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e28e9",
   "metadata": {},
   "source": [
    "## Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b60456d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_cryp_cols(df, title=False, description=False, n_instances=0):\n",
    "    ''' Correction function that calls GPT3.5 to suggest better names for column names (as used in the tool)'''\n",
    "    # Set API key, uncomment to test\n",
    "    client = OpenAI(api_key=\"\") # place your api key here\n",
    "    \n",
    "    # Identify the cryptic column names\n",
    "    cryptic_cols = detect_cryptic(df)\n",
    "    num_cols = len(cryptic_cols)\n",
    "    token_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    if n_instances == 0:\n",
    "        instances = pd.DataFrame()\n",
    "    else:\n",
    "        # Obtain n random instances of content for the cryptic columns\n",
    "        sliced_df = df[cryptic_cols]\n",
    "        indices = random.sample(range(0, len(sliced_df)), n_instances)\n",
    "        instances = sliced_df.iloc[indices]\n",
    "    \n",
    "    # Generate the OpenAI query\n",
    "    query = query_cryp_cols(cryptic_cols, title=title, description=description, content_df=instances)\n",
    "    tokens_used = len(token_encoding.encode(query))\n",
    "    print(tokens_used)\n",
    "\n",
    "    # Call OpenAI's GPT 3.5 Turbo LLM model to generate better column names\n",
    "    completion = client.chat.completions.create(model=\"gpt-3.5-turbo\",temperature=0.0,messages=[{\"role\": \"user\", \"content\": query}])\n",
    "    message = completion.choices[0].message\n",
    "    new_col_names = message.content\n",
    "    return new_col_names, tokens_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392e265",
   "metadata": {},
   "source": [
    "## Generate Cryptic Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2436ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cryptic(non_cryptic_list):\n",
    "    ''' Function to generate cryptic names for non-cryptic column names.'''\n",
    "    cryptic_list = []\n",
    "    not_same = True\n",
    "    with open(\"./lookups/cryptifier_config.json\", \"r\") as fi:\n",
    "        params = json.load(fi)\n",
    "    generator = CrypticNameGenerator(lookup_abbreviation=\"./lookups/abbreviation_samples.json\", lookup_acronym=\"./lookups/acronym_samples.json\", **params)\n",
    "    for non_cryptic_name in non_cryptic_list:\n",
    "        while not_same:\n",
    "            cryptic_name = generator.combine(generator.generate(non_cryptic_name))\n",
    "            if cryptic_name != non_cryptic_name:\n",
    "                cryptic_list.append(cryptic_name)\n",
    "                not_same = False\n",
    "        not_same = True\n",
    "    return cryptic_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c44c92",
   "metadata": {},
   "source": [
    "## Detect Cryptic Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd624c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cryptic(df):\n",
    "    ''' Function to detect cryptic attributes in a dataset.'''\n",
    "    identifier = CrypticIdentifier(\"./lookups/wordnet.json\", \"./lookups/wordninja_words_alpha.txt.gz\")\n",
    "    cryptic_cols = [col for col in df.columns if identifier.doublecheck_cryptic(col)[0]==True or len(col) < 5]\n",
    "    return cryptic_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d103204",
   "metadata": {},
   "source": [
    "## Detect Non-Cryptic Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "239ab4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_non_cryptic(df):\n",
    "    ''' Function to detect non-cryptic attributes in a dataset'''\n",
    "    cryptic = detect_cryptic(df)\n",
    "    non_cryptic = list(set(df.columns) - set(cryptic))\n",
    "    return non_cryptic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0e3a3",
   "metadata": {},
   "source": [
    "## Generate Non-Cryptic Names (GPT-3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b085da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_cols_desc(description, correct_columns, cryptic_columns):\n",
    "    ''' For the queries containing the description, the ground truth column names \n",
    "    in the description have to be replaced by the cryptic names'''\n",
    "    for correct, cryptic in zip(correct_columns, cryptic_columns):\n",
    "        description = description.replace(correct, cryptic)\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2bd7001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_non_cryptic(df, non_cryp_cols, gen_cryp_cols, n_instances, title=False, description=False):\n",
    "    ''' Generate non-cryptic names for the generated cryptic names using GPT-3.5'''\n",
    "        # Set API key, uncomment to test\n",
    "    client = OpenAI(api_key=\"\") # place your API key here\n",
    "    token_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    \n",
    "    if n_instances == 0:\n",
    "        instances = pd.DataFrame()\n",
    "    else:\n",
    "        # Obtain n random instances of content for the cryptic columns\n",
    "        sliced_df = df[non_cryp_cols]\n",
    "        indices = random.sample(range(0, len(sliced_df)), n_instances)\n",
    "        instances = sliced_df.iloc[indices]\n",
    "    \n",
    "    if description:\n",
    "        description = replace_cols_desc(description, non_cryp_cols, gen_cryp_cols)\n",
    "    \n",
    "    # Generate the OpenAI query\n",
    "    query = query_cryp_cols(gen_cryp_cols, title=title, description=description, content_df=instances)\n",
    "    tokens_used = len(token_encoding.encode(query))\n",
    "\n",
    "    # Call OpenAI's GPT 3.5 Turbo LLM model to generate better column names\n",
    "    completion = client.chat.completions.create(model=\"gpt-3.5-turbo\",temperature=0.0,messages=[{\"role\": \"user\", \"content\": query}])\n",
    "    message = completion.choices[0].message\n",
    "    new_col_names_string = message.content\n",
    "    new_col_names = new_col_names_string.split(' | ')\n",
    "    return new_col_names, tokens_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f57f2",
   "metadata": {},
   "source": [
    "## Obtain the 20 Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db40bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "# List all datasets and their properties\n",
    "df_datasets = openml.datasets.list_datasets(output_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ba8031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain 10 datasets with 10 to 20 features and 10 datasets with 20 to 30 features.\n",
    "# Less than 10.000 instances and version 1 are used to filter down all the options\n",
    "df_10_to_20 = df_datasets[(df_datasets['NumberOfFeatures'] >= 10) & (df_datasets['NumberOfFeatures'] <= 20) & (df_datasets['NumberOfInstances'] <= 10000) & (df_datasets['NumberOfInstances'] >= 10) & (df_datasets['version'] == 1)].dropna()\n",
    "df_20_to_30 = df_datasets[(df_datasets['NumberOfFeatures'] > 20) & (df_datasets['NumberOfFeatures'] <= 30) & (df_datasets['NumberOfInstances'] <= 10000) & (df_datasets['NumberOfInstances'] >= 10) & (df_datasets['version'] == 1)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0761eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>uploader</th>\n",
       "      <th>status</th>\n",
       "      <th>format</th>\n",
       "      <th>MajorityClassSize</th>\n",
       "      <th>MaxNominalAttDistinctValues</th>\n",
       "      <th>MinorityClassSize</th>\n",
       "      <th>NumberOfClasses</th>\n",
       "      <th>NumberOfFeatures</th>\n",
       "      <th>NumberOfInstances</th>\n",
       "      <th>NumberOfInstancesWithMissingValues</th>\n",
       "      <th>NumberOfMissingValues</th>\n",
       "      <th>NumberOfNumericFeatures</th>\n",
       "      <th>NumberOfSymbolicFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>autos</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>67.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>4208.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3916.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8124.0</td>\n",
       "      <td>2480.0</td>\n",
       "      <td>2480.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>colic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>232.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>1927.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>credit-g</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>700.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>sick</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>3541.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3772.0</td>\n",
       "      <td>3772.0</td>\n",
       "      <td>6064.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    did      name  version uploader  status format  MajorityClassSize  \\\n",
       "9     9     autos        1        1  active   ARFF               67.0   \n",
       "24   24  mushroom        1        1  active   ARFF             4208.0   \n",
       "25   25     colic        1        1  active   ARFF              232.0   \n",
       "31   31  credit-g        1        1  active   ARFF              700.0   \n",
       "38   38      sick        1        1  active   ARFF             3541.0   \n",
       "\n",
       "    MaxNominalAttDistinctValues  MinorityClassSize  NumberOfClasses  \\\n",
       "9                          22.0                3.0              6.0   \n",
       "24                         12.0             3916.0              2.0   \n",
       "25                         63.0              136.0              2.0   \n",
       "31                         10.0              300.0              2.0   \n",
       "38                          5.0              231.0              2.0   \n",
       "\n",
       "    NumberOfFeatures  NumberOfInstances  NumberOfInstancesWithMissingValues  \\\n",
       "9               26.0              205.0                                46.0   \n",
       "24              23.0             8124.0                              2480.0   \n",
       "25              27.0              368.0                               361.0   \n",
       "31              21.0             1000.0                                 0.0   \n",
       "38              30.0             3772.0                              3772.0   \n",
       "\n",
       "    NumberOfMissingValues  NumberOfNumericFeatures  NumberOfSymbolicFeatures  \n",
       "9                    59.0                     15.0                      11.0  \n",
       "24                 2480.0                      0.0                      23.0  \n",
       "25                 1927.0                      7.0                      20.0  \n",
       "31                    0.0                      7.0                      14.0  \n",
       "38                 6064.0                      7.0                      23.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_20_to_30.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "042b0549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48, 21, 14, 10, 57, 58, 24, 36, 44, 18, 35, 17, 59, 46, 63],\n",
       " [36, 0, 15, 42, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random seed to reproduce the indices\n",
    "# We select 15 datasets from the 10 to 20 features dataset and 5 from the 20 to 30 features dataset\n",
    "random.seed(41)\n",
    "idx_10_20 = random.sample(range(0, 64), 15)\n",
    "idx_20_30 = random.sample(range(0, len(df_20_to_30)), 5)\n",
    "idx_10_20, idx_20_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1d9b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_10_20 = df_10_to_20.iloc[idx_10_20]\n",
    "instances_20_30 = df_20_to_30.iloc[idx_20_30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ec2e52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>uploader</th>\n",
       "      <th>status</th>\n",
       "      <th>format</th>\n",
       "      <th>MajorityClassSize</th>\n",
       "      <th>MaxNominalAttDistinctValues</th>\n",
       "      <th>MinorityClassSize</th>\n",
       "      <th>NumberOfClasses</th>\n",
       "      <th>NumberOfFeatures</th>\n",
       "      <th>NumberOfInstances</th>\n",
       "      <th>NumberOfInstancesWithMissingValues</th>\n",
       "      <th>NumberOfMissingValues</th>\n",
       "      <th>NumberOfNumericFeatures</th>\n",
       "      <th>NumberOfSymbolicFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>1552</td>\n",
       "      <td>autoUniv-au7-1100</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>305.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>375</td>\n",
       "      <td>JapaneseVowels</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9961.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>hepatitis</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>123.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>tic-tac-toe</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>626.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40690</th>\n",
       "      <td>40690</td>\n",
       "      <td>threeOf9</td>\n",
       "      <td>1</td>\n",
       "      <td>869</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>274.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40691</th>\n",
       "      <td>40691</td>\n",
       "      <td>wine-quality-red</td>\n",
       "      <td>1</td>\n",
       "      <td>869</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>681.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>465</td>\n",
       "      <td>analcatdata_cyyoung8092</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>73.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>1100</td>\n",
       "      <td>PopularKids</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>247.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1498</td>\n",
       "      <td>sa-heart</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>302.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>185</td>\n",
       "      <td>baseball</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>1057</td>\n",
       "      <td>usp05-ft</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>55.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171</td>\n",
       "      <td>primary-tumor</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>84.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40693</th>\n",
       "      <td>40693</td>\n",
       "      <td>xd6</td>\n",
       "      <td>1</td>\n",
       "      <td>869</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>651.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>1512</td>\n",
       "      <td>heart-long-beach</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41760</th>\n",
       "      <td>41760</td>\n",
       "      <td>FOREX_eurhuf-day-High</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>arff</td>\n",
       "      <td>953.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1834.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         did                     name  version uploader  status format  \\\n",
       "1552    1552        autoUniv-au7-1100        1       64  active   ARFF   \n",
       "375      375           JapaneseVowels        1        2  active   ARFF   \n",
       "55        55                hepatitis        1        1  active   ARFF   \n",
       "50        50              tic-tac-toe        1        1  active   ARFF   \n",
       "40690  40690                 threeOf9        1      869  active   ARFF   \n",
       "40691  40691         wine-quality-red        1      869  active   ARFF   \n",
       "465      465  analcatdata_cyyoung8092        1        2  active   ARFF   \n",
       "1100    1100              PopularKids        1        2  active   ARFF   \n",
       "1498    1498                 sa-heart        1       64  active   ARFF   \n",
       "185      185                 baseball        1        1  active   ARFF   \n",
       "1057    1057                 usp05-ft        1        2  active   ARFF   \n",
       "171      171            primary-tumor        1        1  active   ARFF   \n",
       "40693  40693                      xd6        1      869  active   ARFF   \n",
       "1512    1512         heart-long-beach        1       64  active   ARFF   \n",
       "41760  41760    FOREX_eurhuf-day-High        1        1  active   arff   \n",
       "\n",
       "       MajorityClassSize  MaxNominalAttDistinctValues  MinorityClassSize  \\\n",
       "1552               305.0                          5.0              153.0   \n",
       "375               1614.0                          9.0              782.0   \n",
       "55                 123.0                          2.0               32.0   \n",
       "50                 626.0                          3.0              332.0   \n",
       "40690              274.0                          2.0              238.0   \n",
       "40691              681.0                          6.0               10.0   \n",
       "465                 73.0                         62.0               24.0   \n",
       "1100               247.0                          9.0               90.0   \n",
       "1498               302.0                          2.0              160.0   \n",
       "185               1215.0                          7.0               57.0   \n",
       "1057                55.0                         16.0                1.0   \n",
       "171                 84.0                         21.0                1.0   \n",
       "40693              651.0                          2.0              322.0   \n",
       "1512                56.0                          5.0               10.0   \n",
       "41760              953.0                          2.0              881.0   \n",
       "\n",
       "       NumberOfClasses  NumberOfFeatures  NumberOfInstances  \\\n",
       "1552               5.0              13.0             1100.0   \n",
       "375                9.0              15.0             9961.0   \n",
       "55                 2.0              20.0              155.0   \n",
       "50                 2.0              10.0              958.0   \n",
       "40690              2.0              10.0              512.0   \n",
       "40691              6.0              12.0             1599.0   \n",
       "465                2.0              11.0               97.0   \n",
       "1100               3.0              11.0              478.0   \n",
       "1498               2.0              10.0              462.0   \n",
       "185                3.0              17.0             1340.0   \n",
       "1057               7.0              15.0               76.0   \n",
       "171               21.0              18.0              339.0   \n",
       "40693              2.0              10.0              973.0   \n",
       "1512               5.0              14.0              200.0   \n",
       "41760              2.0              12.0             1834.0   \n",
       "\n",
       "       NumberOfInstancesWithMissingValues  NumberOfMissingValues  \\\n",
       "1552                                  0.0                    0.0   \n",
       "375                                   0.0                    0.0   \n",
       "55                                   75.0                  167.0   \n",
       "50                                    0.0                    0.0   \n",
       "40690                                 0.0                    0.0   \n",
       "40691                                 0.0                    0.0   \n",
       "465                                   0.0                    0.0   \n",
       "1100                                  0.0                    0.0   \n",
       "1498                                  0.0                    0.0   \n",
       "185                                  20.0                   20.0   \n",
       "1057                                 18.0                   37.0   \n",
       "171                                 207.0                  225.0   \n",
       "40693                                 0.0                    0.0   \n",
       "1512                                  0.0                    0.0   \n",
       "41760                                 0.0                    0.0   \n",
       "\n",
       "       NumberOfNumericFeatures  NumberOfSymbolicFeatures  \n",
       "1552                       8.0                       5.0  \n",
       "375                       14.0                       1.0  \n",
       "55                         6.0                      14.0  \n",
       "50                         0.0                      10.0  \n",
       "40690                      0.0                      10.0  \n",
       "40691                     11.0                       1.0  \n",
       "465                        7.0                       4.0  \n",
       "1100                       6.0                       5.0  \n",
       "1498                       8.0                       2.0  \n",
       "185                       15.0                       2.0  \n",
       "1057                       8.0                       7.0  \n",
       "171                        0.0                      18.0  \n",
       "40693                      0.0                      10.0  \n",
       "1512                      13.0                       1.0  \n",
       "41760                     11.0                       1.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances_10_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7db1125c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>uploader</th>\n",
       "      <th>status</th>\n",
       "      <th>format</th>\n",
       "      <th>MajorityClassSize</th>\n",
       "      <th>MaxNominalAttDistinctValues</th>\n",
       "      <th>MinorityClassSize</th>\n",
       "      <th>NumberOfClasses</th>\n",
       "      <th>NumberOfFeatures</th>\n",
       "      <th>NumberOfInstances</th>\n",
       "      <th>NumberOfInstancesWithMissingValues</th>\n",
       "      <th>NumberOfMissingValues</th>\n",
       "      <th>NumberOfNumericFeatures</th>\n",
       "      <th>NumberOfSymbolicFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40649</th>\n",
       "      <td>40649</td>\n",
       "      <td>GAMETES_Heterogeneity_20atts_1600_Het_0.4_0.2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>869</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>autos</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>67.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>1062</td>\n",
       "      <td>ar5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40708</th>\n",
       "      <td>40708</td>\n",
       "      <td>allrep</td>\n",
       "      <td>1</td>\n",
       "      <td>869</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>3648.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3772.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>4208.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3916.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8124.0</td>\n",
       "      <td>2480.0</td>\n",
       "      <td>2480.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         did                                               name  version  \\\n",
       "40649  40649  GAMETES_Heterogeneity_20atts_1600_Het_0.4_0.2_...        1   \n",
       "9          9                                              autos        1   \n",
       "1062    1062                                                ar5        1   \n",
       "40708  40708                                             allrep        1   \n",
       "24        24                                           mushroom        1   \n",
       "\n",
       "      uploader  status format  MajorityClassSize  MaxNominalAttDistinctValues  \\\n",
       "40649      869  active   ARFF              800.0                          3.0   \n",
       "9            1  active   ARFF               67.0                         22.0   \n",
       "1062         2  active   ARFF               28.0                          2.0   \n",
       "40708      869  active   ARFF             3648.0                          5.0   \n",
       "24           1  active   ARFF             4208.0                         12.0   \n",
       "\n",
       "       MinorityClassSize  NumberOfClasses  NumberOfFeatures  \\\n",
       "40649              800.0              2.0              21.0   \n",
       "9                    3.0              6.0              26.0   \n",
       "1062                 8.0              2.0              30.0   \n",
       "40708               34.0              4.0              30.0   \n",
       "24                3916.0              2.0              23.0   \n",
       "\n",
       "       NumberOfInstances  NumberOfInstancesWithMissingValues  \\\n",
       "40649             1600.0                                 0.0   \n",
       "9                  205.0                                46.0   \n",
       "1062                36.0                                 0.0   \n",
       "40708             3772.0                                 0.0   \n",
       "24                8124.0                              2480.0   \n",
       "\n",
       "       NumberOfMissingValues  NumberOfNumericFeatures  \\\n",
       "40649                    0.0                      0.0   \n",
       "9                       59.0                     15.0   \n",
       "1062                     0.0                     29.0   \n",
       "40708                    0.0                      6.0   \n",
       "24                    2480.0                      0.0   \n",
       "\n",
       "       NumberOfSymbolicFeatures  \n",
       "40649                      21.0  \n",
       "9                          11.0  \n",
       "1062                        1.0  \n",
       "40708                      24.0  \n",
       "24                         23.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances_20_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f5b9a8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40649,\n",
       " 9,\n",
       " 1062,\n",
       " 40708,\n",
       " 24,\n",
       " 1552,\n",
       " 375,\n",
       " 55,\n",
       " 50,\n",
       " 40690,\n",
       " 40691,\n",
       " 465,\n",
       " 1100,\n",
       " 1498,\n",
       " 185,\n",
       " 1057,\n",
       " 171,\n",
       " 40693,\n",
       " 1512,\n",
       " 41760]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dids = instances_20_30['did'].to_list() + instances_10_20['did'].to_list()\n",
    "list_dids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e77530eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_1308\\1445902723.py:5: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 1062, 40708, 24, 375, 55, 50, 40691, 1100, 185, 171, 40693, 41760]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only want datasets with at least 5 non-cryptic attributes\n",
    "dataset_list = []\n",
    "did_good = []\n",
    "for did in list_dids:\n",
    "    dataset = openml.datasets.get_dataset(did)\n",
    "    df, y, _, _ = dataset.get_data(dataset_format=\"dataframe\")\n",
    "    if len(detect_non_cryptic(df)) < 6:\n",
    "        continue\n",
    "    else:\n",
    "        did_good.append(did)\n",
    "did_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8d11dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 40708, 24, 55, 50, 40691, 1100, 185, 171, 40693, 41760]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additionally, it turned out that dataset ids 1062 and 375 gave errors, so we chose two other for them as well\n",
    "did_good.remove(375)\n",
    "did_good.remove(1062)\n",
    "did_good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a20154",
   "metadata": {},
   "source": [
    "## Datasets with not enough non-cryptic columns\n",
    "10 to 20:374, 1552,40690,465,1498,1057,1512 = 7     \n",
    "20 to 30:1062, 40649 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b78601f",
   "metadata": {},
   "source": [
    "By hand we selected those 9 datasets, to get the following list of dataset ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d30df432",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dids = [9,1059,40708,24,36,55,50,40691,1100,185,171,40693,41760,40701,54,4,10,23,13,187]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5be416",
   "metadata": {},
   "source": [
    "# Experiment 1: example instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7b989",
   "metadata": {},
   "source": [
    "## Now that we have sampled 20 datasets with at least 5 non cryptic attributes, we can start the experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "211b4a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_13768\\2399106941.py:10: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: ELONGA | SCLDVARIANCEMJOR | SCATTERRATIO | DISTANCECIRCULARITY | SCALRAISOFGYRATION | SCALEDVAICMINO | RADIUSRATIO | SEWEABOUTMIOR | CMACT | Clss | HOLLOWSRATIO | MAX.LETHRECTANG | CRRTY | MAX.LENGTHAPCTRATO | SKEWABOUMJOR stand for\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13768\\2399106941.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Give the cryptic names as input to GPT combined with an n number of instances and ask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# it to generate non-cryptic names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mgpt_predictions_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_non_cryptic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mgpt_predictions_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_non_cryptic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mgpt_predictions_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_non_cryptic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13768\\1880765121.py\u001b[0m in \u001b[0;36mgenerate_non_cryptic\u001b[1;34m(df, non_cryp_cols, gen_cryp_cols, n_instances, title, description)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Call OpenAI's GPT 3.5 Turbo LLM model to generate better column names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mcompletion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"role\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"user\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"content\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mnew_col_names_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\_utils\\_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\resources\\chat\\completions.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 667\u001b[1;33m         return self._post(\n\u001b[0m\u001b[0;32m    668\u001b[0m             \u001b[1;34m\"/chat/completions\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             body=maybe_transform(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1206\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m         )\n\u001b[1;32m-> 1208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m     def patch(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[1;32m--> 897\u001b[1;33m         return self._request(\n\u001b[0m\u001b[0;32m    898\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    971\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m                 return self._retry_request(\n\u001b[0m\u001b[0;32m    974\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m         return self._request(\n\u001b[0m\u001b[0;32m   1022\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    971\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m                 return self._retry_request(\n\u001b[0m\u001b[0;32m    974\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m         return self._request(\n\u001b[0m\u001b[0;32m   1022\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Re-raising status error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m         return self._process_response(\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# This code generates all predictions for the first experiment with the example instances\n",
    "num = 1\n",
    "pred_dct = dict()\n",
    "token_length_0 = []\n",
    "token_length_1 = []\n",
    "token_length_3 = []\n",
    "token_length_5 = []\n",
    "token_length_10 = []\n",
    "\n",
    "for did in list_dids:\n",
    "    dataset = openml.datasets.get_dataset(did)\n",
    "    df, _, _, _ = dataset.get_data(dataset_format=\"dataframe\")\n",
    "    # Find the non-cryptic columns and convert them to cryptic columns (so we have X and y)\n",
    "    y = detect_non_cryptic(df)\n",
    "    X = generate_cryptic(y)\n",
    "    \n",
    "    # Give the cryptic names as input to GPT combined with an n number of instances and ask\n",
    "    # it to generate non-cryptic names\n",
    "    gpt_predictions_0, t_0 = generate_non_cryptic(df, y, X, 0)\n",
    "    gpt_predictions_1, t_1 = generate_non_cryptic(df, y, X, 1)\n",
    "    gpt_predictions_3, t_3 = generate_non_cryptic(df, y, X, 3)\n",
    "    gpt_predictions_5, t_5 = generate_non_cryptic(df, y, X, 5)\n",
    "    gpt_predictions_10, t_10 = generate_non_cryptic(df, y, X, 10)\n",
    "    \n",
    "    if (len(y) != len(gpt_predictions_0)) or (len(y) != len(gpt_predictions_1)) or (len(y) != len(gpt_predictions_3)) or (len(y) != len(gpt_predictions_5)) or (len(y) != len(gpt_predictions_10)):\n",
    "        print(f\"Something went wrong with the predictions in dataset number {num}\")\n",
    "        print(\"The number of predictions do not align with the number of ground truth values\")\n",
    "        print(f\"Length y: {len(y)}, y_pred_0: {len(gpt_predictions_0)}, y_pred_1: {len(gpt_predictions_1)}, y_pred_3: {len(gpt_predictions_3)}, y_pred_5: {len(gpt_predictions_5)}, y_pred_10: {len(gpt_predictions_10)}\")\n",
    "        print(\"We will not add this dataset to the predictions\")\n",
    "        continue\n",
    "    else:\n",
    "        # Store the predictions in a dataframe\n",
    "        df_predictions = pd.DataFrame()\n",
    "        df_predictions['y'] = y\n",
    "        df_predictions['X'] = X\n",
    "        df_predictions[f'y_pred_0'] = gpt_predictions_0\n",
    "        df_predictions[f'y_pred_1'] = gpt_predictions_1\n",
    "        df_predictions[f'y_pred_3'] = gpt_predictions_3\n",
    "        df_predictions[f'y_pred_5'] = gpt_predictions_5\n",
    "        df_predictions[f'y_pred_10'] = gpt_predictions_10\n",
    "\n",
    "        # To compute the bertscore F1 in the end, we will divide by the average token length for every query\n",
    "        token_length_0.append(t_0)\n",
    "        token_length_1.append(t_1)\n",
    "        token_length_3.append(t_3)\n",
    "        token_length_5.append(t_5)\n",
    "        token_length_10.append(t_10)\n",
    "\n",
    "        # Store the predictions with the token lengths in a dictionary, which we can use to compute the scores\n",
    "        token_lengths = {\"token_length\" : [t_0, t_1, t_3, t_5, t_10]}\n",
    "        pred_dct[f\"pred_df_{num}\"] = [df_predictions, token_lengths]\n",
    "    num += 1\n",
    "pred_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "14e9f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary saved successfully to file\n"
     ]
    }
   ],
   "source": [
    "# Save the predictions to a pickle file\n",
    "with open('data/complete_preds_experiment_1.pkl', 'wb') as fp:\n",
    "    pickle.dump(pred_dct, fp)\n",
    "    print('dictionary saved successfully to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e7489dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_df_1': [                    y            X           y_pred_0           y_pred_1  \\\n",
       "  0              length         lgth             Length             Length   \n",
       "  1          wheel-base    whel-Base         Wheel-Base         Wheel-Base   \n",
       "  2               width         wdth              Width              Width   \n",
       "  3         engine-type    engn-type        Engine Type        Engine Type   \n",
       "  4               price         pric              Price              Price   \n",
       "  5          aspiration       asprtn         Aspiration         Aspiration   \n",
       "  6         curb-weight    curb-wght        Curb Weight        Curb Weight   \n",
       "  7        drive-wheels    drve-Whls       Drive Wheels       Drive Wheels   \n",
       "  8         fuel-system   fuel-systm        Fuel System        Fuel System   \n",
       "  9     engine-location    engi-loca    Engine Location    Engine Location   \n",
       "  10         body-style    body-styl         Body Style         Body Style   \n",
       "  11           peak-rpm     peak-Rpm           Peak RPM           Peak RPM   \n",
       "  12          fuel-type    fuel-Type          Fuel Type          Fuel Type   \n",
       "  13             height         hght             Height             Height   \n",
       "  14  compression-ratio  cmprssn-rat  Compression Ratio  Compression Ratio   \n",
       "  15         horsepower         hswr         Horsepower         Horsepower   \n",
       "  16        engine-size    engn-size        Engine Size        Engine Size   \n",
       "  17             stroke         stro             Stroke             Stroke   \n",
       "  \n",
       "               y_pred_3           y_pred_5          y_pred_10  \n",
       "  0              Length             Length             Length  \n",
       "  1          Wheel-Base         Wheel-Base         Wheel-Base  \n",
       "  2               Width              Width              Width  \n",
       "  3         Engine Type        Engine Type        Engine Type  \n",
       "  4               Price              Price              Price  \n",
       "  5          Aspiration         Aspiration         Aspiration  \n",
       "  6         Curb Weight        Curb Weight        Curb Weight  \n",
       "  7        Drive Wheels       Drive Wheels       Drive Wheels  \n",
       "  8         Fuel System        Fuel System        Fuel System  \n",
       "  9     Engine Location    Engine Location    Engine Location  \n",
       "  10         Body Style         Body Style         Body Style  \n",
       "  11           Peak RPM           Peak RPM           Peak RPM  \n",
       "  12          Fuel Type          Fuel Type          Fuel Type  \n",
       "  13             Height             Height             Height  \n",
       "  14  Compression Ratio  Compression Ratio  Compression Ratio  \n",
       "  15         Horsepower         Horsepower         Horsepower  \n",
       "  16        Engine Size        Engine Size        Engine Size  \n",
       "  17             Stroke             Stroke             Stroke  ,\n",
       "  {'token_length': [107, 199, 350, 500, 871]}],\n",
       " 'pred_df_2': [                           y             X                y_pred_0  \\\n",
       "  0            unique_operands       unqopns          Unique Options   \n",
       "  1          design_complexity      dsgnComp       Design Complexity   \n",
       "  2                    defects         dfcts                 Defects   \n",
       "  3             decision_count      dcsncunt          Decision Count   \n",
       "  4   multiple_condition_count  mltplCntnCnt  Multiple Content Count   \n",
       "  5            total_operators      totaoprs        Total Operations   \n",
       "  6             design_density     dsgnDnsty          Design Density   \n",
       "  7             total_operands      ttalOnds            Total Orders   \n",
       "  8           decision_density  decisionDnsy        Decision Density   \n",
       "  9               branch_count    bnch_count             Bench Count   \n",
       "  10           condition_count    cndtn_coun         Condition Count   \n",
       "  11         formal_parameters    frmlprmtrs       Formal Parameters   \n",
       "  12                call_pairs      callPirs              Call Pairs   \n",
       "  13          unique_operators     uniq_otrs           Unique Others   \n",
       "  \n",
       "                       y_pred_1                  y_pred_3  \\\n",
       "  0           Unique Operations         Unique Operations   \n",
       "  1           Design Complexity         Design Complexity   \n",
       "  2                     Defects                   Defects   \n",
       "  3              Decision Count            Decision Count   \n",
       "  4   Multiple Contention Count  Multiple Condition Count   \n",
       "  5            Total Operations           Total Operators   \n",
       "  6              Design Density            Design Density   \n",
       "  7               Total Opcodes            Total Operands   \n",
       "  8            Decision Density          Decision Density   \n",
       "  9                Branch Count              Branch Count   \n",
       "  10            Condition Count           Condition Count   \n",
       "  11          Formal Parameters         Formal Parameters   \n",
       "  12                 Call Pairs              Called Pairs   \n",
       "  13              Unique Others             Unique Others   \n",
       "  \n",
       "                      y_pred_5                 y_pred_10  \n",
       "  0          Unique Operations         Unique Operations  \n",
       "  1          Design Complexity         Design Complexity  \n",
       "  2                    Defects                   Defects  \n",
       "  3             Decision Count            Decision Count  \n",
       "  4   Multiple Condition Count  Multiple Condition Count  \n",
       "  5            Total Operators           Total Operators  \n",
       "  6             Design Density            Design Density  \n",
       "  7             Total Operands            Total Operands  \n",
       "  8           Decision Density          Decision Density  \n",
       "  9               Branch Count              Branch Count  \n",
       "  10           Condition Count           Condition Count  \n",
       "  11         Formal Parameters         Formal Parameters  \n",
       "  12              Called Pairs              Called Pairs  \n",
       "  13             Unique Others             Unique Others  ,\n",
       "  {'token_length': [104, 172, 271, 365, 605]}],\n",
       " 'pred_df_3': [                   y            X              y_pred_0              y_pred_1  \\\n",
       "  0              tumor          tmr             Treatment             Treatment   \n",
       "  1      hypopituitary       hypptr          Hypertension          Hypertension   \n",
       "  2           pregnant       prgnnt             Pregnancy              Pregnant   \n",
       "  3            lithium         lith               Lithium               Lithium   \n",
       "  4  query_hypothyroid   queryHypot  Query Hypothyroidism  Query Hypothyroidism   \n",
       "  5    referral_source  referralSrc       Referral Source       Referral Source   \n",
       "  6    thyroid_surgery     thyrSurg       Thyroid Surgery       Thyroid Surgery   \n",
       "  7             goitre         gotr                Goiter                Goiter   \n",
       "  8       TSH_measured     TSH_msrd          TSH Measured          TSH Measured   \n",
       "  9              class         clas        Classification        Classification   \n",
       "  \n",
       "                                 y_pred_3              y_pred_5  \\\n",
       "  0                              Tomorrow             Treatment   \n",
       "  1                          Hypopatellar       Hypopituitarism   \n",
       "  2                              Pregnant              Pregnant   \n",
       "  3                               Lithium               Lithium   \n",
       "  4                  Query Hypothyroidism  Query Hypothyroidism   \n",
       "  5                       Referral Source       Referral Source   \n",
       "  6                       Thyroid Surgery       Thyroid Surgery   \n",
       "  7                                Goiter                Goiter   \n",
       "  8  Thyroid-Stimulating Hormone measured          TSH Measured   \n",
       "  9                        Classification        Classification   \n",
       "  \n",
       "                                y_pred_10  \n",
       "  0                              Tomorrow  \n",
       "  1                          Hypopatellar  \n",
       "  2                              Pregnant  \n",
       "  3                               Lithium  \n",
       "  4                  Query Hypothyroidism  \n",
       "  5                       Referral Source  \n",
       "  6                       Thyroid Surgery  \n",
       "  7                                Goiter  \n",
       "  8  Thyroid Stimulating Hormone Measured  \n",
       "  9                        Classification  ,\n",
       "  {'token_length': [70, 118, 176, 234, 379]}],\n",
       " 'pred_df_4': [                           y                    X                  y_pred_0  \\\n",
       "  0                  cap-shape              cap-shp                 Cap Shape   \n",
       "  1   stalk-surface-below-ring   stlk-srfc-blw-ring  Stalk Surface Below Ring   \n",
       "  2            gill-attachment        gill-attchmnt           Gill Attachment   \n",
       "  3     stalk-color-above-ring    stlk-Clr-Abv-Ring    Stalk Color Above Ring   \n",
       "  4                cap-surface             cap-Surf               Cap Surface   \n",
       "  5   stalk-surface-above-ring  stal-Srfc-Abve-Ring  Stalk Surface Above Ring   \n",
       "  6          spore-print-color       spor-print-clr         Spore Print Color   \n",
       "  7                  veil-type            veil-Type                 Veil Type   \n",
       "  8                 gill-color             gill-clr                Gill Color   \n",
       "  9                  gill-size            gill-Size                 Gill Size   \n",
       "  10                population                ppltn                Population   \n",
       "  11                   habitat                 hbtt                   Habitat   \n",
       "  12    stalk-color-below-ring  stalk-colr-blw-ring    Stalk Color Below Ring   \n",
       "  13                veil-color            veil-colo                Veil Color   \n",
       "  14               ring-number            ring-nmbr               Ring Number   \n",
       "  15              gill-spacing            gill-spac              Gill Spacing   \n",
       "  16               stalk-shape           stalk-shpe               Stalk Shape   \n",
       "  17                stalk-root            stlk-root                Stalk Root   \n",
       "  18                 cap-color              cap-clr                 Cap Color   \n",
       "  19                 ring-type            ring-Type                 Ring Type   \n",
       "  20                     class                 clas                     Class   \n",
       "  \n",
       "                      y_pred_1                  y_pred_3  \\\n",
       "  0                  Cap Shape                 Cap Shape   \n",
       "  1   Stalk Surface Below Ring  Stalk Surface Below Ring   \n",
       "  2            Gill Attachment           Gill Attachment   \n",
       "  3     Stalk Color Above Ring    Stalk Color Above Ring   \n",
       "  4                Cap Surface               Cap Surface   \n",
       "  5   Stalk Surface Above Ring  Stalk Surface Above Ring   \n",
       "  6          Spore Print Color         Spore Print Color   \n",
       "  7                  Veil Type                 Veil Type   \n",
       "  8                 Gill Color                Gill Color   \n",
       "  9                  Gill Size                 Gill Size   \n",
       "  10                Population                Population   \n",
       "  11                   Habitat                   Habitat   \n",
       "  12    Stalk Color Below Ring    Stalk Color Below Ring   \n",
       "  13                Veil Color                Veil Color   \n",
       "  14               Ring Number               Ring Number   \n",
       "  15              Gill Spacing              Gill Spacing   \n",
       "  16               Stalk Shape               Stalk Shape   \n",
       "  17                Stalk Root                Stalk Root   \n",
       "  18                 Cap Color                 Cap Color   \n",
       "  19                 Ring Type                 Ring Type   \n",
       "  20                     Class                     Class   \n",
       "  \n",
       "                      y_pred_5                 y_pred_10  \n",
       "  0                  Cap Shape                 Cap Shape  \n",
       "  1   Stalk Surface Below Ring  Stalk Surface Below Ring  \n",
       "  2            Gill Attachment           Gill Attachment  \n",
       "  3     Stalk Color Above Ring    Stalk Color Above Ring  \n",
       "  4                Cap Surface               Cap Surface  \n",
       "  5   Stalk Surface Above Ring  Stalk Surface Above Ring  \n",
       "  6          Spore Print Color         Spore Print Color  \n",
       "  7                  Veil Type                 Veil Type  \n",
       "  8                 Gill Color                Gill Color  \n",
       "  9                  Gill Size                 Gill Size  \n",
       "  10                Population                Population  \n",
       "  11                   Habitat                   Habitat  \n",
       "  12    Stalk Color Below Ring    Stalk Color Below Ring  \n",
       "  13                Veil Color                Veil Color  \n",
       "  14               Ring Number               Ring Number  \n",
       "  15              Gill Spacing              Gill Spacing  \n",
       "  16               Stalk Shape               Stalk Shape  \n",
       "  17                Stalk Root                Stalk Root  \n",
       "  18                 Cap Color                 Cap Color  \n",
       "  19                 Ring Type                 Ring Type  \n",
       "  20                     Class                     Class  ,\n",
       "  {'token_length': [140, 201, 285, 369, 579]}],\n",
       " 'pred_df_5': [                      y               X              y_pred_0  \\\n",
       "  0      top-right-square   top-rght-squa      Top Right Square   \n",
       "  1                 Class            Clas                 Class   \n",
       "  2    middle-left-square   midl-Left-Sqr    Middle Left Square   \n",
       "  3  bottom-middle-square  bttm-mdle-sqre  Bottom Middle Square   \n",
       "  4   middle-right-square  mddl-rght-squr   Middle Right Square   \n",
       "  5       top-left-square   top-left-squr       Top Left Square   \n",
       "  6   bottom-right-square   bott-rght-sqr   Bottom Right Square   \n",
       "  7  middle-middle-square  mdle-Mddl-Sqre  Middle Middle Square   \n",
       "  8    bottom-left-square   btom-left-sqr    Bottom Left Square   \n",
       "  9     top-middle-square   top-mddl-sqre     Top Middle Square   \n",
       "  \n",
       "                 y_pred_1              y_pred_3              y_pred_5  \\\n",
       "  0      Top Right Square      Top Right Square      Top Right Square   \n",
       "  1                 Class                 Class                 Class   \n",
       "  2    Middle Left Square    Middle Left Square    Middle Left Square   \n",
       "  3  Bottom Middle Square  Bottom Middle Square  Bottom Middle Square   \n",
       "  4   Middle Right Square   Middle Right Square   Middle Right Square   \n",
       "  5       Top Left Square       Top Left Square       Top Left Square   \n",
       "  6   Bottom Right Square   Bottom Right Square   Bottom Right Square   \n",
       "  7  Middle Middle Square  Middle Middle Square  Middle Middle Square   \n",
       "  8    Bottom Left Square    Bottom Left Square    Bottom Left Square   \n",
       "  9     Top Middle Square     Top Middle Square     Top Middle Square   \n",
       "  \n",
       "                y_pred_10  \n",
       "  0      Top Right Square  \n",
       "  1                 Class  \n",
       "  2    Middle Left Square  \n",
       "  3  Bottom Middle Square  \n",
       "  4   Middle Right Square  \n",
       "  5       Top Left Square  \n",
       "  6   Bottom Right Square  \n",
       "  7  Middle Middle Square  \n",
       "  8    Bottom Left Square  \n",
       "  9     Top Middle Square  ,\n",
       "  {'token_length': [103, 142, 182, 222, 322]}],\n",
       " 'pred_df_6': [                       y              X              y_pred_0  \\\n",
       "  0                density           dens               Density   \n",
       "  1              sulphates           shts         Sugar Content   \n",
       "  2                alcohol           achl       Alcohol Content   \n",
       "  3                  class           clas        Classification   \n",
       "  4    free_sulfur_dioxide  free_slfr_dxd   Free Sulfur Dioxide   \n",
       "  5            citric_acid      citr_acid           Citric Acid   \n",
       "  6       volatile_acidity       vltladty            Volatility   \n",
       "  7          fixed_acidity     fixd_acdty         Fixed Acidity   \n",
       "  8   total_sulfur_dioxide  tota_slfr_dxd  Total Sulfur Dioxide   \n",
       "  9         residual_sugar       rsdlsuga        Residual Sugar   \n",
       "  10             chlorides           chld              Chloride   \n",
       "  \n",
       "                  y_pred_1              y_pred_3              y_pred_5  \\\n",
       "  0                Density               Density               Density   \n",
       "  1          Sugar Content              Sulfates             Sulphates   \n",
       "  2        Alcohol Content               Alcohol               Alcohol   \n",
       "  3         Classification                 Class                 Class   \n",
       "  4    Free Sulfur Dioxide   Free Sulfur Dioxide   Free Sulfur Dioxide   \n",
       "  5            Citric Acid           Citric Acid           Citric Acid   \n",
       "  6             Volatility            Volatility            Volatility   \n",
       "  7          Fixed Acidity         Fixed Acidity         Fixed Acidity   \n",
       "  8   Total Sulfur Dioxide  Total Sulfur Dioxide  Total Sulfur Dioxide   \n",
       "  9         Residual Sugar        Residual Sugar        Residual Sugar   \n",
       "  10             Chlorides             Chlorides             Chlorides   \n",
       "  \n",
       "                 y_pred_10  \n",
       "  0                Density  \n",
       "  1              Sulphates  \n",
       "  2                Alcohol  \n",
       "  3                  Class  \n",
       "  4    Free Sulfur Dioxide  \n",
       "  5            Citric Acid  \n",
       "  6             Volatility  \n",
       "  7          Fixed Acidity  \n",
       "  8   Total Sulfur Dioxide  \n",
       "  9         Residual Sugar  \n",
       "  10             Chlorides  ,\n",
       "  {'token_length': [82, 154, 260, 365, 630]}],\n",
       " 'pred_df_7': [             y          X     y_pred_0     y_pred_1     y_pred_3     y_pred_5  \\\n",
       "  0        Looks       Loks     Location     Location     Location     Location   \n",
       "  1       Gender       Gend       Gender       Gender       Gender       Gender   \n",
       "  2  Urban/Rural  Urbn/Rura  Urban/Rural  Urban/Rural  Urban/Rural  Urban/Rural   \n",
       "  3       Sports      Sprts       Sports       Sports       Sports       Sports   \n",
       "  4        Goals       Goal        Goals         Goal         Goal         Goal   \n",
       "  5        Money       Mone        Money        Money        Money        Money   \n",
       "  6       Grades       Grds       Grades       Grades       Grades       Grades   \n",
       "  7        Grade       Grad   Graduation        Grade        Grade        Grade   \n",
       "  8       School       Schl       School       School       School       School   \n",
       "  \n",
       "       y_pred_10  \n",
       "  0        Looks  \n",
       "  1       Gender  \n",
       "  2  Urban/Rural  \n",
       "  3       Sports  \n",
       "  4        Goals  \n",
       "  5        Money  \n",
       "  6       Grades  \n",
       "  7   Graduation  \n",
       "  8       School  ,\n",
       "  {'token_length': [60, 101, 152, 197, 309]}],\n",
       " 'pred_df_8': [                 y            X           y_pred_0           y_pred_1  \\\n",
       "  0  Batting_average     BtngAver    Batting Average    Batting Average   \n",
       "  1          Doubles         Doub            Doubles            Doubles   \n",
       "  2          At_bats       Atbats            At Bats            At Bats   \n",
       "  3          Triples        Trpls            Triples            Triples   \n",
       "  4     Games_played      GmsPlyd       Games Played       Games Played   \n",
       "  5       Strikeouts       Strkts         Strikeouts         Strikeouts   \n",
       "  6     Hall_of_Fame   HallOfFame       Hall of Fame       Hall of Fame   \n",
       "  7   Number_seasons  NmbrSeasons  Number of Seasons  Number of Seasons   \n",
       "  8         Position         Pstn           Position           Position   \n",
       "  9            Walks         Wlks              Walks              Walks   \n",
       "  \n",
       "              y_pred_3           y_pred_5          y_pred_10  \n",
       "  0    Batting Average    Batting Average    Batting Average  \n",
       "  1            Doubles            Doubles            Doubles  \n",
       "  2            At Bats            At Bats            At Bats  \n",
       "  3            Triples            Triples            Triples  \n",
       "  4       Games Played       Games Played       Games Played  \n",
       "  5         Strikeouts         Strikeouts            Strikes  \n",
       "  6       Hall of Fame       Hall of Fame       Hall of Fame  \n",
       "  7  Number of Seasons  Number of Seasons  Number of Seasons  \n",
       "  8           Position           Position           Position  \n",
       "  9              Walks              Walks              Walks  ,\n",
       "  {'token_length': [78, 131, 201, 272, 446]}],\n",
       " 'pred_df_9': [                 y          X           y_pred_0         y_pred_1  \\\n",
       "  0       peritoneum      prtnm       Patient Name     Protein Name   \n",
       "  1      mediastinum     mdstnm  Medical Test Name  Metastasis Name   \n",
       "  2            brain       brin  Blood Test Result            Brain   \n",
       "  3        abdominal       abnl    Abnormal Result         Abnormal   \n",
       "  4           pleura       pler     Platelet Count    Proliferation   \n",
       "  5  histologic-type  htlg-Type    Hematology Type   Histology Type   \n",
       "  6      bone-marrow  bone-mrrw   Bone Marrow Test      Bone Marrow   \n",
       "  7            liver        lvr         Liver Test            Liver   \n",
       "  8            class       clas     Classification   Classification   \n",
       "  \n",
       "            y_pred_3         y_pred_5        y_pred_10  \n",
       "  0     Protein Name     Protein Name     Protein Name  \n",
       "  1  Metastasis Name  Metastasis Name  Metastasis Name  \n",
       "  2            Brain            Brain            Brain  \n",
       "  3         Abnormal         Abnormal         Abnormal  \n",
       "  4             Pler    Proliferation    Proliferation  \n",
       "  5   Histology Type   Histology Type   Histology Type  \n",
       "  6      Bone Marrow      Bone Marrow      Bone Marrow  \n",
       "  7            Liver            Liver            Liver  \n",
       "  8   Classification   Classification   Classification  ,\n",
       "  {'token_length': [66, 104, 144, 184, 279]}],\n",
       " 'pred_df_10': [             y         X     y_pred_0     y_pred_1     y_pred_3     y_pred_5  \\\n",
       "  0  Attribute_7    Arbt_7  Attribute 7  Attribute_7  Attribute_7  Attribute_7   \n",
       "  1  Attribute_3     Atrt3  Attribute 3  Attribute_3  Attribute_3  Attribute_3   \n",
       "  2  Attribute_2   Attrbt2  Attribute 2  Attribute_2  Attribute_2  Attribute_2   \n",
       "  3  Attribute_6    Atrb_6  Attribute 6  Attribute_6  Attribute_6  Attribute_6   \n",
       "  4  Attribute_1     Atbt1  Attribute 1  Attribute_1  Attribute_1  Attribute_1   \n",
       "  5  Attribute_5   Attrbt5  Attribute 5  Attribute_5  Attribute_5  Attribute_5   \n",
       "  6  Attribute_4    Atrt_4  Attribute 4  Attribute_4  Attribute_4  Attribute_4   \n",
       "  7  Attribute_8  Attrbt_8  Attribute 8  Attribute_8  Attribute_8  Attribute_8   \n",
       "  8        class      clas        Class        Class        class        class   \n",
       "  9  Attribute_9   Attrbt9  Attribute 9  Attribute_9  Attribute_9  Attribute_9   \n",
       "  \n",
       "       y_pred_10  \n",
       "  0  Attribute_7  \n",
       "  1  Attribute_3  \n",
       "  2  Attribute_2  \n",
       "  3  Attribute_6  \n",
       "  4  Attribute_1  \n",
       "  5  Attribute_5  \n",
       "  6  Attribute_4  \n",
       "  7  Attribute_8  \n",
       "  8        Class  \n",
       "  9  Attribute_9  ,\n",
       "  {'token_length': [79, 127, 185, 243, 388]}],\n",
       " 'pred_df_11': [            y        X    y_pred_0    y_pred_1    y_pred_3    y_pred_5  \\\n",
       "  0   Bid_Close  Bid_Cls   Bid Close   Bid Close   Bid Close   Bid Close   \n",
       "  1       Class     Clas       Class       Class       Class       Class   \n",
       "  2  Ask_Volume  AskVolm  Ask Volume  Ask Volume  Ask Volume  Ask Volume   \n",
       "  3    Bid_Open  BidOpen    Bid Open    Bid Open    Bid Open    Bid Open   \n",
       "  4  Bid_Volume   BidVlm  Bid Volume  Bid Volume  Bid Volume  Bid Volume   \n",
       "  5    Ask_High  AskHigh    Ask High    Ask High    Ask High    Ask High   \n",
       "  6    Bid_High  BidHigh    Bid High    Bid High    Bid High    Bid High   \n",
       "  7    Ask_Open  AskOpen    Ask Open    Ask Open    Ask Open    Ask Open   \n",
       "  8   Ask_Close   AskCls   Ask Close   Ask Close   Ask Close   Ask Close   \n",
       "  \n",
       "      y_pred_10  \n",
       "  0   Bid Close  \n",
       "  1       Class  \n",
       "  2  Ask Volume  \n",
       "  3    Bid Open  \n",
       "  4  Bid Volume  \n",
       "  5    Ask High  \n",
       "  6    Bid High  \n",
       "  7    Ask Open  \n",
       "  8   Ask Close  ,\n",
       "  {'token_length': [64, 126, 217, 302, 518]}],\n",
       " 'pred_df_12': [                                y                        X  \\\n",
       "  0                 total_eve_calls               ttlevecals   \n",
       "  1               total_day_minutes             totaldaymnts   \n",
       "  2             total_night_minutes             totlNghtMnts   \n",
       "  3               total_night_calls            totaNghtCalls   \n",
       "  4              international_plan            intrntnl_plan   \n",
       "  5                 voice_mail_plan           voce_mail_plan   \n",
       "  6                total_day_charge             totaldaychrg   \n",
       "  7                 total_day_calls              totldaycals   \n",
       "  8                           state                     stte   \n",
       "  9                       area_code                 areacode   \n",
       "  10              total_eve_minutes             ttl_eve_mnts   \n",
       "  11  number_customer_service_calls  nmbr_customer_serv_clls   \n",
       "  12                   phone_number                phon_nmbr   \n",
       "  13               total_eve_charge              ttalevechrg   \n",
       "  14             total_night_charge          total_nigh_chrg   \n",
       "  15                          class                     clss   \n",
       "  16                 account_length                acco_lgth   \n",
       "  \n",
       "                              y_pred_0                          y_pred_1  \\\n",
       "  0    Title of the Customer's Account            Total Minutes of Calls   \n",
       "  1                  Total Day Minutes                 Total Day Amounts   \n",
       "  2                Total Night Minutes               Total Night Minutes   \n",
       "  3                  Total Night Calls                 Total Night Calls   \n",
       "  4                 International Plan                International Plan   \n",
       "  5                    Voice Mail Plan                   Voice Mail Plan   \n",
       "  6                   Total Day Charge                  Total Day Charge   \n",
       "  7                    Total Day Calls                   Total Day Calls   \n",
       "  8                              State                             State   \n",
       "  9                          Area Code                         Area Code   \n",
       "  10             Total Evening Minutes             Total Evening Minutes   \n",
       "  11  Number of Customer Service Calls  Number of Customer Service Calls   \n",
       "  12                      Phone Number                      Phone Number   \n",
       "  13              Total Evening Charge              Total Evening Charge   \n",
       "  14                Total Night Charge                Total Night Charge   \n",
       "  15                             Class                             Class   \n",
       "  16                    Account Length                    Account Length   \n",
       "  \n",
       "                              y_pred_3                          y_pred_5  \\\n",
       "  0                Total Evening Calls                   Title Vechicles   \n",
       "  1                  Total Day Minutes                 Total Day Minutes   \n",
       "  2                Total Night Minutes               Total Night Minutes   \n",
       "  3                  Total Night Calls                 Total Night Calls   \n",
       "  4                 International Plan                International Plan   \n",
       "  5                    Voice Mail Plan                   Voice Mail Plan   \n",
       "  6                   Total Day Charge                  Total Day Charge   \n",
       "  7                    Total Day Calls                   Total Day Calls   \n",
       "  8                              State                             State   \n",
       "  9                          Area Code                         Area Code   \n",
       "  10             Total Evening Minutes             Total Evening Minutes   \n",
       "  11  Number of Customer Service Calls  Number of Customer Service Calls   \n",
       "  12                      Phone Number                      Phone Number   \n",
       "  13              Total Evening Charge              Total Evening Charge   \n",
       "  14                Total Night Charge                Total Night Charge   \n",
       "  15                             Class                             Class   \n",
       "  16                    Account Length                    Account Length   \n",
       "  \n",
       "                          y_pred_10  \n",
       "  0                 Title Vechicles  \n",
       "  1               Total Day Minutes  \n",
       "  2             Total Night Minutes  \n",
       "  3               Total Night Calls  \n",
       "  4              International Plan  \n",
       "  5                 Voice Mail Plan  \n",
       "  6                Total Day Charge  \n",
       "  7                 Total Day Calls  \n",
       "  8                           State  \n",
       "  9                       Area Code  \n",
       "  10          Total Evening Minutes  \n",
       "  11  Number Customer Service Calls  \n",
       "  12                   Phone Number  \n",
       "  13           Total Evening Charge  \n",
       "  14             Total Night Charge  \n",
       "  15                          Class  \n",
       "  16                 Account Length  ,\n",
       "  {'token_length': [122, 206, 336, 465, 790]}],\n",
       " 'pred_df_13': [                                 y                     X  \\\n",
       "  0      contribution-to-health-plan   contri-To-Hlth-Plan   \n",
       "  1                    working-hours             wrng-hors   \n",
       "  2                      standby-pay            stndby-pay   \n",
       "  3               shift-differential           shif-differ   \n",
       "  4        wage-increase-second-year  wage-incrs-scnd-year   \n",
       "  5           bereavement-assistance         bvmnt-Assstnc   \n",
       "  6                         duration                  drtn   \n",
       "  7      contribution-to-dental-plan   cntrbt-To-Dntl-Plan   \n",
       "  8              education-allowance   education-Allowance   \n",
       "  9               statutory-holidays           stttry-hdys   \n",
       "  10       cost-of-living-adjustment     cost-Of-Lvng-Adju   \n",
       "  11                         pension                  pnsn   \n",
       "  12                        vacation                  vaca   \n",
       "  13        wage-increase-third-year  wage-inrs-third-year   \n",
       "  14        wage-increase-first-year   wage-incr-frst-year   \n",
       "  15                           class                  clss   \n",
       "  16  longterm-disability-assistance    longterm-disa-asnc   \n",
       "  \n",
       "                              y_pred_0                         y_pred_1  \\\n",
       "  0        Contribution to Health Plan     Contributions to Health Plan   \n",
       "  1                        Wrong Hours                      Wrong Hours   \n",
       "  2                        Standby Pay                      Standby Pay   \n",
       "  3                 Shift Differential               Shift Differential   \n",
       "  4          Wage Increase Second Year       Wage Increases Second Year   \n",
       "  5              Benevolent Assistance           Bereavement Assistance   \n",
       "  6                           Duration                         Duration   \n",
       "  7        Contribution to Dental Plan      Contribution to Dental Plan   \n",
       "  8                Education Allowance              Education Allowance   \n",
       "  9                 Statutory Holidays               Statutory Holidays   \n",
       "  10         Cost of Living Adjustment        Cost of Living Adjustment   \n",
       "  11                           Pension                          Pension   \n",
       "  12                          Vacation                         Vacation   \n",
       "  13          Wage Increase Third Year        Wage Increases Third Year   \n",
       "  14          Wage Increase First Year         Wage Increase First Year   \n",
       "  15                             Class                            Class   \n",
       "  16  Long-term Disability Assistance.  Long-Term Disability Assistance   \n",
       "  \n",
       "                             y_pred_3                         y_pred_5  \\\n",
       "  0      Contributions to Health Plan     Contributions to Health Plan   \n",
       "  1                       Wrong Hours                      Wrong Hours   \n",
       "  2                       Standby Pay                      Standby Pay   \n",
       "  3                Shift Differential               Shift Differential   \n",
       "  4        Wage Increases Second Year       Wage Increases Second Year   \n",
       "  5            Bereavement Assistance           Bereavement Assistance   \n",
       "  6                          Duration                         Duration   \n",
       "  7       Contribution to Dental Plan      Contribution to Dental Plan   \n",
       "  8               Education Allowance              Education Allowance   \n",
       "  9                Statutory Holidays               Statutory Holidays   \n",
       "  10        Cost of Living Adjustment        Cost of Living Adjustment   \n",
       "  11                          Pension                          Pension   \n",
       "  12                         Vacation                         Vacation   \n",
       "  13        Wage Increases Third Year        Wage Increases Third Year   \n",
       "  14         Wage Increase First Year         Wage Increase First Year   \n",
       "  15                            Class                            Class   \n",
       "  16  Long-Term Disability Assistance  Long-Term Disability Assistance   \n",
       "  \n",
       "                            y_pred_10  \n",
       "  0      Contributions to Health Plan  \n",
       "  1                       Wrong Hours  \n",
       "  2                       Standby Pay  \n",
       "  3                Shift Differential  \n",
       "  4        Wage Increases Second Year  \n",
       "  5             Benevolent Assistance  \n",
       "  6                          Duration  \n",
       "  7       Contribution to Dental Plan  \n",
       "  8               Education Allowance  \n",
       "  9                Statutory Holidays  \n",
       "  10        Cost of Living Adjustment  \n",
       "  11                          Pension  \n",
       "  12                         Vacation  \n",
       "  13        Wage Increases Third Year  \n",
       "  14         Wage Increase First Year  \n",
       "  15                            Class  \n",
       "  16  Long-term Disability Assistance  ,\n",
       "  {'token_length': [135, 204, 308, 406, 673]}],\n",
       " 'pred_df_14': [                 y           X             y_pred_0             y_pred_1  \\\n",
       "  0          by_pass      byPass               Bypass               Bypass   \n",
       "  1  regeneration_of    rgnrtnOf     Region of Origin      Regeneration of   \n",
       "  2  exclusion_of_no  excl_of_no  Exclusion of Number  Exclusion of Number   \n",
       "  3    special_forms   spcl_frms        Special Forms        Special Forms   \n",
       "  4   dislocation_of     dsltnof      Desalination of        Desolation of   \n",
       "  5            class        clas       Classification                Class   \n",
       "  \n",
       "            y_pred_3         y_pred_5        y_pred_10  \n",
       "  0           Bypass           Bypass           Bypass  \n",
       "  1  Regeneration of  Regeneration of  Regeneration of  \n",
       "  2  Exclusion of No  Exclusion of No  Exclusion of No  \n",
       "  3    Special Forms    Special Forms    Special Forms  \n",
       "  4    Desolation of   Dislocation of   Dislocation of  \n",
       "  5            Class   Classification            Class  ,\n",
       "  {'token_length': [59, 93, 122, 149, 222]}],\n",
       " 'pred_df_15': [                              y                       X  \\\n",
       "  0     Contraceptive_method_used          Cntrpvmthdused   \n",
       "  1  Number_of_children_ever_born  Nmbrofchildreneverborn   \n",
       "  2      Standard-of-living_index       Sdrd-of-lvng_indx   \n",
       "  3                Media_exposure             Media_expsr   \n",
       "  4               Wifes_education               WifeEdctn   \n",
       "  5                Wifes_religion            WfesReligion   \n",
       "  6           Husbands_occupation               Husb_ocpt   \n",
       "  7                     Wifes_age                  Wfsage   \n",
       "  8            Husbands_education             Hsbnds_educ   \n",
       "  \n",
       "                         y_pred_0                       y_pred_1  \\\n",
       "  0      Contrapptive method used  Counterproductive method used   \n",
       "  1  Number of children ever born   Number of children ever born   \n",
       "  2      Standard of living index       Standard of living index   \n",
       "  3                Media exposure                 Media exposure   \n",
       "  4              Wife's education               Wife's education   \n",
       "  5               Wife's religion                Wife's religion   \n",
       "  6          Husband's occupation           Husband's occupation   \n",
       "  7                    Wife's age                     Wife's age   \n",
       "  8           Husband's education            Husband's education   \n",
       "  \n",
       "                          y_pred_3                       y_pred_5  \\\n",
       "  0  Counterproductive method used  Counterproductive method used   \n",
       "  1   Number of children ever born  Number of children never born   \n",
       "  2       Standard of living index       Standard of living index   \n",
       "  3                 Media exposure                 Media exposure   \n",
       "  4               Wife's education               Wife's education   \n",
       "  5                Wife's religion                Wife's religion   \n",
       "  6           Husband's occupation           Husband's occupation   \n",
       "  7                     Wife's age                     Wife's age   \n",
       "  8            Husband's education            Husband's education   \n",
       "  \n",
       "                         y_pred_10  \n",
       "  0  Counterproductive method used  \n",
       "  1  Number of children never born  \n",
       "  2       Standard of living index  \n",
       "  3                 Media exposure  \n",
       "  4               Wife's education  \n",
       "  5                Wife's religion  \n",
       "  6           Husband's occupation  \n",
       "  7                     Wife's age  \n",
       "  8            Husband's education  ,\n",
       "  {'token_length': [92, 137, 189, 241, 371]}],\n",
       " 'pred_df_16': [             y          X         y_pred_0         y_pred_1          y_pred_3  \\\n",
       "  0  breast-quad  brst-quad  Breast Quadrant  Breast Quadrant   Breast Quadrant   \n",
       "  1        Class       Clss            Class            Class             Class   \n",
       "  2    node-caps  node-Caps    Node Capsules    Node Capsules     Node Capsules   \n",
       "  3    menopause       mnps          Mitoses          Margins  Menopause Status   \n",
       "  4       breast       brea  Bland Chromatin           Breast     Breast Cancer   \n",
       "  5   tumor-size   tmr-Size       Tumor Size       Tumor Size        Tumor Size   \n",
       "  \n",
       "            y_pred_5        y_pred_10  \n",
       "  0  Breast Quadrant  Breast Quadrant  \n",
       "  1            Class            Class  \n",
       "  2    Node Capsules    Node Capsules  \n",
       "  3        Menopause        Menopause  \n",
       "  4           Breast           Breast  \n",
       "  5       Tumor Size       Tumor Size  ,\n",
       "  {'token_length': [57, 96, 133, 172, 267]}],\n",
       " 'pred_df_17': [                 y              X         y_pred_0         y_pred_1  \\\n",
       "  0        Magnesium           Mgns        Magnesium        Magnesium   \n",
       "  1  Color_intensity  Color_intnsty  Color Intensity  Color Intensity   \n",
       "  2            class           clas            Class            Class   \n",
       "  3          Alcohol          Alchl          Alcohol          Alcohol   \n",
       "  4    Total_phenols       Ttalphls    Total Phenols    Total Phenols   \n",
       "  5          Proline           Prln          Proline          Proline   \n",
       "  \n",
       "            y_pred_3         y_pred_5        y_pred_10  \n",
       "  0        Magnesium        Magnesium        Magnesium  \n",
       "  1  Color Intensity  Color Intensity  Color Intensity  \n",
       "  2            Class            Class            Class  \n",
       "  3          Alcohol          Alcohol          Alcohol  \n",
       "  4    Total Phenols    Total Phenols    Total Phenols  \n",
       "  5          Proline          Proline          Proline  ,\n",
       "  {'token_length': [55, 97, 144, 189, 306]}],\n",
       " 'pred_df_18': [                       y                 X              y_pred_0  \\\n",
       "  0          rawgreen-mean        rwgrn-mean        Row Green Mean   \n",
       "  1           rawblue-mean         rwbl-mean         Row Blue Mean   \n",
       "  2   short-line-density-2  shor-line-dnsy-2  Short Line Density 2   \n",
       "  3        saturation-mean         strn-mean           Strain Mean   \n",
       "  4               hue-mean          hue-Mean              Hue Mean   \n",
       "  5             hedge-mean          hdg-Mean          Heading Mean   \n",
       "  6     region-pixel-count    rgin-pxl-count    Region Pixel Count   \n",
       "  7   short-line-density-5  shrt-line-dnsy-5  Short Line Density 5   \n",
       "  8         intensity-mean      intnsty-mean        Intensity Mean   \n",
       "  9             value-mean         vale-mean            Value Mean   \n",
       "  10   region-centroid-col   region-cntr-col  Region Center Column   \n",
       "  11                 class              clas                 Class   \n",
       "  12   region-centroid-row     rgn-Cntrd-Row   Region Centered Row   \n",
       "  \n",
       "                  y_pred_1              y_pred_3              y_pred_5  \\\n",
       "  0         Row Green Mean        Row Green Mean        Row Green Mean   \n",
       "  1          Row Blue Mean         Row Blue Mean         Row Blue Mean   \n",
       "  2   Short Line Density-2  Short Line Density-2  Short Line Density-2   \n",
       "  3            Strain Mean           Strain Mean           Strain Mean   \n",
       "  4               Hue Mean              Hue Mean              Hue Mean   \n",
       "  5           Heading Mean          Heading Mean          Heading Mean   \n",
       "  6     Region Pixel Count    Region Pixel Count    Region Pixel Count   \n",
       "  7   Short Line Density-5  Short Line Density-5  Short Line Density-5   \n",
       "  8         Intensity Mean        Intensity Mean        Intensity Mean   \n",
       "  9             Value Mean            Value Mean            Value Mean   \n",
       "  10   Region Center Color  Region Center Column   Region Center Color   \n",
       "  11                 Class                 Class                 Class   \n",
       "  12   Region Centered Row   Region Centered Row   Region Centered Row   \n",
       "  \n",
       "                 y_pred_10  \n",
       "  0         Row Green Mean  \n",
       "  1          Row Blue Mean  \n",
       "  2   Short Line Density-2  \n",
       "  3            Strain Mean  \n",
       "  4               Hue Mean  \n",
       "  5           Heading Mean  \n",
       "  6     Region Pixel Count  \n",
       "  7   Short Line Density-5  \n",
       "  8         Intensity Mean  \n",
       "  9             Value Mean  \n",
       "  10   Region Center Color  \n",
       "  11                 Class  \n",
       "  12   Region Centered Row  ,\n",
       "  {'token_length': [105, 186, 309, 431, 734]}],\n",
       " 'pred_df_19': [                  y            X                        y_pred_0  \\\n",
       "  0             Class         Clas                  Classification   \n",
       "  1           STEROID         STER                         Steroid   \n",
       "  2         BILIRUBIN         BIIB                        Biologic   \n",
       "  3           VARICES         VACS                        Vaccines   \n",
       "  4           FATIGUE         FATI              Fatty Infiltration   \n",
       "  5           ASCITES         ASCI                         Ascites   \n",
       "  6           ALBUMIN         ALBU                         Albumin   \n",
       "  7        LIVER_FIRM    LIVERFIRM                  Liver Firmness   \n",
       "  8        ANTIVIRALS         ATIV                        Activity   \n",
       "  9           MALAISE         MLIE                      Malignancy   \n",
       "  10         ANOREXIA         ANOR                       Anomalies   \n",
       "  11        HISTOLOGY         HSOG                    Hepatomegaly   \n",
       "  12          SPIDERS         SERS                       Serositis   \n",
       "  13  SPLEEN_PALPABLE  SPLEEN_PLPA  Splenomegaly - Platelet Count.   \n",
       "  \n",
       "                y_pred_1         y_pred_3          y_pred_5        y_pred_10  \n",
       "  0       Classification            Class             Class            Class  \n",
       "  1             Steroids         Steroids          Steroids         Steroids  \n",
       "  2               Biopsy        Bilirubin            Biopsy           Biopsy  \n",
       "  3             Vaccines         Vaccines          Vaccines         Vaccines  \n",
       "  4   Fatty Infiltration          Fatigue           Fatigue          Fatigue  \n",
       "  5              Ascites          Ascites           Ascites          Ascites  \n",
       "  6              Albumin          Albumin           Albumin          Albumin  \n",
       "  7       Liver Firmness   Liver Firmness    Liver Firmness   Liver Firmness  \n",
       "  8             Activity         Activity          Activity         Activity  \n",
       "  9           Malignancy          Malaise        Malignancy       Malignancy  \n",
       "  10            Anorexia         Anorexia          Anorexia         Anorexia  \n",
       "  11        Hepatomegaly     Hepatomegaly      Hepatomegaly     Hepatomegaly  \n",
       "  12        Splenomegaly        Serositis      Splenomegaly        Serositis  \n",
       "  13     Spleen Palpable  Spleen Palpable  Spleen Palpation  Spleen Palpable  ,\n",
       "  {'token_length': [81, 135, 202, 275, 435]}],\n",
       " 'pred_df_20': [                            y                  X                   y_pred_0  \\\n",
       "  0               HOLLOWS_RATIO       HOLLOWSRATIO              Hollows Ratio   \n",
       "  1   MAX.LENGTH_RECTANGULARITY  MAX.LENGTHRECTANG       Max Length Rectangle   \n",
       "  2                       Class               Clas                      Class   \n",
       "  3     MAX.LENGTH_ASPECT_RATIO   MAX.LETHAECTRAIO       Max Length Rectangle   \n",
       "  4                RADIUS_RATIO          RAIS_RTIO               Radius Ratio   \n",
       "  5                 CIRCULARITY              CUARY                  Curvature   \n",
       "  6               ELONGATEDNESS             ELOENS                Ellipticity   \n",
       "  7       SCALED_VARIANCE_MAJOR    SCALEDVARIMAJOR      Scaled Variance Major   \n",
       "  8        SKEWNESS_ABOUT_MAJOR  SKEWNESSABOUTMAJO       Skewness About Major   \n",
       "  9                 COMPACTNESS              COMPA                Compactness   \n",
       "  10  SCALED_RADIUS_OF_GYRATION  SAED_RIUS_OF_GYRA  Scaled Radius of Gyration   \n",
       "  11              SCATTER_RATIO         SCTR_RATIO              Scatter Ratio   \n",
       "  12      SCALED_VARIANCE_MINOR    SCALEDVRACMINOR      Scaled Variance Minor   \n",
       "  13       DISTANCE_CIRCULARITY          DSTECIRCU               Eccentricity   \n",
       "  14       SKEWNESS_ABOUT_MINOR     SKEW_ABUT_MIOR           Skew About Minor   \n",
       "  \n",
       "                       y_pred_1                   y_pred_3  \\\n",
       "  0               Hollows Ratio              Hollows Ratio   \n",
       "  1        Max Length Rectangle       Max Length Rectangle   \n",
       "  2                       Class                      Class   \n",
       "  3        Max Length Rectangle    Max Length Aspect Ratio   \n",
       "  4                Radius Ratio               Radius Ratio   \n",
       "  5                       Curvy                Compactness   \n",
       "  6                      Ellens                 Elongation   \n",
       "  7       Scaled Variance Major      Scaled Variance Major   \n",
       "  8        Skewness About Major       Skewness About Major   \n",
       "  9                 Compactness                Compactness   \n",
       "  10  Scaled Radius of Gyration  Scaled Radius of Gyration   \n",
       "  11              Scatter Ratio              Scatter Ratio   \n",
       "  12      Scaled Variance Minor      Scaled Variance Minor   \n",
       "  13               Eccentricity               Eccentricity   \n",
       "  14           Skew About Minor           Skew About Minor   \n",
       "  \n",
       "                       y_pred_5                  y_pred_10  \n",
       "  0               Hollows Ratio              Hollows Ratio  \n",
       "  1        Max Length Rectangle       Max Length Rectangle  \n",
       "  2                       Class                      Class  \n",
       "  3        Max Length Rectangle       Max Length Rectangle  \n",
       "  4                Radius Ratio               Radius Ratio  \n",
       "  5                   Curvature                  Curvature  \n",
       "  6                 Ellipticity                Ellipticity  \n",
       "  7       Scaled Variance Major      Scaled Variance Major  \n",
       "  8        Skewness About Major       Skewness About Major  \n",
       "  9                 Compactness                Compactness  \n",
       "  10  Scaled Radius of Gyration  Scaled Radius of Gyration  \n",
       "  11              Scatter Ratio              Scatter Ratio  \n",
       "  12      Scaled Variance Minor      Scaled Variance Minor  \n",
       "  13               Eccentricity               Eccentricity  \n",
       "  14           Skew About Minor           Skew About Minor  ,\n",
       "  {'token_length': [121, 194, 301, 408, 676]}]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the predictions in to calculate the performances\n",
    "with open(\"data/complete_preds_experiment_1.pkl\", 'rb') as fp:\n",
    "    pred_dct_inst = pickle.load(fp)\n",
    "pred_dct_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d222c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the average token lengths for all query types\n",
    "token_len_0 = []\n",
    "token_len_1 = []\n",
    "token_len_3 = []\n",
    "token_len_5 = []\n",
    "token_len_10 = []\n",
    "\n",
    "for df,tokens in pred_dct_inst.values():\n",
    "    token_len_0.append(tokens['token_length'][0])\n",
    "    token_len_1.append(tokens['token_length'][1])\n",
    "    token_len_3.append(tokens['token_length'][2])\n",
    "    token_len_5.append(tokens['token_length'][3])\n",
    "    token_len_10.append(tokens['token_length'][4])\n",
    "avg_tokens_0 = mean(token_len_0)\n",
    "avg_tokens_1 = mean(token_len_1)\n",
    "avg_tokens_3 = mean(token_len_3)\n",
    "avg_tokens_5 = mean(token_len_5)\n",
    "avg_tokens_10 = mean(token_len_10)\n",
    "avg_token = [avg_tokens_0, avg_tokens_1, avg_tokens_3, avg_tokens_5, avg_tokens_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95f36a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the model to calculate the bertscore F1 scores\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d9edb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to convert strings into base forms split in a list\n",
    "# Used for calculating EM and F1 scores\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "def convert2base(text: str) -> str:\n",
    "    return \" \".join([t.lemma_ for t in spacy_nlp(text)])\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def replace_underscore_with_space(text):\n",
    "        return text.replace(\"_\", \" \")\n",
    "\n",
    "    def replace_hyphen_with_space(text):\n",
    "        return text.replace(\"-\", \" \")\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    res = []\n",
    "    if \"_\" in s or \"-\" in s:\n",
    "        new_ans1 = white_space_fix(\n",
    "            remove_articles(\n",
    "                remove_punc(\n",
    "                    replace_hyphen_with_space(\n",
    "                        replace_underscore_with_space(lower(s))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        res.append(new_ans1)\n",
    "\n",
    "    new_answer = white_space_fix(remove_articles(remove_punc((lower(s)))))\n",
    "    res.append(new_answer)\n",
    "    new_answer = convert2base(new_answer)\n",
    "    if new_answer not in res:\n",
    "        res.append(new_answer)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84e942a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length'] ['length']\n",
      "['wheel base', 'wheelbase'] ['wheel base', 'wheelbase']\n",
      "['width'] ['width']\n",
      "['engine type'] ['engine type', 'enginetype']\n",
      "['price'] ['price']\n",
      "['aspiration'] ['aspiration']\n",
      "['curb weight'] ['curb weight', 'curbweight']\n",
      "['drive wheels', 'drive wheel'] ['drive wheels', 'drivewheels', 'drivewheel']\n",
      "['fuel system'] ['fuel system', 'fuelsystem']\n",
      "['engine location'] ['engine location', 'enginelocation']\n",
      "['body style'] ['body style', 'bodystyle']\n",
      "['peak rpm'] ['peak rpm', 'peakrpm']\n",
      "['fuel type'] ['fuel type', 'fueltype']\n",
      "['height'] ['height']\n",
      "['compression ratio'] ['compression ratio', 'compressionratio']\n",
      "['horsepower'] ['horsepower']\n",
      "['engine size'] ['engine size', 'enginesize']\n",
      "['stroke'] ['stroke']\n",
      "['unique options', 'unique option'] ['unique operands', 'uniqueoperands', 'uniqueoperand']\n",
      "['design complexity'] ['design complexity', 'designcomplexity']\n",
      "['defects', 'defect'] ['defects', 'defect']\n",
      "['decision count'] ['decision count', 'decisioncount']\n",
      "['multiple content count'] ['multiple condition count', 'multipleconditioncount']\n",
      "['total operations', 'total operation'] ['total operators', 'totaloperators', 'totaloperator']\n",
      "['design density'] ['design density', 'designdensity']\n",
      "['total orders', 'total order'] ['total operands', 'totaloperands', 'totaloperand']\n",
      "['decision density'] ['decision density', 'decisiondensity']\n",
      "['bench count'] ['branch count', 'branchcount']\n",
      "['condition count'] ['condition count', 'conditioncount']\n",
      "['formal parameters', 'formal parameter'] ['formal parameters', 'formalparameters', 'formalparameter']\n",
      "['call pairs', 'call pair'] ['call pairs', 'callpairs', 'callpair']\n",
      "['unique others', 'unique other'] ['unique operators', 'uniqueoperators', 'uniqueoperator']\n",
      "['treatment'] ['tumor']\n",
      "['hypertension'] ['hypopituitary']\n",
      "['pregnancy'] ['pregnant']\n",
      "['lithium'] ['lithium']\n",
      "['query hypothyroidism'] ['query hypothyroid', 'queryhypothyroid']\n",
      "['referral source'] ['referral source', 'referralsource']\n",
      "['thyroid surgery'] ['thyroid surgery', 'thyroidsurgery']\n",
      "['goiter'] ['goitre']\n",
      "['tsh measured', 'tsh measure'] ['tsh measured', 'tshmeasured', 'tshmeasure']\n",
      "['classification'] ['class']\n",
      "['cap shape'] ['cap shape', 'capshape']\n",
      "['stalk surface below ring'] ['stalk surface below ring', 'stalksurfacebelowring', 'stalksurfacebelowre']\n",
      "['gill attachment'] ['gill attachment', 'gillattachment']\n",
      "['stalk color above ring'] ['stalk color above ring', 'stalkcolorabovering', 'stalkcolorabovere']\n",
      "['cap surface'] ['cap surface', 'capsurface']\n",
      "['stalk surface above ring'] ['stalk surface above ring', 'stalksurfaceabovering', 'stalksurfaceabovere']\n",
      "['spore print color'] ['spore print color', 'sporeprintcolor']\n",
      "['veil type'] ['veil type', 'veiltype']\n",
      "['gill color'] ['gill color', 'gillcolor']\n",
      "['gill size'] ['gill size', 'gillsize']\n",
      "['population'] ['population']\n",
      "['habitat'] ['habitat']\n",
      "['stalk color below ring'] ['stalk color below ring', 'stalkcolorbelowring', 'stalkcolorbelowre']\n",
      "['veil color'] ['veil color', 'veilcolor']\n",
      "['ring number'] ['ring number', 'ringnumber']\n",
      "['gill spacing', 'gill space'] ['gill spacing', 'gillspacing', 'gillspace']\n",
      "['stalk shape'] ['stalk shape', 'stalkshape']\n",
      "['stalk root'] ['stalk root', 'stalkroot']\n",
      "['cap color'] ['cap color', 'capcolor']\n",
      "['ring type'] ['ring type', 'ringtype']\n",
      "['class'] ['class']\n",
      "['top right square'] ['top right square', 'toprightsquare']\n",
      "['class'] ['class']\n",
      "['middle left square', 'middle leave square'] ['middle left square', 'middleleftsquare']\n",
      "['bottom middle square'] ['bottom middle square', 'bottommiddlesquare']\n",
      "['middle right square'] ['middle right square', 'middlerightsquare']\n",
      "['top left square', 'top leave square'] ['top left square', 'topleftsquare']\n",
      "['bottom right square'] ['bottom right square', 'bottomrightsquare']\n",
      "['middle middle square'] ['middle middle square', 'middlemiddlesquare']\n",
      "['bottom left square', 'bottom leave square'] ['bottom left square', 'bottomleftsquare']\n",
      "['top middle square'] ['top middle square', 'topmiddlesquare']\n",
      "['density'] ['density']\n",
      "['sugar content'] ['sulphates', 'sulphate']\n",
      "['alcohol content'] ['alcohol']\n",
      "['classification'] ['class']\n",
      "['free sulfur dioxide'] ['free sulfur dioxide', 'freesulfurdioxide']\n",
      "['citric acid'] ['citric acid', 'citricacid']\n",
      "['volatility'] ['volatile acidity', 'volatileacidity']\n",
      "['fixed acidity', 'fix acidity'] ['fixed acidity', 'fixedacidity']\n",
      "['total sulfur dioxide'] ['total sulfur dioxide', 'totalsulfurdioxide']\n",
      "['residual sugar'] ['residual sugar', 'residualsugar']\n",
      "['chloride'] ['chlorides', 'chloride']\n",
      "['location'] ['looks', 'look']\n",
      "['gender'] ['gender']\n",
      "['urbanrural'] ['urbanrural']\n",
      "['sports', 'sport'] ['sports', 'sport']\n",
      "['goals', 'goal'] ['goals', 'goal']\n",
      "['money'] ['money']\n",
      "['grades', 'grade'] ['grades', 'grade']\n",
      "['graduation'] ['grade']\n",
      "['school'] ['school']\n",
      "['batting average'] ['batting average', 'battingaverage']\n",
      "['doubles', 'double'] ['doubles', 'double']\n",
      "['at bats', 'at bat'] ['at bats', 'atbats', 'atbat']\n",
      "['triples', 'triple'] ['triples', 'triple']\n",
      "['games played', 'game play'] ['games played', 'gamesplayed', 'gamesplaye']\n",
      "['strikeouts', 'strikeout'] ['strikeouts', 'strikeout']\n",
      "['hall of fame'] ['hall of fame', 'halloffame']\n",
      "['number of seasons', 'number of season'] ['number seasons', 'numberseasons', 'numberseason']\n",
      "['position'] ['position']\n",
      "['walks', 'walk'] ['walks', 'walk']\n",
      "['patient name'] ['peritoneum']\n",
      "['medical test name'] ['mediastinum']\n",
      "['blood test result'] ['brain']\n",
      "['abnormal result'] ['abdominal']\n",
      "['platelet count'] ['pleura']\n",
      "['hematology type'] ['histologic type', 'histologictype']\n",
      "['bone marrow test'] ['bone marrow', 'bonemarrow']\n",
      "['liver test'] ['liver']\n",
      "['classification'] ['class']\n",
      "['attribute 7'] ['attribute 7', 'attribute7']\n",
      "['attribute 3'] ['attribute 3', 'attribute3']\n",
      "['attribute 2'] ['attribute 2', 'attribute2']\n",
      "['attribute 6'] ['attribute 6', 'attribute6']\n",
      "['attribute 1'] ['attribute 1', 'attribute1']\n",
      "['attribute 5'] ['attribute 5', 'attribute5']\n",
      "['attribute 4'] ['attribute 4', 'attribute4']\n",
      "['attribute 8'] ['attribute 8', 'attribute8']\n",
      "['class'] ['class']\n",
      "['attribute 9'] ['attribute 9', 'attribute9']\n",
      "['bid close'] ['bid close', 'bidclose']\n",
      "['class'] ['class']\n",
      "['ask volume'] ['ask volume', 'askvolume']\n",
      "['bid open'] ['bid open', 'bidopen']\n",
      "['bid volume'] ['bid volume', 'bidvolume']\n",
      "['ask high'] ['ask high', 'askhigh']\n",
      "['bid high'] ['bid high', 'bidhigh']\n",
      "['ask open'] ['ask open', 'askopen']\n",
      "['ask close'] ['ask close', 'askclose']\n",
      "['title of customers account', 'title of customer account'] ['total eve calls', 'totalevecalls', 'totalevecall']\n",
      "['total day minutes', 'total day minute'] ['total day minutes', 'totaldayminutes', 'totaldayminute']\n",
      "['total night minutes', 'total night minute'] ['total night minutes', 'totalnightminutes', 'totalnightminute']\n",
      "['total night calls', 'total night call'] ['total night calls', 'totalnightcalls', 'totalnightcall']\n",
      "['international plan'] ['international plan', 'internationalplan']\n",
      "['voice mail plan'] ['voice mail plan', 'voicemailplan']\n",
      "['total day charge'] ['total day charge', 'totaldaycharge']\n",
      "['total day calls', 'total day call'] ['total day calls', 'totaldaycalls', 'totaldaycall']\n",
      "['state'] ['state']\n",
      "['area code'] ['area code', 'areacode']\n",
      "['total evening minutes', 'total evening minute'] ['total eve minutes', 'totaleveminutes', 'totaleveminute']\n",
      "['number of customer service calls', 'number of customer service call'] ['number customer service calls', 'numbercustomerservicecalls', 'numbercustomerservicecall']\n",
      "['phone number'] ['phone number', 'phonenumber']\n",
      "['total evening charge'] ['total eve charge', 'totalevecharge']\n",
      "['total night charge'] ['total night charge', 'totalnightcharge']\n",
      "['class'] ['class']\n",
      "['account length'] ['account length', 'accountlength']\n",
      "['contribution to health plan'] ['contribution to health plan', 'contributiontohealthplan']\n",
      "['wrong hours', 'wrong hour'] ['working hours', 'workinghours', 'workinghour']\n",
      "['standby pay'] ['standby pay', 'standbypay']\n",
      "['shift differential'] ['shift differential', 'shiftdifferential']\n",
      "['wage increase second year'] ['wage increase second year', 'wageincreasesecondyear']\n",
      "['benevolent assistance'] ['bereavement assistance', 'bereavementassistance']\n",
      "['duration'] ['duration']\n",
      "['contribution to dental plan'] ['contribution to dental plan', 'contributiontodentalplan']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['education allowance'] ['education allowance', 'educationallowance']\n",
      "['statutory holidays', 'statutory holiday'] ['statutory holidays', 'statutoryholidays', 'statutoryholiday']\n",
      "['cost of living adjustment', 'cost of live adjustment'] ['cost of living adjustment', 'costoflivingadjustment']\n",
      "['pension'] ['pension']\n",
      "['vacation'] ['vacation']\n",
      "['wage increase third year'] ['wage increase third year', 'wageincreasethirdyear']\n",
      "['wage increase first year'] ['wage increase first year', 'wageincreasefirstyear']\n",
      "['class'] ['class']\n",
      "['long term disability assistance', 'longterm disability assistance'] ['longterm disability assistance', 'longtermdisabilityassistance']\n",
      "['bypass'] ['by pass', 'bypass']\n",
      "['region of origin'] ['regeneration of', 'regenerationof']\n",
      "['exclusion of number'] ['exclusion of no', 'exclusionofno']\n",
      "['special forms', 'special form'] ['special forms', 'specialforms', 'specialform']\n",
      "['desalination of'] ['dislocation of', 'dislocationof']\n",
      "['classification'] ['class']\n",
      "['contrapptive method used', 'contrapptive method use'] ['contraceptive method used', 'contraceptivemethodused', 'contraceptivemethoduse']\n",
      "['number of children ever born', 'number of child ever bear'] ['number of children ever born', 'numberofchildreneverborn']\n",
      "['standard of living index', 'standard of live index'] ['standard of living index', 'standardoflivingindex']\n",
      "['media exposure', 'medium exposure'] ['media exposure', 'mediaexposure']\n",
      "['wifes education', 'wife education'] ['wifes education', 'wifeseducation']\n",
      "['wifes religion', 'wife religion'] ['wifes religion', 'wifesreligion']\n",
      "['husbands occupation', 'husband occupation'] ['husbands occupation', 'husbandsoccupation']\n",
      "['wifes age', 'wife age'] ['wifes age', 'wifesage']\n",
      "['husbands education', 'husband education'] ['husbands education', 'husbandseducation']\n",
      "['breast quadrant'] ['breast quad', 'breastquad']\n",
      "['class'] ['class']\n",
      "['node capsules', 'node capsule'] ['node caps', 'nodecaps', 'nodecap']\n",
      "['mitoses', 'mitose'] ['menopause']\n",
      "['bland chromatin'] ['breast']\n",
      "['tumor size'] ['tumor size', 'tumorsize']\n",
      "['magnesium'] ['magnesium']\n",
      "['color intensity'] ['color intensity', 'colorintensity']\n",
      "['class'] ['class']\n",
      "['alcohol'] ['alcohol']\n",
      "['total phenols', 'total phenol'] ['total phenols', 'totalphenols', 'totalphenol']\n",
      "['proline'] ['proline']\n",
      "['row green mean'] ['rawgreen mean', 'rawgreenmean']\n",
      "['row blue mean'] ['rawblue mean', 'rawbluemean']\n",
      "['short line density 2'] ['short line density 2', 'shortlinedensity2']\n",
      "['strain mean'] ['saturation mean', 'saturationmean']\n",
      "['hue mean'] ['hue mean', 'huemean']\n",
      "['heading mean', 'head mean'] ['hedge mean', 'hedgemean']\n",
      "['region pixel count'] ['region pixel count', 'regionpixelcount']\n",
      "['short line density 5'] ['short line density 5', 'shortlinedensity5']\n",
      "['intensity mean'] ['intensity mean', 'intensitymean']\n",
      "['value mean'] ['value mean', 'valuemean']\n",
      "['region center column'] ['region centroid col', 'regioncentroidcol']\n",
      "['class'] ['class']\n",
      "['region centered row', 'region center row'] ['region centroid row', 'regioncentroidrow']\n",
      "['classification'] ['class']\n",
      "['steroid'] ['steroid']\n",
      "['biologic'] ['bilirubin']\n",
      "['vaccines', 'vaccine'] ['varices', 'varix']\n",
      "['fatty infiltration'] ['fatigue']\n",
      "['ascites', 'ascite'] ['ascites', 'ascite']\n",
      "['albumin'] ['albumin']\n",
      "['liver firmness'] ['liver firm', 'liverfirm']\n",
      "['activity'] ['antivirals', 'antiviral']\n",
      "['malignancy'] ['malaise']\n",
      "['anomalies', 'anomaly'] ['anorexia']\n",
      "['hepatomegaly'] ['histology']\n",
      "['serositis'] ['spiders', 'spider']\n",
      "['splenomegaly platelet count', 'splenomegaly platelet count'] ['spleen palpable', 'spleenpalpable']\n",
      "['hollows ratio', 'hollow ratio'] ['hollows ratio', 'hollowsratio']\n",
      "['max length rectangle'] ['maxlength rectangularity', 'maxlengthrectangularity']\n",
      "['class'] ['class']\n",
      "['max length rectangle'] ['maxlength aspect ratio', 'maxlengthaspectratio']\n",
      "['radius ratio'] ['radius ratio', 'radiusratio']\n",
      "['curvature'] ['circularity']\n",
      "['ellipticity'] ['elongatedness']\n",
      "['scaled variance major', 'scale variance major'] ['scaled variance major', 'scaledvariancemajor']\n",
      "['skewness about major'] ['skewness about major', 'skewnessaboutmajor']\n",
      "['compactness'] ['compactness']\n",
      "['scaled radius of gyration', 'scale radius of gyration'] ['scaled radius of gyration', 'scaledradiusofgyration']\n",
      "['scatter ratio'] ['scatter ratio', 'scatterratio']\n",
      "['scaled variance minor', 'scale variance minor'] ['scaled variance minor', 'scaledvarianceminor']\n",
      "['eccentricity'] ['distance circularity', 'distancecircularity']\n",
      "['skew about minor'] ['skewness about minor', 'skewnessaboutminor']\n",
      "['length'] ['length']\n",
      "['wheel base', 'wheelbase'] ['wheel base', 'wheelbase']\n",
      "['width'] ['width']\n",
      "['engine type'] ['engine type', 'enginetype']\n",
      "['price'] ['price']\n",
      "['aspiration'] ['aspiration']\n",
      "['curb weight'] ['curb weight', 'curbweight']\n",
      "['drive wheels', 'drive wheel'] ['drive wheels', 'drivewheels', 'drivewheel']\n",
      "['fuel system'] ['fuel system', 'fuelsystem']\n",
      "['engine location'] ['engine location', 'enginelocation']\n",
      "['body style'] ['body style', 'bodystyle']\n",
      "['peak rpm'] ['peak rpm', 'peakrpm']\n",
      "['fuel type'] ['fuel type', 'fueltype']\n",
      "['height'] ['height']\n",
      "['compression ratio'] ['compression ratio', 'compressionratio']\n",
      "['horsepower'] ['horsepower']\n",
      "['engine size'] ['engine size', 'enginesize']\n",
      "['stroke'] ['stroke']\n",
      "['unique operations', 'unique operation'] ['unique operands', 'uniqueoperands', 'uniqueoperand']\n",
      "['design complexity'] ['design complexity', 'designcomplexity']\n",
      "['defects', 'defect'] ['defects', 'defect']\n",
      "['decision count'] ['decision count', 'decisioncount']\n",
      "['multiple contention count'] ['multiple condition count', 'multipleconditioncount']\n",
      "['total operations', 'total operation'] ['total operators', 'totaloperators', 'totaloperator']\n",
      "['design density'] ['design density', 'designdensity']\n",
      "['total opcodes', 'total opcode'] ['total operands', 'totaloperands', 'totaloperand']\n",
      "['decision density'] ['decision density', 'decisiondensity']\n",
      "['branch count'] ['branch count', 'branchcount']\n",
      "['condition count'] ['condition count', 'conditioncount']\n",
      "['formal parameters', 'formal parameter'] ['formal parameters', 'formalparameters', 'formalparameter']\n",
      "['call pairs', 'call pair'] ['call pairs', 'callpairs', 'callpair']\n",
      "['unique others', 'unique other'] ['unique operators', 'uniqueoperators', 'uniqueoperator']\n",
      "['treatment'] ['tumor']\n",
      "['hypertension'] ['hypopituitary']\n",
      "['pregnant'] ['pregnant']\n",
      "['lithium'] ['lithium']\n",
      "['query hypothyroidism'] ['query hypothyroid', 'queryhypothyroid']\n",
      "['referral source'] ['referral source', 'referralsource']\n",
      "['thyroid surgery'] ['thyroid surgery', 'thyroidsurgery']\n",
      "['goiter'] ['goitre']\n",
      "['tsh measured', 'tsh measure'] ['tsh measured', 'tshmeasured', 'tshmeasure']\n",
      "['classification'] ['class']\n",
      "['cap shape'] ['cap shape', 'capshape']\n",
      "['stalk surface below ring'] ['stalk surface below ring', 'stalksurfacebelowring', 'stalksurfacebelowre']\n",
      "['gill attachment'] ['gill attachment', 'gillattachment']\n",
      "['stalk color above ring'] ['stalk color above ring', 'stalkcolorabovering', 'stalkcolorabovere']\n",
      "['cap surface'] ['cap surface', 'capsurface']\n",
      "['stalk surface above ring'] ['stalk surface above ring', 'stalksurfaceabovering', 'stalksurfaceabovere']\n",
      "['spore print color'] ['spore print color', 'sporeprintcolor']\n",
      "['veil type'] ['veil type', 'veiltype']\n",
      "['gill color'] ['gill color', 'gillcolor']\n",
      "['gill size'] ['gill size', 'gillsize']\n",
      "['population'] ['population']\n",
      "['habitat'] ['habitat']\n",
      "['stalk color below ring'] ['stalk color below ring', 'stalkcolorbelowring', 'stalkcolorbelowre']\n",
      "['veil color'] ['veil color', 'veilcolor']\n",
      "['ring number'] ['ring number', 'ringnumber']\n",
      "['gill spacing', 'gill space'] ['gill spacing', 'gillspacing', 'gillspace']\n",
      "['stalk shape'] ['stalk shape', 'stalkshape']\n",
      "['stalk root'] ['stalk root', 'stalkroot']\n",
      "['cap color'] ['cap color', 'capcolor']\n",
      "['ring type'] ['ring type', 'ringtype']\n",
      "['class'] ['class']\n",
      "['top right square'] ['top right square', 'toprightsquare']\n",
      "['class'] ['class']\n",
      "['middle left square', 'middle leave square'] ['middle left square', 'middleleftsquare']\n",
      "['bottom middle square'] ['bottom middle square', 'bottommiddlesquare']\n",
      "['middle right square'] ['middle right square', 'middlerightsquare']\n",
      "['top left square', 'top leave square'] ['top left square', 'topleftsquare']\n",
      "['bottom right square'] ['bottom right square', 'bottomrightsquare']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['middle middle square'] ['middle middle square', 'middlemiddlesquare']\n",
      "['bottom left square', 'bottom leave square'] ['bottom left square', 'bottomleftsquare']\n",
      "['top middle square'] ['top middle square', 'topmiddlesquare']\n",
      "['density'] ['density']\n",
      "['sugar content'] ['sulphates', 'sulphate']\n",
      "['alcohol content'] ['alcohol']\n",
      "['classification'] ['class']\n",
      "['free sulfur dioxide'] ['free sulfur dioxide', 'freesulfurdioxide']\n",
      "['citric acid'] ['citric acid', 'citricacid']\n",
      "['volatility'] ['volatile acidity', 'volatileacidity']\n",
      "['fixed acidity', 'fix acidity'] ['fixed acidity', 'fixedacidity']\n",
      "['total sulfur dioxide'] ['total sulfur dioxide', 'totalsulfurdioxide']\n",
      "['residual sugar'] ['residual sugar', 'residualsugar']\n",
      "['chlorides', 'chloride'] ['chlorides', 'chloride']\n",
      "['location'] ['looks', 'look']\n",
      "['gender'] ['gender']\n",
      "['urbanrural'] ['urbanrural']\n",
      "['sports', 'sport'] ['sports', 'sport']\n",
      "['goal'] ['goals', 'goal']\n",
      "['money'] ['money']\n",
      "['grades', 'grade'] ['grades', 'grade']\n",
      "['grade'] ['grade']\n",
      "['school'] ['school']\n",
      "['batting average'] ['batting average', 'battingaverage']\n",
      "['doubles', 'double'] ['doubles', 'double']\n",
      "['at bats', 'at bat'] ['at bats', 'atbats', 'atbat']\n",
      "['triples', 'triple'] ['triples', 'triple']\n",
      "['games played', 'game play'] ['games played', 'gamesplayed', 'gamesplaye']\n",
      "['strikeouts', 'strikeout'] ['strikeouts', 'strikeout']\n",
      "['hall of fame'] ['hall of fame', 'halloffame']\n",
      "['number of seasons', 'number of season'] ['number seasons', 'numberseasons', 'numberseason']\n",
      "['position'] ['position']\n",
      "['walks', 'walk'] ['walks', 'walk']\n",
      "['protein name'] ['peritoneum']\n",
      "['metastasis name'] ['mediastinum']\n",
      "['brain'] ['brain']\n",
      "['abnormal'] ['abdominal']\n",
      "['proliferation'] ['pleura']\n",
      "['histology type'] ['histologic type', 'histologictype']\n",
      "['bone marrow'] ['bone marrow', 'bonemarrow']\n",
      "['liver'] ['liver']\n",
      "['classification'] ['class']\n",
      "['attribute 7', 'attribute7'] ['attribute 7', 'attribute7']\n",
      "['attribute 3', 'attribute3'] ['attribute 3', 'attribute3']\n",
      "['attribute 2', 'attribute2'] ['attribute 2', 'attribute2']\n",
      "['attribute 6', 'attribute6'] ['attribute 6', 'attribute6']\n",
      "['attribute 1', 'attribute1'] ['attribute 1', 'attribute1']\n",
      "['attribute 5', 'attribute5'] ['attribute 5', 'attribute5']\n",
      "['attribute 4', 'attribute4'] ['attribute 4', 'attribute4']\n",
      "['attribute 8', 'attribute8'] ['attribute 8', 'attribute8']\n",
      "['class'] ['class']\n",
      "['attribute 9', 'attribute9'] ['attribute 9', 'attribute9']\n",
      "['bid close'] ['bid close', 'bidclose']\n",
      "['class'] ['class']\n",
      "['ask volume'] ['ask volume', 'askvolume']\n",
      "['bid open'] ['bid open', 'bidopen']\n",
      "['bid volume'] ['bid volume', 'bidvolume']\n",
      "['ask high'] ['ask high', 'askhigh']\n",
      "['bid high'] ['bid high', 'bidhigh']\n",
      "['ask open'] ['ask open', 'askopen']\n",
      "['ask close'] ['ask close', 'askclose']\n",
      "['total minutes of calls', 'total minute of call'] ['total eve calls', 'totalevecalls', 'totalevecall']\n",
      "['total day amounts', 'total day amount'] ['total day minutes', 'totaldayminutes', 'totaldayminute']\n",
      "['total night minutes', 'total night minute'] ['total night minutes', 'totalnightminutes', 'totalnightminute']\n",
      "['total night calls', 'total night call'] ['total night calls', 'totalnightcalls', 'totalnightcall']\n",
      "['international plan'] ['international plan', 'internationalplan']\n",
      "['voice mail plan'] ['voice mail plan', 'voicemailplan']\n",
      "['total day charge'] ['total day charge', 'totaldaycharge']\n",
      "['total day calls', 'total day call'] ['total day calls', 'totaldaycalls', 'totaldaycall']\n",
      "['state'] ['state']\n",
      "['area code'] ['area code', 'areacode']\n",
      "['total evening minutes', 'total evening minute'] ['total eve minutes', 'totaleveminutes', 'totaleveminute']\n",
      "['number of customer service calls', 'number of customer service call'] ['number customer service calls', 'numbercustomerservicecalls', 'numbercustomerservicecall']\n",
      "['phone number'] ['phone number', 'phonenumber']\n",
      "['total evening charge'] ['total eve charge', 'totalevecharge']\n",
      "['total night charge'] ['total night charge', 'totalnightcharge']\n",
      "['class'] ['class']\n",
      "['account length'] ['account length', 'accountlength']\n",
      "['contributions to health plan', 'contribution to health plan'] ['contribution to health plan', 'contributiontohealthplan']\n",
      "['wrong hours', 'wrong hour'] ['working hours', 'workinghours', 'workinghour']\n",
      "['standby pay'] ['standby pay', 'standbypay']\n",
      "['shift differential'] ['shift differential', 'shiftdifferential']\n",
      "['wage increases second year', 'wage increase second year'] ['wage increase second year', 'wageincreasesecondyear']\n",
      "['bereavement assistance'] ['bereavement assistance', 'bereavementassistance']\n",
      "['duration'] ['duration']\n",
      "['contribution to dental plan'] ['contribution to dental plan', 'contributiontodentalplan']\n",
      "['education allowance'] ['education allowance', 'educationallowance']\n",
      "['statutory holidays', 'statutory holiday'] ['statutory holidays', 'statutoryholidays', 'statutoryholiday']\n",
      "['cost of living adjustment', 'cost of live adjustment'] ['cost of living adjustment', 'costoflivingadjustment']\n",
      "['pension'] ['pension']\n",
      "['vacation'] ['vacation']\n",
      "['wage increases third year', 'wage increase third year'] ['wage increase third year', 'wageincreasethirdyear']\n",
      "['wage increase first year'] ['wage increase first year', 'wageincreasefirstyear']\n",
      "['class'] ['class']\n",
      "['long term disability assistance', 'longterm disability assistance'] ['longterm disability assistance', 'longtermdisabilityassistance']\n",
      "['bypass'] ['by pass', 'bypass']\n",
      "['regeneration of'] ['regeneration of', 'regenerationof']\n",
      "['exclusion of number'] ['exclusion of no', 'exclusionofno']\n",
      "['special forms', 'special form'] ['special forms', 'specialforms', 'specialform']\n",
      "['desolation of'] ['dislocation of', 'dislocationof']\n",
      "['class'] ['class']\n",
      "['counterproductive method used', 'counterproductive method use'] ['contraceptive method used', 'contraceptivemethodused', 'contraceptivemethoduse']\n",
      "['number of children ever born', 'number of child ever bear'] ['number of children ever born', 'numberofchildreneverborn']\n",
      "['standard of living index', 'standard of live index'] ['standard of living index', 'standardoflivingindex']\n",
      "['media exposure', 'medium exposure'] ['media exposure', 'mediaexposure']\n",
      "['wifes education', 'wife education'] ['wifes education', 'wifeseducation']\n",
      "['wifes religion', 'wife religion'] ['wifes religion', 'wifesreligion']\n",
      "['husbands occupation', 'husband occupation'] ['husbands occupation', 'husbandsoccupation']\n",
      "['wifes age', 'wife age'] ['wifes age', 'wifesage']\n",
      "['husbands education', 'husband education'] ['husbands education', 'husbandseducation']\n",
      "['breast quadrant'] ['breast quad', 'breastquad']\n",
      "['class'] ['class']\n",
      "['node capsules', 'node capsule'] ['node caps', 'nodecaps', 'nodecap']\n",
      "['margins', 'margin'] ['menopause']\n",
      "['breast'] ['breast']\n",
      "['tumor size'] ['tumor size', 'tumorsize']\n",
      "['magnesium'] ['magnesium']\n",
      "['color intensity'] ['color intensity', 'colorintensity']\n",
      "['class'] ['class']\n",
      "['alcohol'] ['alcohol']\n",
      "['total phenols', 'total phenol'] ['total phenols', 'totalphenols', 'totalphenol']\n",
      "['proline'] ['proline']\n",
      "['row green mean'] ['rawgreen mean', 'rawgreenmean']\n",
      "['row blue mean'] ['rawblue mean', 'rawbluemean']\n",
      "['short line density 2', 'short line density2'] ['short line density 2', 'shortlinedensity2']\n",
      "['strain mean'] ['saturation mean', 'saturationmean']\n",
      "['hue mean'] ['hue mean', 'huemean']\n",
      "['heading mean', 'head mean'] ['hedge mean', 'hedgemean']\n",
      "['region pixel count'] ['region pixel count', 'regionpixelcount']\n",
      "['short line density 5', 'short line density5'] ['short line density 5', 'shortlinedensity5']\n",
      "['intensity mean'] ['intensity mean', 'intensitymean']\n",
      "['value mean'] ['value mean', 'valuemean']\n",
      "['region center color'] ['region centroid col', 'regioncentroidcol']\n",
      "['class'] ['class']\n",
      "['region centered row', 'region center row'] ['region centroid row', 'regioncentroidrow']\n",
      "['classification'] ['class']\n",
      "['steroids', 'steroid'] ['steroid']\n",
      "['biopsy'] ['bilirubin']\n",
      "['vaccines', 'vaccine'] ['varices', 'varix']\n",
      "['fatty infiltration'] ['fatigue']\n",
      "['ascites', 'ascite'] ['ascites', 'ascite']\n",
      "['albumin'] ['albumin']\n",
      "['liver firmness'] ['liver firm', 'liverfirm']\n",
      "['activity'] ['antivirals', 'antiviral']\n",
      "['malignancy'] ['malaise']\n",
      "['anorexia'] ['anorexia']\n",
      "['hepatomegaly'] ['histology']\n",
      "['splenomegaly'] ['spiders', 'spider']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spleen palpable'] ['spleen palpable', 'spleenpalpable']\n",
      "['hollows ratio', 'hollow ratio'] ['hollows ratio', 'hollowsratio']\n",
      "['max length rectangle'] ['maxlength rectangularity', 'maxlengthrectangularity']\n",
      "['class'] ['class']\n",
      "['max length rectangle'] ['maxlength aspect ratio', 'maxlengthaspectratio']\n",
      "['radius ratio'] ['radius ratio', 'radiusratio']\n",
      "['curvy'] ['circularity']\n",
      "['ellens', 'ellen'] ['elongatedness']\n",
      "['scaled variance major', 'scale variance major'] ['scaled variance major', 'scaledvariancemajor']\n",
      "['skewness about major'] ['skewness about major', 'skewnessaboutmajor']\n",
      "['compactness'] ['compactness']\n",
      "['scaled radius of gyration', 'scale radius of gyration'] ['scaled radius of gyration', 'scaledradiusofgyration']\n",
      "['scatter ratio'] ['scatter ratio', 'scatterratio']\n",
      "['scaled variance minor', 'scale variance minor'] ['scaled variance minor', 'scaledvarianceminor']\n",
      "['eccentricity'] ['distance circularity', 'distancecircularity']\n",
      "['skew about minor'] ['skewness about minor', 'skewnessaboutminor']\n",
      "['length'] ['length']\n",
      "['wheel base', 'wheelbase'] ['wheel base', 'wheelbase']\n",
      "['width'] ['width']\n",
      "['engine type'] ['engine type', 'enginetype']\n",
      "['price'] ['price']\n",
      "['aspiration'] ['aspiration']\n",
      "['curb weight'] ['curb weight', 'curbweight']\n",
      "['drive wheels', 'drive wheel'] ['drive wheels', 'drivewheels', 'drivewheel']\n",
      "['fuel system'] ['fuel system', 'fuelsystem']\n",
      "['engine location'] ['engine location', 'enginelocation']\n",
      "['body style'] ['body style', 'bodystyle']\n",
      "['peak rpm'] ['peak rpm', 'peakrpm']\n",
      "['fuel type'] ['fuel type', 'fueltype']\n",
      "['height'] ['height']\n",
      "['compression ratio'] ['compression ratio', 'compressionratio']\n",
      "['horsepower'] ['horsepower']\n",
      "['engine size'] ['engine size', 'enginesize']\n",
      "['stroke'] ['stroke']\n",
      "['unique operations', 'unique operation'] ['unique operands', 'uniqueoperands', 'uniqueoperand']\n",
      "['design complexity'] ['design complexity', 'designcomplexity']\n",
      "['defects', 'defect'] ['defects', 'defect']\n",
      "['decision count'] ['decision count', 'decisioncount']\n",
      "['multiple condition count'] ['multiple condition count', 'multipleconditioncount']\n",
      "['total operators', 'total operator'] ['total operators', 'totaloperators', 'totaloperator']\n",
      "['design density'] ['design density', 'designdensity']\n",
      "['total operands', 'total operand'] ['total operands', 'totaloperands', 'totaloperand']\n",
      "['decision density'] ['decision density', 'decisiondensity']\n",
      "['branch count'] ['branch count', 'branchcount']\n",
      "['condition count'] ['condition count', 'conditioncount']\n",
      "['formal parameters', 'formal parameter'] ['formal parameters', 'formalparameters', 'formalparameter']\n",
      "['called pairs', 'call pair'] ['call pairs', 'callpairs', 'callpair']\n",
      "['unique others', 'unique other'] ['unique operators', 'uniqueoperators', 'uniqueoperator']\n",
      "['tomorrow'] ['tumor']\n",
      "['hypopatellar'] ['hypopituitary']\n",
      "['pregnant'] ['pregnant']\n",
      "['lithium'] ['lithium']\n",
      "['query hypothyroidism'] ['query hypothyroid', 'queryhypothyroid']\n",
      "['referral source'] ['referral source', 'referralsource']\n",
      "['thyroid surgery'] ['thyroid surgery', 'thyroidsurgery']\n",
      "['goiter'] ['goitre']\n",
      "['thyroid stimulating hormone measured', 'thyroidstimulating hormone measured', 'thyroidstimulate hormone measure'] ['tsh measured', 'tshmeasured', 'tshmeasure']\n",
      "['classification'] ['class']\n",
      "['cap shape'] ['cap shape', 'capshape']\n",
      "['stalk surface below ring'] ['stalk surface below ring', 'stalksurfacebelowring', 'stalksurfacebelowre']\n",
      "['gill attachment'] ['gill attachment', 'gillattachment']\n",
      "['stalk color above ring'] ['stalk color above ring', 'stalkcolorabovering', 'stalkcolorabovere']\n",
      "['cap surface'] ['cap surface', 'capsurface']\n",
      "['stalk surface above ring'] ['stalk surface above ring', 'stalksurfaceabovering', 'stalksurfaceabovere']\n",
      "['spore print color'] ['spore print color', 'sporeprintcolor']\n",
      "['veil type'] ['veil type', 'veiltype']\n",
      "['gill color'] ['gill color', 'gillcolor']\n",
      "['gill size'] ['gill size', 'gillsize']\n",
      "['population'] ['population']\n",
      "['habitat'] ['habitat']\n",
      "['stalk color below ring'] ['stalk color below ring', 'stalkcolorbelowring', 'stalkcolorbelowre']\n",
      "['veil color'] ['veil color', 'veilcolor']\n",
      "['ring number'] ['ring number', 'ringnumber']\n",
      "['gill spacing', 'gill space'] ['gill spacing', 'gillspacing', 'gillspace']\n",
      "['stalk shape'] ['stalk shape', 'stalkshape']\n",
      "['stalk root'] ['stalk root', 'stalkroot']\n",
      "['cap color'] ['cap color', 'capcolor']\n",
      "['ring type'] ['ring type', 'ringtype']\n",
      "['class'] ['class']\n",
      "['top right square'] ['top right square', 'toprightsquare']\n",
      "['class'] ['class']\n",
      "['middle left square', 'middle leave square'] ['middle left square', 'middleleftsquare']\n",
      "['bottom middle square'] ['bottom middle square', 'bottommiddlesquare']\n",
      "['middle right square'] ['middle right square', 'middlerightsquare']\n",
      "['top left square', 'top leave square'] ['top left square', 'topleftsquare']\n",
      "['bottom right square'] ['bottom right square', 'bottomrightsquare']\n",
      "['middle middle square'] ['middle middle square', 'middlemiddlesquare']\n",
      "['bottom left square', 'bottom leave square'] ['bottom left square', 'bottomleftsquare']\n",
      "['top middle square'] ['top middle square', 'topmiddlesquare']\n",
      "['density'] ['density']\n",
      "['sulfates', 'sulfate'] ['sulphates', 'sulphate']\n",
      "['alcohol'] ['alcohol']\n",
      "['class'] ['class']\n",
      "['free sulfur dioxide'] ['free sulfur dioxide', 'freesulfurdioxide']\n",
      "['citric acid'] ['citric acid', 'citricacid']\n",
      "['volatility'] ['volatile acidity', 'volatileacidity']\n",
      "['fixed acidity', 'fix acidity'] ['fixed acidity', 'fixedacidity']\n",
      "['total sulfur dioxide'] ['total sulfur dioxide', 'totalsulfurdioxide']\n",
      "['residual sugar'] ['residual sugar', 'residualsugar']\n",
      "['chlorides', 'chloride'] ['chlorides', 'chloride']\n",
      "['location'] ['looks', 'look']\n",
      "['gender'] ['gender']\n",
      "['urbanrural'] ['urbanrural']\n",
      "['sports', 'sport'] ['sports', 'sport']\n",
      "['goal'] ['goals', 'goal']\n",
      "['money'] ['money']\n",
      "['grades', 'grade'] ['grades', 'grade']\n",
      "['grade'] ['grade']\n",
      "['school'] ['school']\n",
      "['batting average'] ['batting average', 'battingaverage']\n",
      "['doubles', 'double'] ['doubles', 'double']\n",
      "['at bats', 'at bat'] ['at bats', 'atbats', 'atbat']\n",
      "['triples', 'triple'] ['triples', 'triple']\n",
      "['games played', 'game play'] ['games played', 'gamesplayed', 'gamesplaye']\n",
      "['strikeouts', 'strikeout'] ['strikeouts', 'strikeout']\n",
      "['hall of fame'] ['hall of fame', 'halloffame']\n",
      "['number of seasons', 'number of season'] ['number seasons', 'numberseasons', 'numberseason']\n",
      "['position'] ['position']\n",
      "['walks', 'walk'] ['walks', 'walk']\n",
      "['protein name'] ['peritoneum']\n",
      "['metastasis name'] ['mediastinum']\n",
      "['brain'] ['brain']\n",
      "['abnormal'] ['abdominal']\n",
      "['pler'] ['pleura']\n",
      "['histology type'] ['histologic type', 'histologictype']\n",
      "['bone marrow'] ['bone marrow', 'bonemarrow']\n",
      "['liver'] ['liver']\n",
      "['classification'] ['class']\n",
      "['attribute 7', 'attribute7'] ['attribute 7', 'attribute7']\n",
      "['attribute 3', 'attribute3'] ['attribute 3', 'attribute3']\n",
      "['attribute 2', 'attribute2'] ['attribute 2', 'attribute2']\n",
      "['attribute 6', 'attribute6'] ['attribute 6', 'attribute6']\n",
      "['attribute 1', 'attribute1'] ['attribute 1', 'attribute1']\n",
      "['attribute 5', 'attribute5'] ['attribute 5', 'attribute5']\n",
      "['attribute 4', 'attribute4'] ['attribute 4', 'attribute4']\n",
      "['attribute 8', 'attribute8'] ['attribute 8', 'attribute8']\n",
      "['class'] ['class']\n",
      "['attribute 9', 'attribute9'] ['attribute 9', 'attribute9']\n",
      "['bid close'] ['bid close', 'bidclose']\n",
      "['class'] ['class']\n",
      "['ask volume'] ['ask volume', 'askvolume']\n",
      "['bid open'] ['bid open', 'bidopen']\n",
      "['bid volume'] ['bid volume', 'bidvolume']\n",
      "['ask high'] ['ask high', 'askhigh']\n",
      "['bid high'] ['bid high', 'bidhigh']\n",
      "['ask open'] ['ask open', 'askopen']\n",
      "['ask close'] ['ask close', 'askclose']\n",
      "['total evening calls', 'total evening call'] ['total eve calls', 'totalevecalls', 'totalevecall']\n",
      "['total day minutes', 'total day minute'] ['total day minutes', 'totaldayminutes', 'totaldayminute']\n",
      "['total night minutes', 'total night minute'] ['total night minutes', 'totalnightminutes', 'totalnightminute']\n",
      "['total night calls', 'total night call'] ['total night calls', 'totalnightcalls', 'totalnightcall']\n",
      "['international plan'] ['international plan', 'internationalplan']\n",
      "['voice mail plan'] ['voice mail plan', 'voicemailplan']\n",
      "['total day charge'] ['total day charge', 'totaldaycharge']\n",
      "['total day calls', 'total day call'] ['total day calls', 'totaldaycalls', 'totaldaycall']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['state'] ['state']\n",
      "['area code'] ['area code', 'areacode']\n",
      "['total evening minutes', 'total evening minute'] ['total eve minutes', 'totaleveminutes', 'totaleveminute']\n",
      "['number of customer service calls', 'number of customer service call'] ['number customer service calls', 'numbercustomerservicecalls', 'numbercustomerservicecall']\n",
      "['phone number'] ['phone number', 'phonenumber']\n",
      "['total evening charge'] ['total eve charge', 'totalevecharge']\n",
      "['total night charge'] ['total night charge', 'totalnightcharge']\n",
      "['class'] ['class']\n",
      "['account length'] ['account length', 'accountlength']\n",
      "['contributions to health plan', 'contribution to health plan'] ['contribution to health plan', 'contributiontohealthplan']\n",
      "['wrong hours', 'wrong hour'] ['working hours', 'workinghours', 'workinghour']\n",
      "['standby pay'] ['standby pay', 'standbypay']\n",
      "['shift differential'] ['shift differential', 'shiftdifferential']\n",
      "['wage increases second year', 'wage increase second year'] ['wage increase second year', 'wageincreasesecondyear']\n",
      "['bereavement assistance'] ['bereavement assistance', 'bereavementassistance']\n",
      "['duration'] ['duration']\n",
      "['contribution to dental plan'] ['contribution to dental plan', 'contributiontodentalplan']\n",
      "['education allowance'] ['education allowance', 'educationallowance']\n",
      "['statutory holidays', 'statutory holiday'] ['statutory holidays', 'statutoryholidays', 'statutoryholiday']\n",
      "['cost of living adjustment', 'cost of live adjustment'] ['cost of living adjustment', 'costoflivingadjustment']\n",
      "['pension'] ['pension']\n",
      "['vacation'] ['vacation']\n",
      "['wage increases third year', 'wage increase third year'] ['wage increase third year', 'wageincreasethirdyear']\n",
      "['wage increase first year'] ['wage increase first year', 'wageincreasefirstyear']\n",
      "['class'] ['class']\n",
      "['long term disability assistance', 'longterm disability assistance'] ['longterm disability assistance', 'longtermdisabilityassistance']\n",
      "['bypass'] ['by pass', 'bypass']\n",
      "['regeneration of'] ['regeneration of', 'regenerationof']\n",
      "['exclusion of no'] ['exclusion of no', 'exclusionofno']\n",
      "['special forms', 'special form'] ['special forms', 'specialforms', 'specialform']\n",
      "['desolation of'] ['dislocation of', 'dislocationof']\n",
      "['class'] ['class']\n",
      "['counterproductive method used', 'counterproductive method use'] ['contraceptive method used', 'contraceptivemethodused', 'contraceptivemethoduse']\n",
      "['number of children ever born', 'number of child ever bear'] ['number of children ever born', 'numberofchildreneverborn']\n",
      "['standard of living index', 'standard of live index'] ['standard of living index', 'standardoflivingindex']\n",
      "['media exposure', 'medium exposure'] ['media exposure', 'mediaexposure']\n",
      "['wifes education', 'wife education'] ['wifes education', 'wifeseducation']\n",
      "['wifes religion', 'wife religion'] ['wifes religion', 'wifesreligion']\n",
      "['husbands occupation', 'husband occupation'] ['husbands occupation', 'husbandsoccupation']\n",
      "['wifes age', 'wife age'] ['wifes age', 'wifesage']\n",
      "['husbands education', 'husband education'] ['husbands education', 'husbandseducation']\n",
      "['breast quadrant'] ['breast quad', 'breastquad']\n",
      "['class'] ['class']\n",
      "['node capsules', 'node capsule'] ['node caps', 'nodecaps', 'nodecap']\n",
      "['menopause status'] ['menopause']\n",
      "['breast cancer'] ['breast']\n",
      "['tumor size'] ['tumor size', 'tumorsize']\n",
      "['magnesium'] ['magnesium']\n",
      "['color intensity'] ['color intensity', 'colorintensity']\n",
      "['class'] ['class']\n",
      "['alcohol'] ['alcohol']\n",
      "['total phenols', 'total phenol'] ['total phenols', 'totalphenols', 'totalphenol']\n",
      "['proline'] ['proline']\n",
      "['row green mean'] ['rawgreen mean', 'rawgreenmean']\n",
      "['row blue mean'] ['rawblue mean', 'rawbluemean']\n",
      "['short line density 2', 'short line density2'] ['short line density 2', 'shortlinedensity2']\n",
      "['strain mean'] ['saturation mean', 'saturationmean']\n",
      "['hue mean'] ['hue mean', 'huemean']\n",
      "['heading mean', 'head mean'] ['hedge mean', 'hedgemean']\n",
      "['region pixel count'] ['region pixel count', 'regionpixelcount']\n",
      "['short line density 5', 'short line density5'] ['short line density 5', 'shortlinedensity5']\n",
      "['intensity mean'] ['intensity mean', 'intensitymean']\n",
      "['value mean'] ['value mean', 'valuemean']\n",
      "['region center column'] ['region centroid col', 'regioncentroidcol']\n",
      "['class'] ['class']\n",
      "['region centered row', 'region center row'] ['region centroid row', 'regioncentroidrow']\n",
      "['class'] ['class']\n",
      "['steroids', 'steroid'] ['steroid']\n",
      "['bilirubin'] ['bilirubin']\n",
      "['vaccines', 'vaccine'] ['varices', 'varix']\n",
      "['fatigue'] ['fatigue']\n",
      "['ascites', 'ascite'] ['ascites', 'ascite']\n",
      "['albumin'] ['albumin']\n",
      "['liver firmness'] ['liver firm', 'liverfirm']\n",
      "['activity'] ['antivirals', 'antiviral']\n",
      "['malaise'] ['malaise']\n",
      "['anorexia'] ['anorexia']\n",
      "['hepatomegaly'] ['histology']\n",
      "['serositis'] ['spiders', 'spider']\n",
      "['spleen palpable'] ['spleen palpable', 'spleenpalpable']\n",
      "['hollows ratio', 'hollow ratio'] ['hollows ratio', 'hollowsratio']\n",
      "['max length rectangle'] ['maxlength rectangularity', 'maxlengthrectangularity']\n",
      "['class'] ['class']\n",
      "['max length aspect ratio'] ['maxlength aspect ratio', 'maxlengthaspectratio']\n",
      "['radius ratio'] ['radius ratio', 'radiusratio']\n",
      "['compactness'] ['circularity']\n",
      "['elongation'] ['elongatedness']\n",
      "['scaled variance major', 'scale variance major'] ['scaled variance major', 'scaledvariancemajor']\n",
      "['skewness about major'] ['skewness about major', 'skewnessaboutmajor']\n",
      "['compactness'] ['compactness']\n",
      "['scaled radius of gyration', 'scale radius of gyration'] ['scaled radius of gyration', 'scaledradiusofgyration']\n",
      "['scatter ratio'] ['scatter ratio', 'scatterratio']\n",
      "['scaled variance minor', 'scale variance minor'] ['scaled variance minor', 'scaledvarianceminor']\n",
      "['eccentricity'] ['distance circularity', 'distancecircularity']\n",
      "['skew about minor'] ['skewness about minor', 'skewnessaboutminor']\n",
      "['length'] ['length']\n",
      "['wheel base', 'wheelbase'] ['wheel base', 'wheelbase']\n",
      "['width'] ['width']\n",
      "['engine type'] ['engine type', 'enginetype']\n",
      "['price'] ['price']\n",
      "['aspiration'] ['aspiration']\n",
      "['curb weight'] ['curb weight', 'curbweight']\n",
      "['drive wheels', 'drive wheel'] ['drive wheels', 'drivewheels', 'drivewheel']\n",
      "['fuel system'] ['fuel system', 'fuelsystem']\n",
      "['engine location'] ['engine location', 'enginelocation']\n",
      "['body style'] ['body style', 'bodystyle']\n",
      "['peak rpm'] ['peak rpm', 'peakrpm']\n",
      "['fuel type'] ['fuel type', 'fueltype']\n",
      "['height'] ['height']\n",
      "['compression ratio'] ['compression ratio', 'compressionratio']\n",
      "['horsepower'] ['horsepower']\n",
      "['engine size'] ['engine size', 'enginesize']\n",
      "['stroke'] ['stroke']\n",
      "['unique operations', 'unique operation'] ['unique operands', 'uniqueoperands', 'uniqueoperand']\n",
      "['design complexity'] ['design complexity', 'designcomplexity']\n",
      "['defects', 'defect'] ['defects', 'defect']\n",
      "['decision count'] ['decision count', 'decisioncount']\n",
      "['multiple condition count'] ['multiple condition count', 'multipleconditioncount']\n",
      "['total operators', 'total operator'] ['total operators', 'totaloperators', 'totaloperator']\n",
      "['design density'] ['design density', 'designdensity']\n",
      "['total operands', 'total operand'] ['total operands', 'totaloperands', 'totaloperand']\n",
      "['decision density'] ['decision density', 'decisiondensity']\n",
      "['branch count'] ['branch count', 'branchcount']\n",
      "['condition count'] ['condition count', 'conditioncount']\n",
      "['formal parameters', 'formal parameter'] ['formal parameters', 'formalparameters', 'formalparameter']\n",
      "['called pairs', 'call pair'] ['call pairs', 'callpairs', 'callpair']\n",
      "['unique others', 'unique other'] ['unique operators', 'uniqueoperators', 'uniqueoperator']\n",
      "['treatment'] ['tumor']\n",
      "['hypopituitarism'] ['hypopituitary']\n",
      "['pregnant'] ['pregnant']\n",
      "['lithium'] ['lithium']\n",
      "['query hypothyroidism'] ['query hypothyroid', 'queryhypothyroid']\n",
      "['referral source'] ['referral source', 'referralsource']\n",
      "['thyroid surgery'] ['thyroid surgery', 'thyroidsurgery']\n",
      "['goiter'] ['goitre']\n",
      "['tsh measured', 'tsh measure'] ['tsh measured', 'tshmeasured', 'tshmeasure']\n",
      "['classification'] ['class']\n",
      "['cap shape'] ['cap shape', 'capshape']\n",
      "['stalk surface below ring'] ['stalk surface below ring', 'stalksurfacebelowring', 'stalksurfacebelowre']\n",
      "['gill attachment'] ['gill attachment', 'gillattachment']\n",
      "['stalk color above ring'] ['stalk color above ring', 'stalkcolorabovering', 'stalkcolorabovere']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cap surface'] ['cap surface', 'capsurface']\n",
      "['stalk surface above ring'] ['stalk surface above ring', 'stalksurfaceabovering', 'stalksurfaceabovere']\n",
      "['spore print color'] ['spore print color', 'sporeprintcolor']\n",
      "['veil type'] ['veil type', 'veiltype']\n",
      "['gill color'] ['gill color', 'gillcolor']\n",
      "['gill size'] ['gill size', 'gillsize']\n",
      "['population'] ['population']\n",
      "['habitat'] ['habitat']\n",
      "['stalk color below ring'] ['stalk color below ring', 'stalkcolorbelowring', 'stalkcolorbelowre']\n",
      "['veil color'] ['veil color', 'veilcolor']\n",
      "['ring number'] ['ring number', 'ringnumber']\n",
      "['gill spacing', 'gill space'] ['gill spacing', 'gillspacing', 'gillspace']\n",
      "['stalk shape'] ['stalk shape', 'stalkshape']\n",
      "['stalk root'] ['stalk root', 'stalkroot']\n",
      "['cap color'] ['cap color', 'capcolor']\n",
      "['ring type'] ['ring type', 'ringtype']\n",
      "['class'] ['class']\n",
      "['top right square'] ['top right square', 'toprightsquare']\n",
      "['class'] ['class']\n",
      "['middle left square', 'middle leave square'] ['middle left square', 'middleleftsquare']\n",
      "['bottom middle square'] ['bottom middle square', 'bottommiddlesquare']\n",
      "['middle right square'] ['middle right square', 'middlerightsquare']\n",
      "['top left square', 'top leave square'] ['top left square', 'topleftsquare']\n",
      "['bottom right square'] ['bottom right square', 'bottomrightsquare']\n",
      "['middle middle square'] ['middle middle square', 'middlemiddlesquare']\n",
      "['bottom left square', 'bottom leave square'] ['bottom left square', 'bottomleftsquare']\n",
      "['top middle square'] ['top middle square', 'topmiddlesquare']\n",
      "['density'] ['density']\n",
      "['sulphates', 'sulphate'] ['sulphates', 'sulphate']\n",
      "['alcohol'] ['alcohol']\n",
      "['class'] ['class']\n",
      "['free sulfur dioxide'] ['free sulfur dioxide', 'freesulfurdioxide']\n",
      "['citric acid'] ['citric acid', 'citricacid']\n",
      "['volatility'] ['volatile acidity', 'volatileacidity']\n",
      "['fixed acidity', 'fix acidity'] ['fixed acidity', 'fixedacidity']\n",
      "['total sulfur dioxide'] ['total sulfur dioxide', 'totalsulfurdioxide']\n",
      "['residual sugar'] ['residual sugar', 'residualsugar']\n",
      "['chlorides', 'chloride'] ['chlorides', 'chloride']\n",
      "['location'] ['looks', 'look']\n",
      "['gender'] ['gender']\n",
      "['urbanrural'] ['urbanrural']\n",
      "['sports', 'sport'] ['sports', 'sport']\n",
      "['goal'] ['goals', 'goal']\n",
      "['money'] ['money']\n",
      "['grades', 'grade'] ['grades', 'grade']\n",
      "['grade'] ['grade']\n",
      "['school'] ['school']\n",
      "['batting average'] ['batting average', 'battingaverage']\n",
      "['doubles', 'double'] ['doubles', 'double']\n",
      "['at bats', 'at bat'] ['at bats', 'atbats', 'atbat']\n",
      "['triples', 'triple'] ['triples', 'triple']\n",
      "['games played', 'game play'] ['games played', 'gamesplayed', 'gamesplaye']\n",
      "['strikeouts', 'strikeout'] ['strikeouts', 'strikeout']\n",
      "['hall of fame'] ['hall of fame', 'halloffame']\n",
      "['number of seasons', 'number of season'] ['number seasons', 'numberseasons', 'numberseason']\n",
      "['position'] ['position']\n",
      "['walks', 'walk'] ['walks', 'walk']\n",
      "['protein name'] ['peritoneum']\n",
      "['metastasis name'] ['mediastinum']\n",
      "['brain'] ['brain']\n",
      "['abnormal'] ['abdominal']\n",
      "['proliferation'] ['pleura']\n",
      "['histology type'] ['histologic type', 'histologictype']\n",
      "['bone marrow'] ['bone marrow', 'bonemarrow']\n",
      "['liver'] ['liver']\n",
      "['classification'] ['class']\n",
      "['attribute 7', 'attribute7'] ['attribute 7', 'attribute7']\n",
      "['attribute 3', 'attribute3'] ['attribute 3', 'attribute3']\n",
      "['attribute 2', 'attribute2'] ['attribute 2', 'attribute2']\n",
      "['attribute 6', 'attribute6'] ['attribute 6', 'attribute6']\n",
      "['attribute 1', 'attribute1'] ['attribute 1', 'attribute1']\n",
      "['attribute 5', 'attribute5'] ['attribute 5', 'attribute5']\n",
      "['attribute 4', 'attribute4'] ['attribute 4', 'attribute4']\n",
      "['attribute 8', 'attribute8'] ['attribute 8', 'attribute8']\n",
      "['class'] ['class']\n",
      "['attribute 9', 'attribute9'] ['attribute 9', 'attribute9']\n",
      "['bid close'] ['bid close', 'bidclose']\n",
      "['class'] ['class']\n",
      "['ask volume'] ['ask volume', 'askvolume']\n",
      "['bid open'] ['bid open', 'bidopen']\n",
      "['bid volume'] ['bid volume', 'bidvolume']\n",
      "['ask high'] ['ask high', 'askhigh']\n",
      "['bid high'] ['bid high', 'bidhigh']\n",
      "['ask open'] ['ask open', 'askopen']\n",
      "['ask close'] ['ask close', 'askclose']\n",
      "['title vechicles', 'title vechicle'] ['total eve calls', 'totalevecalls', 'totalevecall']\n",
      "['total day minutes', 'total day minute'] ['total day minutes', 'totaldayminutes', 'totaldayminute']\n",
      "['total night minutes', 'total night minute'] ['total night minutes', 'totalnightminutes', 'totalnightminute']\n",
      "['total night calls', 'total night call'] ['total night calls', 'totalnightcalls', 'totalnightcall']\n",
      "['international plan'] ['international plan', 'internationalplan']\n",
      "['voice mail plan'] ['voice mail plan', 'voicemailplan']\n",
      "['total day charge'] ['total day charge', 'totaldaycharge']\n",
      "['total day calls', 'total day call'] ['total day calls', 'totaldaycalls', 'totaldaycall']\n",
      "['state'] ['state']\n",
      "['area code'] ['area code', 'areacode']\n",
      "['total evening minutes', 'total evening minute'] ['total eve minutes', 'totaleveminutes', 'totaleveminute']\n",
      "['number of customer service calls', 'number of customer service call'] ['number customer service calls', 'numbercustomerservicecalls', 'numbercustomerservicecall']\n",
      "['phone number'] ['phone number', 'phonenumber']\n",
      "['total evening charge'] ['total eve charge', 'totalevecharge']\n",
      "['total night charge'] ['total night charge', 'totalnightcharge']\n",
      "['class'] ['class']\n",
      "['account length'] ['account length', 'accountlength']\n",
      "['contributions to health plan', 'contribution to health plan'] ['contribution to health plan', 'contributiontohealthplan']\n",
      "['wrong hours', 'wrong hour'] ['working hours', 'workinghours', 'workinghour']\n",
      "['standby pay'] ['standby pay', 'standbypay']\n",
      "['shift differential'] ['shift differential', 'shiftdifferential']\n",
      "['wage increases second year', 'wage increase second year'] ['wage increase second year', 'wageincreasesecondyear']\n",
      "['bereavement assistance'] ['bereavement assistance', 'bereavementassistance']\n",
      "['duration'] ['duration']\n",
      "['contribution to dental plan'] ['contribution to dental plan', 'contributiontodentalplan']\n",
      "['education allowance'] ['education allowance', 'educationallowance']\n",
      "['statutory holidays', 'statutory holiday'] ['statutory holidays', 'statutoryholidays', 'statutoryholiday']\n",
      "['cost of living adjustment', 'cost of live adjustment'] ['cost of living adjustment', 'costoflivingadjustment']\n",
      "['pension'] ['pension']\n",
      "['vacation'] ['vacation']\n",
      "['wage increases third year', 'wage increase third year'] ['wage increase third year', 'wageincreasethirdyear']\n",
      "['wage increase first year'] ['wage increase first year', 'wageincreasefirstyear']\n",
      "['class'] ['class']\n",
      "['long term disability assistance', 'longterm disability assistance'] ['longterm disability assistance', 'longtermdisabilityassistance']\n",
      "['bypass'] ['by pass', 'bypass']\n",
      "['regeneration of'] ['regeneration of', 'regenerationof']\n",
      "['exclusion of no'] ['exclusion of no', 'exclusionofno']\n",
      "['special forms', 'special form'] ['special forms', 'specialforms', 'specialform']\n",
      "['dislocation of'] ['dislocation of', 'dislocationof']\n",
      "['classification'] ['class']\n",
      "['counterproductive method used', 'counterproductive method use'] ['contraceptive method used', 'contraceptivemethodused', 'contraceptivemethoduse']\n",
      "['number of children never born', 'number of child never bear'] ['number of children ever born', 'numberofchildreneverborn']\n",
      "['standard of living index', 'standard of live index'] ['standard of living index', 'standardoflivingindex']\n",
      "['media exposure', 'medium exposure'] ['media exposure', 'mediaexposure']\n",
      "['wifes education', 'wife education'] ['wifes education', 'wifeseducation']\n",
      "['wifes religion', 'wife religion'] ['wifes religion', 'wifesreligion']\n",
      "['husbands occupation', 'husband occupation'] ['husbands occupation', 'husbandsoccupation']\n",
      "['wifes age', 'wife age'] ['wifes age', 'wifesage']\n",
      "['husbands education', 'husband education'] ['husbands education', 'husbandseducation']\n",
      "['breast quadrant'] ['breast quad', 'breastquad']\n",
      "['class'] ['class']\n",
      "['node capsules', 'node capsule'] ['node caps', 'nodecaps', 'nodecap']\n",
      "['menopause'] ['menopause']\n",
      "['breast'] ['breast']\n",
      "['tumor size'] ['tumor size', 'tumorsize']\n",
      "['magnesium'] ['magnesium']\n",
      "['color intensity'] ['color intensity', 'colorintensity']\n",
      "['class'] ['class']\n",
      "['alcohol'] ['alcohol']\n",
      "['total phenols', 'total phenol'] ['total phenols', 'totalphenols', 'totalphenol']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proline'] ['proline']\n",
      "['row green mean'] ['rawgreen mean', 'rawgreenmean']\n",
      "['row blue mean'] ['rawblue mean', 'rawbluemean']\n",
      "['short line density 2', 'short line density2'] ['short line density 2', 'shortlinedensity2']\n",
      "['strain mean'] ['saturation mean', 'saturationmean']\n",
      "['hue mean'] ['hue mean', 'huemean']\n",
      "['heading mean', 'head mean'] ['hedge mean', 'hedgemean']\n",
      "['region pixel count'] ['region pixel count', 'regionpixelcount']\n",
      "['short line density 5', 'short line density5'] ['short line density 5', 'shortlinedensity5']\n",
      "['intensity mean'] ['intensity mean', 'intensitymean']\n",
      "['value mean'] ['value mean', 'valuemean']\n",
      "['region center color'] ['region centroid col', 'regioncentroidcol']\n",
      "['class'] ['class']\n",
      "['region centered row', 'region center row'] ['region centroid row', 'regioncentroidrow']\n",
      "['class'] ['class']\n",
      "['steroids', 'steroid'] ['steroid']\n",
      "['biopsy'] ['bilirubin']\n",
      "['vaccines', 'vaccine'] ['varices', 'varix']\n",
      "['fatigue'] ['fatigue']\n",
      "['ascites', 'ascite'] ['ascites', 'ascite']\n",
      "['albumin'] ['albumin']\n",
      "['liver firmness'] ['liver firm', 'liverfirm']\n",
      "['activity'] ['antivirals', 'antiviral']\n",
      "['malignancy'] ['malaise']\n",
      "['anorexia'] ['anorexia']\n",
      "['hepatomegaly'] ['histology']\n",
      "['splenomegaly'] ['spiders', 'spider']\n",
      "['spleen palpation'] ['spleen palpable', 'spleenpalpable']\n",
      "['hollows ratio', 'hollow ratio'] ['hollows ratio', 'hollowsratio']\n",
      "['max length rectangle'] ['maxlength rectangularity', 'maxlengthrectangularity']\n",
      "['class'] ['class']\n",
      "['max length rectangle'] ['maxlength aspect ratio', 'maxlengthaspectratio']\n",
      "['radius ratio'] ['radius ratio', 'radiusratio']\n",
      "['curvature'] ['circularity']\n",
      "['ellipticity'] ['elongatedness']\n",
      "['scaled variance major', 'scale variance major'] ['scaled variance major', 'scaledvariancemajor']\n",
      "['skewness about major'] ['skewness about major', 'skewnessaboutmajor']\n",
      "['compactness'] ['compactness']\n",
      "['scaled radius of gyration', 'scale radius of gyration'] ['scaled radius of gyration', 'scaledradiusofgyration']\n",
      "['scatter ratio'] ['scatter ratio', 'scatterratio']\n",
      "['scaled variance minor', 'scale variance minor'] ['scaled variance minor', 'scaledvarianceminor']\n",
      "['eccentricity'] ['distance circularity', 'distancecircularity']\n",
      "['skew about minor'] ['skewness about minor', 'skewnessaboutminor']\n",
      "['length'] ['length']\n",
      "['wheel base', 'wheelbase'] ['wheel base', 'wheelbase']\n",
      "['width'] ['width']\n",
      "['engine type'] ['engine type', 'enginetype']\n",
      "['price'] ['price']\n",
      "['aspiration'] ['aspiration']\n",
      "['curb weight'] ['curb weight', 'curbweight']\n",
      "['drive wheels', 'drive wheel'] ['drive wheels', 'drivewheels', 'drivewheel']\n",
      "['fuel system'] ['fuel system', 'fuelsystem']\n",
      "['engine location'] ['engine location', 'enginelocation']\n",
      "['body style'] ['body style', 'bodystyle']\n",
      "['peak rpm'] ['peak rpm', 'peakrpm']\n",
      "['fuel type'] ['fuel type', 'fueltype']\n",
      "['height'] ['height']\n",
      "['compression ratio'] ['compression ratio', 'compressionratio']\n",
      "['horsepower'] ['horsepower']\n",
      "['engine size'] ['engine size', 'enginesize']\n",
      "['stroke'] ['stroke']\n",
      "['unique operations', 'unique operation'] ['unique operands', 'uniqueoperands', 'uniqueoperand']\n",
      "['design complexity'] ['design complexity', 'designcomplexity']\n",
      "['defects', 'defect'] ['defects', 'defect']\n",
      "['decision count'] ['decision count', 'decisioncount']\n",
      "['multiple condition count'] ['multiple condition count', 'multipleconditioncount']\n",
      "['total operators', 'total operator'] ['total operators', 'totaloperators', 'totaloperator']\n",
      "['design density'] ['design density', 'designdensity']\n",
      "['total operands', 'total operand'] ['total operands', 'totaloperands', 'totaloperand']\n",
      "['decision density'] ['decision density', 'decisiondensity']\n",
      "['branch count'] ['branch count', 'branchcount']\n",
      "['condition count'] ['condition count', 'conditioncount']\n",
      "['formal parameters', 'formal parameter'] ['formal parameters', 'formalparameters', 'formalparameter']\n",
      "['called pairs', 'call pair'] ['call pairs', 'callpairs', 'callpair']\n",
      "['unique others', 'unique other'] ['unique operators', 'uniqueoperators', 'uniqueoperator']\n",
      "['tomorrow'] ['tumor']\n",
      "['hypopatellar'] ['hypopituitary']\n",
      "['pregnant'] ['pregnant']\n",
      "['lithium'] ['lithium']\n",
      "['query hypothyroidism'] ['query hypothyroid', 'queryhypothyroid']\n",
      "['referral source'] ['referral source', 'referralsource']\n",
      "['thyroid surgery'] ['thyroid surgery', 'thyroidsurgery']\n",
      "['goiter'] ['goitre']\n",
      "['thyroid stimulating hormone measured', 'thyroid stimulate hormone measure'] ['tsh measured', 'tshmeasured', 'tshmeasure']\n",
      "['classification'] ['class']\n",
      "['cap shape'] ['cap shape', 'capshape']\n",
      "['stalk surface below ring'] ['stalk surface below ring', 'stalksurfacebelowring', 'stalksurfacebelowre']\n",
      "['gill attachment'] ['gill attachment', 'gillattachment']\n",
      "['stalk color above ring'] ['stalk color above ring', 'stalkcolorabovering', 'stalkcolorabovere']\n",
      "['cap surface'] ['cap surface', 'capsurface']\n",
      "['stalk surface above ring'] ['stalk surface above ring', 'stalksurfaceabovering', 'stalksurfaceabovere']\n",
      "['spore print color'] ['spore print color', 'sporeprintcolor']\n",
      "['veil type'] ['veil type', 'veiltype']\n",
      "['gill color'] ['gill color', 'gillcolor']\n",
      "['gill size'] ['gill size', 'gillsize']\n",
      "['population'] ['population']\n",
      "['habitat'] ['habitat']\n",
      "['stalk color below ring'] ['stalk color below ring', 'stalkcolorbelowring', 'stalkcolorbelowre']\n",
      "['veil color'] ['veil color', 'veilcolor']\n",
      "['ring number'] ['ring number', 'ringnumber']\n",
      "['gill spacing', 'gill space'] ['gill spacing', 'gillspacing', 'gillspace']\n",
      "['stalk shape'] ['stalk shape', 'stalkshape']\n",
      "['stalk root'] ['stalk root', 'stalkroot']\n",
      "['cap color'] ['cap color', 'capcolor']\n",
      "['ring type'] ['ring type', 'ringtype']\n",
      "['class'] ['class']\n",
      "['top right square'] ['top right square', 'toprightsquare']\n",
      "['class'] ['class']\n",
      "['middle left square', 'middle leave square'] ['middle left square', 'middleleftsquare']\n",
      "['bottom middle square'] ['bottom middle square', 'bottommiddlesquare']\n",
      "['middle right square'] ['middle right square', 'middlerightsquare']\n",
      "['top left square', 'top leave square'] ['top left square', 'topleftsquare']\n",
      "['bottom right square'] ['bottom right square', 'bottomrightsquare']\n",
      "['middle middle square'] ['middle middle square', 'middlemiddlesquare']\n",
      "['bottom left square', 'bottom leave square'] ['bottom left square', 'bottomleftsquare']\n",
      "['top middle square'] ['top middle square', 'topmiddlesquare']\n",
      "['density'] ['density']\n",
      "['sulphates', 'sulphate'] ['sulphates', 'sulphate']\n",
      "['alcohol'] ['alcohol']\n",
      "['class'] ['class']\n",
      "['free sulfur dioxide'] ['free sulfur dioxide', 'freesulfurdioxide']\n",
      "['citric acid'] ['citric acid', 'citricacid']\n",
      "['volatility'] ['volatile acidity', 'volatileacidity']\n",
      "['fixed acidity', 'fix acidity'] ['fixed acidity', 'fixedacidity']\n",
      "['total sulfur dioxide'] ['total sulfur dioxide', 'totalsulfurdioxide']\n",
      "['residual sugar'] ['residual sugar', 'residualsugar']\n",
      "['chlorides', 'chloride'] ['chlorides', 'chloride']\n",
      "['looks', 'look'] ['looks', 'look']\n",
      "['gender'] ['gender']\n",
      "['urbanrural'] ['urbanrural']\n",
      "['sports', 'sport'] ['sports', 'sport']\n",
      "['goals', 'goal'] ['goals', 'goal']\n",
      "['money'] ['money']\n",
      "['grades', 'grade'] ['grades', 'grade']\n",
      "['graduation'] ['grade']\n",
      "['school'] ['school']\n",
      "['batting average'] ['batting average', 'battingaverage']\n",
      "['doubles', 'double'] ['doubles', 'double']\n",
      "['at bats', 'at bat'] ['at bats', 'atbats', 'atbat']\n",
      "['triples', 'triple'] ['triples', 'triple']\n",
      "['games played', 'game play'] ['games played', 'gamesplayed', 'gamesplaye']\n",
      "['strikes', 'strike'] ['strikeouts', 'strikeout']\n",
      "['hall of fame'] ['hall of fame', 'halloffame']\n",
      "['number of seasons', 'number of season'] ['number seasons', 'numberseasons', 'numberseason']\n",
      "['position'] ['position']\n",
      "['walks', 'walk'] ['walks', 'walk']\n",
      "['protein name'] ['peritoneum']\n",
      "['metastasis name'] ['mediastinum']\n",
      "['brain'] ['brain']\n",
      "['abnormal'] ['abdominal']\n",
      "['proliferation'] ['pleura']\n",
      "['histology type'] ['histologic type', 'histologictype']\n",
      "['bone marrow'] ['bone marrow', 'bonemarrow']\n",
      "['liver'] ['liver']\n",
      "['classification'] ['class']\n",
      "['attribute 7', 'attribute7'] ['attribute 7', 'attribute7']\n",
      "['attribute 3', 'attribute3'] ['attribute 3', 'attribute3']\n",
      "['attribute 2', 'attribute2'] ['attribute 2', 'attribute2']\n",
      "['attribute 6', 'attribute6'] ['attribute 6', 'attribute6']\n",
      "['attribute 1', 'attribute1'] ['attribute 1', 'attribute1']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attribute 5', 'attribute5'] ['attribute 5', 'attribute5']\n",
      "['attribute 4', 'attribute4'] ['attribute 4', 'attribute4']\n",
      "['attribute 8', 'attribute8'] ['attribute 8', 'attribute8']\n",
      "['class'] ['class']\n",
      "['attribute 9', 'attribute9'] ['attribute 9', 'attribute9']\n",
      "['bid close'] ['bid close', 'bidclose']\n",
      "['class'] ['class']\n",
      "['ask volume'] ['ask volume', 'askvolume']\n",
      "['bid open'] ['bid open', 'bidopen']\n",
      "['bid volume'] ['bid volume', 'bidvolume']\n",
      "['ask high'] ['ask high', 'askhigh']\n",
      "['bid high'] ['bid high', 'bidhigh']\n",
      "['ask open'] ['ask open', 'askopen']\n",
      "['ask close'] ['ask close', 'askclose']\n",
      "['title vechicles', 'title vechicle'] ['total eve calls', 'totalevecalls', 'totalevecall']\n",
      "['total day minutes', 'total day minute'] ['total day minutes', 'totaldayminutes', 'totaldayminute']\n",
      "['total night minutes', 'total night minute'] ['total night minutes', 'totalnightminutes', 'totalnightminute']\n",
      "['total night calls', 'total night call'] ['total night calls', 'totalnightcalls', 'totalnightcall']\n",
      "['international plan'] ['international plan', 'internationalplan']\n",
      "['voice mail plan'] ['voice mail plan', 'voicemailplan']\n",
      "['total day charge'] ['total day charge', 'totaldaycharge']\n",
      "['total day calls', 'total day call'] ['total day calls', 'totaldaycalls', 'totaldaycall']\n",
      "['state'] ['state']\n",
      "['area code'] ['area code', 'areacode']\n",
      "['total evening minutes', 'total evening minute'] ['total eve minutes', 'totaleveminutes', 'totaleveminute']\n",
      "['number customer service calls', 'number customer service call'] ['number customer service calls', 'numbercustomerservicecalls', 'numbercustomerservicecall']\n",
      "['phone number'] ['phone number', 'phonenumber']\n",
      "['total evening charge'] ['total eve charge', 'totalevecharge']\n",
      "['total night charge'] ['total night charge', 'totalnightcharge']\n",
      "['class'] ['class']\n",
      "['account length'] ['account length', 'accountlength']\n",
      "['contributions to health plan', 'contribution to health plan'] ['contribution to health plan', 'contributiontohealthplan']\n",
      "['wrong hours', 'wrong hour'] ['working hours', 'workinghours', 'workinghour']\n",
      "['standby pay'] ['standby pay', 'standbypay']\n",
      "['shift differential'] ['shift differential', 'shiftdifferential']\n",
      "['wage increases second year', 'wage increase second year'] ['wage increase second year', 'wageincreasesecondyear']\n",
      "['benevolent assistance'] ['bereavement assistance', 'bereavementassistance']\n",
      "['duration'] ['duration']\n",
      "['contribution to dental plan'] ['contribution to dental plan', 'contributiontodentalplan']\n",
      "['education allowance'] ['education allowance', 'educationallowance']\n",
      "['statutory holidays', 'statutory holiday'] ['statutory holidays', 'statutoryholidays', 'statutoryholiday']\n",
      "['cost of living adjustment', 'cost of live adjustment'] ['cost of living adjustment', 'costoflivingadjustment']\n",
      "['pension'] ['pension']\n",
      "['vacation'] ['vacation']\n",
      "['wage increases third year', 'wage increase third year'] ['wage increase third year', 'wageincreasethirdyear']\n",
      "['wage increase first year'] ['wage increase first year', 'wageincreasefirstyear']\n",
      "['class'] ['class']\n",
      "['long term disability assistance', 'longterm disability assistance'] ['longterm disability assistance', 'longtermdisabilityassistance']\n",
      "['bypass'] ['by pass', 'bypass']\n",
      "['regeneration of'] ['regeneration of', 'regenerationof']\n",
      "['exclusion of no'] ['exclusion of no', 'exclusionofno']\n",
      "['special forms', 'special form'] ['special forms', 'specialforms', 'specialform']\n",
      "['dislocation of'] ['dislocation of', 'dislocationof']\n",
      "['class'] ['class']\n",
      "['counterproductive method used', 'counterproductive method use'] ['contraceptive method used', 'contraceptivemethodused', 'contraceptivemethoduse']\n",
      "['number of children never born', 'number of child never bear'] ['number of children ever born', 'numberofchildreneverborn']\n",
      "['standard of living index', 'standard of live index'] ['standard of living index', 'standardoflivingindex']\n",
      "['media exposure', 'medium exposure'] ['media exposure', 'mediaexposure']\n",
      "['wifes education', 'wife education'] ['wifes education', 'wifeseducation']\n",
      "['wifes religion', 'wife religion'] ['wifes religion', 'wifesreligion']\n",
      "['husbands occupation', 'husband occupation'] ['husbands occupation', 'husbandsoccupation']\n",
      "['wifes age', 'wife age'] ['wifes age', 'wifesage']\n",
      "['husbands education', 'husband education'] ['husbands education', 'husbandseducation']\n",
      "['breast quadrant'] ['breast quad', 'breastquad']\n",
      "['class'] ['class']\n",
      "['node capsules', 'node capsule'] ['node caps', 'nodecaps', 'nodecap']\n",
      "['menopause'] ['menopause']\n",
      "['breast'] ['breast']\n",
      "['tumor size'] ['tumor size', 'tumorsize']\n",
      "['magnesium'] ['magnesium']\n",
      "['color intensity'] ['color intensity', 'colorintensity']\n",
      "['class'] ['class']\n",
      "['alcohol'] ['alcohol']\n",
      "['total phenols', 'total phenol'] ['total phenols', 'totalphenols', 'totalphenol']\n",
      "['proline'] ['proline']\n",
      "['row green mean'] ['rawgreen mean', 'rawgreenmean']\n",
      "['row blue mean'] ['rawblue mean', 'rawbluemean']\n",
      "['short line density 2', 'short line density2'] ['short line density 2', 'shortlinedensity2']\n",
      "['strain mean'] ['saturation mean', 'saturationmean']\n",
      "['hue mean'] ['hue mean', 'huemean']\n",
      "['heading mean', 'head mean'] ['hedge mean', 'hedgemean']\n",
      "['region pixel count'] ['region pixel count', 'regionpixelcount']\n",
      "['short line density 5', 'short line density5'] ['short line density 5', 'shortlinedensity5']\n",
      "['intensity mean'] ['intensity mean', 'intensitymean']\n",
      "['value mean'] ['value mean', 'valuemean']\n",
      "['region center color'] ['region centroid col', 'regioncentroidcol']\n",
      "['class'] ['class']\n",
      "['region centered row', 'region center row'] ['region centroid row', 'regioncentroidrow']\n",
      "['class'] ['class']\n",
      "['steroids', 'steroid'] ['steroid']\n",
      "['biopsy'] ['bilirubin']\n",
      "['vaccines', 'vaccine'] ['varices', 'varix']\n",
      "['fatigue'] ['fatigue']\n",
      "['ascites', 'ascite'] ['ascites', 'ascite']\n",
      "['albumin'] ['albumin']\n",
      "['liver firmness'] ['liver firm', 'liverfirm']\n",
      "['activity'] ['antivirals', 'antiviral']\n",
      "['malignancy'] ['malaise']\n",
      "['anorexia'] ['anorexia']\n",
      "['hepatomegaly'] ['histology']\n",
      "['serositis'] ['spiders', 'spider']\n",
      "['spleen palpable'] ['spleen palpable', 'spleenpalpable']\n",
      "['hollows ratio', 'hollow ratio'] ['hollows ratio', 'hollowsratio']\n",
      "['max length rectangle'] ['maxlength rectangularity', 'maxlengthrectangularity']\n",
      "['class'] ['class']\n",
      "['max length rectangle'] ['maxlength aspect ratio', 'maxlengthaspectratio']\n",
      "['radius ratio'] ['radius ratio', 'radiusratio']\n",
      "['curvature'] ['circularity']\n",
      "['ellipticity'] ['elongatedness']\n",
      "['scaled variance major', 'scale variance major'] ['scaled variance major', 'scaledvariancemajor']\n",
      "['skewness about major'] ['skewness about major', 'skewnessaboutmajor']\n",
      "['compactness'] ['compactness']\n",
      "['scaled radius of gyration', 'scale radius of gyration'] ['scaled radius of gyration', 'scaledradiusofgyration']\n",
      "['scatter ratio'] ['scatter ratio', 'scatterratio']\n",
      "['scaled variance minor', 'scale variance minor'] ['scaled variance minor', 'scaledvarianceminor']\n",
      "['eccentricity'] ['distance circularity', 'distancecircularity']\n",
      "['skew about minor'] ['skewness about minor', 'skewnessaboutminor']\n"
     ]
    }
   ],
   "source": [
    "def f1_scores_per_type(pred_dct, num):\n",
    "    ''' Function to generate the F1 scores and EM scores per query type'''\n",
    "    f1_scores = []\n",
    "    em_scores = []\n",
    "    for key, value in pred_dct.items():\n",
    "        predictions_1_truth = value[0]['y']\n",
    "        predictions_1_0 = value[0][f'y_pred_{num}']\n",
    "        for references, predictions in zip(predictions_1_truth, predictions_1_0):\n",
    "            refs = normalize_answer(references)\n",
    "            preds = normalize_answer(predictions)\n",
    "            f1_sub_scores = []\n",
    "            em_sub_scores = []\n",
    "            print(preds, refs)\n",
    "            for pred in preds:\n",
    "                for ref in refs:\n",
    "                    f1_sub_scores.append(bertscore.compute(predictions=[pred],references=[ref],model_type=\"distilbert-base-uncased\")['f1'][0])\n",
    "                    if pred == ref:\n",
    "                        em_sub_scores.append(1)\n",
    "                    else:\n",
    "                        em_sub_scores.append(0)\n",
    "            f1_scores.append(max(f1_sub_scores))\n",
    "            em_scores.append(max(em_sub_scores))\n",
    "    return f1_scores, em_scores\n",
    "f1_scores_0, em_scores_0 = f1_scores_per_type(pred_dct_inst, 0)\n",
    "f1_scores_1, em_scores_1 = f1_scores_per_type(pred_dct_inst, 1)\n",
    "f1_scores_3, em_scores_3 = f1_scores_per_type(pred_dct_inst, 3)\n",
    "f1_scores_5, em_scores_5 = f1_scores_per_type(pred_dct_inst, 5)\n",
    "f1_scores_10, em_scores_10 = f1_scores_per_type(pred_dct_inst, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26f26bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 234, 234, 234, 234, 234, 234, 234, 234, 234)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f1_scores_0), len(f1_scores_1), len(f1_scores_3), len(f1_scores_5), len(f1_scores_10), len(em_scores_0), len(em_scores_1), len(em_scores_3), len(em_scores_5), len(em_scores_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6680013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.945578867298925,\n",
       "  0.9560033205227975,\n",
       "  0.9658105131397899,\n",
       "  0.9644685535349398,\n",
       "  0.9626528776099539],\n",
       " [0.717948717948718,\n",
       "  0.7649572649572649,\n",
       "  0.7991452991452992,\n",
       "  0.7991452991452992,\n",
       "  0.7991452991452992])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the avg f1 and em scores\n",
    "avg_f1_0 = mean(f1_scores_0)\n",
    "avg_f1_1 = mean(f1_scores_1)\n",
    "avg_f1_3 = mean(f1_scores_3)\n",
    "avg_f1_5 = mean(f1_scores_5)\n",
    "avg_f1_10 = mean(f1_scores_10)\n",
    "avg_em_0 = mean(em_scores_0)\n",
    "avg_em_1 = mean(em_scores_1)\n",
    "avg_em_3 = mean(em_scores_3)\n",
    "avg_em_5 = mean(em_scores_5)\n",
    "avg_em_10 = mean(em_scores_10)\n",
    "avg_f1 = [avg_f1_0, avg_f1_1, avg_f1_3, avg_f1_5, avg_f1_10]\n",
    "avg_em = [avg_em_0, avg_em_1, avg_em_3, avg_em_5, avg_em_10]\n",
    "avg_f1, avg_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a2c9f746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89, 146.15, 223.35, 299.45, 490]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8101b09d",
   "metadata": {},
   "source": [
    "## Normalized F1 scores for experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d2f52b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01054744971889487\n",
      "0.0065412474890372725\n",
      "0.0043242019840599505\n",
      "0.003220799978410218\n",
      "0.001964597709408069\n"
     ]
    }
   ],
   "source": [
    "for f1, token in zip(avg_f1, avg_token):\n",
    "    if token == 89:\n",
    "        print(f1 / 89.65) # In experiment 2, we slightly changed the query format and then the token length for the base line was 89.65 instead of 89, thats why we changed it here\n",
    "    else:\n",
    "        print(f1 / token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0593c5",
   "metadata": {},
   "source": [
    "## Normalized EM scores for experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5fba448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008008351566633775\n",
      "0.005234055866967259\n",
      "0.003577995518895452\n",
      "0.002668710299366503\n",
      "0.0016309087737659167\n"
     ]
    }
   ],
   "source": [
    "for f1, token in zip(avg_em, avg_token):\n",
    "    if token == 89:\n",
    "        print(f1 / 89.65)\n",
    "    else:\n",
    "        print(f1 / token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a1e85",
   "metadata": {},
   "source": [
    "# Experiment 2: title, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18aa0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dids = [9,1059,40708,24,36,55,50,40691,1100,185,171,40693,41760,40701,54,4,10,23,13,187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef860e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: engine-Lctn | strk | engi-Type | body-styl | drve-whls | peak-Rpm | aspn | engn-Size | wdth | compr-rat | fuel-Type | whl-Base | lnth | hrspwr | heig | pric | fuel-Syst | curb-Weight stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: autos \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: engine-Lctn | strk | engi-Type | body-styl | drve-whls | peak-Rpm | aspn | engn-Size | wdth | compr-rat | fuel-Type | whl-Base | lnth | hrspwr | heig | pric | fuel-Syst | curb-Weight stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)   \n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Automobile) - 1987  \n",
      "**Please cite**:   \n",
      "\n",
      "**1985 Auto Imports Database**  \n",
      "This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars.  The second rating corresponds to the degree to which the auto is more risky than its pric indicates. Cars are initially assigned a risk factor symbol associated with its pric.   Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale.  Actuarians call this process \"symboling\".  A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n",
      " \n",
      "The third factor is the relative average loss payment per insured vehicle year.  This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year.\n",
      " \n",
      "Several of the attributes in the database could be used as a \"class\" attribute.\n",
      "\n",
      "Sources:  \n",
      "1) 1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive Yearbook.\n",
      "2) Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038 \n",
      "3) Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037\n",
      " \n",
      "Past Usage:  \n",
      "Kibler,~D., Aha,~D.~W., & Albert,~M. (1989).  Instance-based prediction of real-valued attributes.  {it Computational Intelligence}, {it 5}, 51--57.\n",
      "\n",
      " \n",
      "Attribute Information:\n",
      ">\n",
      "   1. symboling:                -3, -2, -1, 0, 1, 2, 3.\n",
      "   2. normalized-losses:        continuous from 65 to 256.\n",
      "   3. make:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,\n",
      "                                isuzu, jaguar, mazda, mercedes-benz, mercury,\n",
      "                                mitsubishi, nissan, peugot, plymouth, porsche,\n",
      "                                renault, saab, subaru, toyota, volkswagen, volvo\n",
      "   4. fuel-Type:                diesel, gas.\n",
      "   5. aspn:               std, turbo.\n",
      "   6. num-of-doors:             four, two.\n",
      "   7. body-styl:               hardtop, wagon, sedan, hatchback, convertible.\n",
      "   8. drve-whls:             4wd, fwd, rwd.\n",
      "   9. engine-Lctn:          front, rear.\n",
      "  10. whl-Base:               continuous from 86.6 120.9.\n",
      "  11. lnth:                   continuous from 141.1 to 208.1.\n",
      "  12. wdth:                    continuous from 60.3 to 72.3.\n",
      "  13. heig:                   continuous from 47.8 to 59.8.\n",
      "  14. curb-Weight:              continuous from 1488 to 4066.\n",
      "  15. engi-Type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n",
      "  16. num-of-cylinders:         eight, five, four, six, three, twelve, two.\n",
      "  17. engn-Size:              continuous from 61 to 326.\n",
      "  18. fuel-Syst:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n",
      "  19. bore:                     continuous from 2.54 to 3.94.\n",
      "  20. strk:                   continuous from 2.07 to 4.17.\n",
      "  21. compr-rat:        continuous from 7 to 23.\n",
      "  22. hrspwr:               continuous from 48 to 288.\n",
      "  23. peak-Rpm:                 continuous from 4150 to 6600.\n",
      "  24. city-mpg:                 continuous from 13 to 49.\n",
      "  25. highway-mpg:              continuous from 16 to 54.\n",
      "  26. pric:                    continuous from 5118 to 45400. \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: engine-Lctn | strk | engi-Type | body-styl | drve-whls | peak-Rpm | aspn | engn-Size | wdth | compr-rat | fuel-Type | whl-Base | lnth | hrspwr | heig | pric | fuel-Syst | curb-Weight stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: autos \n",
      "with description: **Author**: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)   \n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Automobile) - 1987  \n",
      "**Please cite**:   \n",
      "\n",
      "**1985 Auto Imports Database**  \n",
      "This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars.  The second rating corresponds to the degree to which the auto is more risky than its pric indicates. Cars are initially assigned a risk factor symbol associated with its pric.   Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale.  Actuarians call this process \"symboling\".  A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n",
      " \n",
      "The third factor is the relative average loss payment per insured vehicle year.  This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year.\n",
      " \n",
      "Several of the attributes in the database could be used as a \"class\" attribute.\n",
      "\n",
      "Sources:  \n",
      "1) 1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive Yearbook.\n",
      "2) Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038 \n",
      "3) Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037\n",
      " \n",
      "Past Usage:  \n",
      "Kibler,~D., Aha,~D.~W., & Albert,~M. (1989).  Instance-based prediction of real-valued attributes.  {it Computational Intelligence}, {it 5}, 51--57.\n",
      "\n",
      " \n",
      "Attribute Information:\n",
      ">\n",
      "   1. symboling:                -3, -2, -1, 0, 1, 2, 3.\n",
      "   2. normalized-losses:        continuous from 65 to 256.\n",
      "   3. make:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,\n",
      "                                isuzu, jaguar, mazda, mercedes-benz, mercury,\n",
      "                                mitsubishi, nissan, peugot, plymouth, porsche,\n",
      "                                renault, saab, subaru, toyota, volkswagen, volvo\n",
      "   4. fuel-Type:                diesel, gas.\n",
      "   5. aspn:               std, turbo.\n",
      "   6. num-of-doors:             four, two.\n",
      "   7. body-styl:               hardtop, wagon, sedan, hatchback, convertible.\n",
      "   8. drve-whls:             4wd, fwd, rwd.\n",
      "   9. engine-Lctn:          front, rear.\n",
      "  10. whl-Base:               continuous from 86.6 120.9.\n",
      "  11. lnth:                   continuous from 141.1 to 208.1.\n",
      "  12. wdth:                    continuous from 60.3 to 72.3.\n",
      "  13. heig:                   continuous from 47.8 to 59.8.\n",
      "  14. curb-Weight:              continuous from 1488 to 4066.\n",
      "  15. engi-Type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n",
      "  16. num-of-cylinders:         eight, five, four, six, three, twelve, two.\n",
      "  17. engn-Size:              continuous from 61 to 326.\n",
      "  18. fuel-Syst:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n",
      "  19. bore:                     continuous from 2.54 to 3.94.\n",
      "  20. strk:                   continuous from 2.07 to 4.17.\n",
      "  21. compr-rat:        continuous from 7 to 23.\n",
      "  22. hrspwr:               continuous from 48 to 288.\n",
      "  23. peak-Rpm:                 continuous from 4150 to 6600.\n",
      "  24. city-mpg:                 continuous from 13 to 49.\n",
      "  25. highway-mpg:              continuous from 16 to 54.\n",
      "  26. pric:                    continuous from 5118 to 45400. \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: engine-Lctn | strk | engi-Type | body-styl | drve-whls | peak-Rpm | aspn | engn-Size | wdth | compr-rat | fuel-Type | whl-Base | lnth | hrspwr | heig | pric | fuel-Syst | curb-Weight stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: dcsn_cnt | callpairs | unqeoper | brnchCnt | dfcs | dcsnDnsy | ttloper | mlpl_cndtn_cnt | condition_cont | dsgn_cmlt | frmlPmts | ttalOprr | unqe_oprtrs | dsgndensity stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: ar1 \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: dcsn_cnt | callpairs | unqeoper | brnchCnt | dfcs | dcsnDnsy | ttloper | mlpl_cndtn_cnt | condition_cont | dsgn_cmlt | frmlPmts | ttalOprr | unqe_oprtrs | dsgndensity stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "%%\n",
      "This is a PROMISE Software Engineering Repository data set made publicly\n",
      "available in order to encourage repeatable, refutable, verifiable, and/or\n",
      "improvable predictive models of software engineering.\n",
      "\n",
      "If you publish material based on PROMISE data sets then, please follow\n",
      "the acknowledgment guidelines posted on the PROMISE repository web page\n",
      "http://promise.site.uottowa.ca/SERepository.\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "--Title: AR1 /Software Defect Prediction\n",
      "--Date: February, 4th, 2009\n",
      "--Data from a Turkish white-goods manufacturer\n",
      "--Donated by: Software Research Laboratory (Softlab),\n",
      "Bogazici University, Istanbul, Turkey\n",
      "--Website: http://softlab.boun.edu.tr\n",
      "--Contact address: ayse.tosun@boun.edu.tr, bener@boun.edu.tr\n",
      "\n",
      "--Description:\n",
      "Embedded software in a white-goods product.\n",
      "Implemented in C.\n",
      "Consists of 121 modules (9 defective / 112 defect-free)\n",
      "29 static code attributes (McCabe, Halstead and LOC measures) and 1 defect information(false/true)\n",
      "Function/method level static code attributes are collected using\n",
      "Prest Metrics Extraction and Analysis Tool [1].\n",
      "[1] Prest Metrics Extraction and Analysis Tool, available at http://softlab.boun.edu.tr/?q=resources&i=tools. \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: dcsn_cnt | callpairs | unqeoper | brnchCnt | dfcs | dcsnDnsy | ttloper | mlpl_cndtn_cnt | condition_cont | dsgn_cmlt | frmlPmts | ttalOprr | unqe_oprtrs | dsgndensity stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: ar1 \n",
      "with description: **Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "%%\n",
      "This is a PROMISE Software Engineering Repository data set made publicly\n",
      "available in order to encourage repeatable, refutable, verifiable, and/or\n",
      "improvable predictive models of software engineering.\n",
      "\n",
      "If you publish material based on PROMISE data sets then, please follow\n",
      "the acknowledgment guidelines posted on the PROMISE repository web page\n",
      "http://promise.site.uottowa.ca/SERepository.\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "--Title: AR1 /Software Defect Prediction\n",
      "--Date: February, 4th, 2009\n",
      "--Data from a Turkish white-goods manufacturer\n",
      "--Donated by: Software Research Laboratory (Softlab),\n",
      "Bogazici University, Istanbul, Turkey\n",
      "--Website: http://softlab.boun.edu.tr\n",
      "--Contact address: ayse.tosun@boun.edu.tr, bener@boun.edu.tr\n",
      "\n",
      "--Description:\n",
      "Embedded software in a white-goods product.\n",
      "Implemented in C.\n",
      "Consists of 121 modules (9 defective / 112 defect-free)\n",
      "29 static code attributes (McCabe, Halstead and LOC measures) and 1 defect information(false/true)\n",
      "Function/method level static code attributes are collected using\n",
      "Prest Metrics Extraction and Analysis Tool [1].\n",
      "[1] Prest Metrics Extraction and Analysis Tool, available at http://softlab.boun.edu.tr/?q=resources&i=tools. \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: dcsn_cnt | callpairs | unqeoper | brnchCnt | dfcs | dcsnDnsy | ttloper | mlpl_cndtn_cnt | condition_cont | dsgn_cmlt | frmlPmts | ttalOprr | unqe_oprtrs | dsgndensity stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: thrdSrgr | tumr | queryhypot | prgnnt | TSHmeas | gtr | lith | hpptry | rfrl_sour | clas stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: allrep \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: thrdSrgr | tumr | queryhypot | prgnnt | TSHmeas | gtr | lith | hpptry | rfrl_sour | clas stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: allrep-pmlb \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: thrdSrgr | tumr | queryhypot | prgnnt | TSHmeas | gtr | lith | hpptry | rfrl_sour | clas stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: allrep \n",
      "with description: allrep-pmlb \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: thrdSrgr | tumr | queryhypot | prgnnt | TSHmeas | gtr | lith | hpptry | rfrl_sour | clas stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: cap-shpe | stlk-Srfc-Blw-Ring | gill-clr | spor-Prnt-Color | cap-clr | ring-numb | stal-root | ring-Type | clas | stlk-shpe | stlk-Colr-Abv-Ring | ppltn | gill-Size | hbtt | veil-Type | stlk-srfc-above-ring | gill-Spac | gill-Atta | stlk-color-belo-ring | veil-Colo | cap-Surf stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: mushroom \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: cap-shpe | stlk-Srfc-Blw-Ring | gill-clr | spor-Prnt-Color | cap-clr | ring-numb | stal-root | ring-Type | clas | stlk-shpe | stlk-Colr-Abv-Ring | ppltn | gill-Size | hbtt | veil-Type | stlk-srfc-above-ring | gill-Spac | gill-Atta | stlk-color-belo-ring | veil-Colo | cap-Surf stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**: [Jeff Schlimmer](Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \n",
      "\n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/mushroom) - 1981     \n",
      "\n",
      "**Please cite**:  The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "\n",
      "\n",
      "This dataset describes mushrooms in terms of their physical characteristics. They are clasified into: poisonous or edible.\n",
      "\n",
      "\n",
      "\n",
      "### Source\n",
      "\n",
      "```\n",
      "\n",
      "(a) Origin: \n",
      "\n",
      "Mushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \n",
      "\n",
      "\n",
      "\n",
      "(b) Donor: \n",
      "\n",
      "Jeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Dataset description\n",
      "\n",
      "\n",
      "\n",
      "This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter clas was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\n",
      "\n",
      "\n",
      "\n",
      "### Attributes Information\n",
      "\n",
      "```\n",
      "\n",
      "1. cap-shpe: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s \n",
      "\n",
      "2. cap-Surf: fibrous=f,grooves=g,scaly=y,smooth=s \n",
      "\n",
      "3. cap-clr: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y \n",
      "\n",
      "4. bruises?: bruises=t,no=f \n",
      "\n",
      "5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s \n",
      "\n",
      "6. gill-Atta: attached=a,descending=d,free=f,notched=n \n",
      "\n",
      "7. gill-Spac: close=c,crowded=w,distant=d \n",
      "\n",
      "8. gill-Size: broad=b,narrow=n \n",
      "\n",
      "9. gill-clr: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y \n",
      "\n",
      "10. stlk-shpe: enlarging=e,tapering=t \n",
      "\n",
      "11. stal-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=? \n",
      "\n",
      "12. stlk-srfc-above-ring: fibrous=f,scaly=y,silky=k,smooth=s \n",
      "\n",
      "13. stlk-Srfc-Blw-Ring: fibrous=f,scaly=y,silky=k,smooth=s \n",
      "\n",
      "14. stlk-Colr-Abv-Ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \n",
      "\n",
      "15. stlk-color-belo-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \n",
      "\n",
      "16. veil-Type: partial=p,universal=u \n",
      "\n",
      "17. veil-Colo: brown=n,orange=o,white=w,yellow=y \n",
      "\n",
      "18. ring-numb: none=n,one=o,two=t \n",
      "\n",
      "19. ring-Type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z \n",
      "\n",
      "20. spor-Prnt-Color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y \n",
      "\n",
      "21. ppltn: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y \n",
      "\n",
      "22. hbtt: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Relevant papers\n",
      "\n",
      "\n",
      "\n",
      "Schlimmer,J.S. (1987). Concept Acquisition Through Representational Adjustment (Technical Report 87-19). Doctoral disseration, Department of Information and Computer Science, University of California, Irvine. \n",
      "\n",
      "\n",
      "\n",
      "Iba,W., Wogulis,J., & Langley,P. (1988). Trading off Simplicity and Coverage in Incremental Concept Learning. In Proceedings of the 5th International Conference on Machine Learning, 73-79. Ann Arbor, Michigan: Morgan Kaufmann. \n",
      "\n",
      "\n",
      "\n",
      "Duch W, Adamczak R, Grabczewski K (1996) Extraction of logical rules from training data using backpropagation networks, in: Proc. of the The 1st Online Workshop on Soft Computing, 19-30.Aug.1996, pp. 25-30, [Web Link] \n",
      "\n",
      "\n",
      "\n",
      "Duch W, Adamczak R, Grabczewski K, Ishikawa M, Ueda H, Extraction of crisp logical rules using constrained backpropagation networks - comparison of two new approaches, in: Proc. of the European Symposium on Artificial Neural Networks (ESANN'97), Bruge, Belgium 16-18.4.1997. \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: cap-shpe | stlk-Srfc-Blw-Ring | gill-clr | spor-Prnt-Color | cap-clr | ring-numb | stal-root | ring-Type | clas | stlk-shpe | stlk-Colr-Abv-Ring | ppltn | gill-Size | hbtt | veil-Type | stlk-srfc-above-ring | gill-Spac | gill-Atta | stlk-color-belo-ring | veil-Colo | cap-Surf stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: mushroom \n",
      "with description: **Author**: [Jeff Schlimmer](Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \n",
      "\n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/mushroom) - 1981     \n",
      "\n",
      "**Please cite**:  The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "\n",
      "\n",
      "This dataset describes mushrooms in terms of their physical characteristics. They are clasified into: poisonous or edible.\n",
      "\n",
      "\n",
      "\n",
      "### Source\n",
      "\n",
      "```\n",
      "\n",
      "(a) Origin: \n",
      "\n",
      "Mushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \n",
      "\n",
      "\n",
      "\n",
      "(b) Donor: \n",
      "\n",
      "Jeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Dataset description\n",
      "\n",
      "\n",
      "\n",
      "This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter clas was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\n",
      "\n",
      "\n",
      "\n",
      "### Attributes Information\n",
      "\n",
      "```\n",
      "\n",
      "1. cap-shpe: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s \n",
      "\n",
      "2. cap-Surf: fibrous=f,grooves=g,scaly=y,smooth=s \n",
      "\n",
      "3. cap-clr: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y \n",
      "\n",
      "4. bruises?: bruises=t,no=f \n",
      "\n",
      "5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s \n",
      "\n",
      "6. gill-Atta: attached=a,descending=d,free=f,notched=n \n",
      "\n",
      "7. gill-Spac: close=c,crowded=w,distant=d \n",
      "\n",
      "8. gill-Size: broad=b,narrow=n \n",
      "\n",
      "9. gill-clr: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y \n",
      "\n",
      "10. stlk-shpe: enlarging=e,tapering=t \n",
      "\n",
      "11. stal-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=? \n",
      "\n",
      "12. stlk-srfc-above-ring: fibrous=f,scaly=y,silky=k,smooth=s \n",
      "\n",
      "13. stlk-Srfc-Blw-Ring: fibrous=f,scaly=y,silky=k,smooth=s \n",
      "\n",
      "14. stlk-Colr-Abv-Ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \n",
      "\n",
      "15. stlk-color-belo-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \n",
      "\n",
      "16. veil-Type: partial=p,universal=u \n",
      "\n",
      "17. veil-Colo: brown=n,orange=o,white=w,yellow=y \n",
      "\n",
      "18. ring-numb: none=n,one=o,two=t \n",
      "\n",
      "19. ring-Type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z \n",
      "\n",
      "20. spor-Prnt-Color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y \n",
      "\n",
      "21. ppltn: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y \n",
      "\n",
      "22. hbtt: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Relevant papers\n",
      "\n",
      "\n",
      "\n",
      "Schlimmer,J.S. (1987). Concept Acquisition Through Representational Adjustment (Technical Report 87-19). Doctoral disseration, Department of Information and Computer Science, University of California, Irvine. \n",
      "\n",
      "\n",
      "\n",
      "Iba,W., Wogulis,J., & Langley,P. (1988). Trading off Simplicity and Coverage in Incremental Concept Learning. In Proceedings of the 5th International Conference on Machine Learning, 73-79. Ann Arbor, Michigan: Morgan Kaufmann. \n",
      "\n",
      "\n",
      "\n",
      "Duch W, Adamczak R, Grabczewski K (1996) Extraction of logical rules from training data using backpropagation networks, in: Proc. of the The 1st Online Workshop on Soft Computing, 19-30.Aug.1996, pp. 25-30, [Web Link] \n",
      "\n",
      "\n",
      "\n",
      "Duch W, Adamczak R, Grabczewski K, Ishikawa M, Ueda H, Extraction of crisp logical rules using constrained backpropagation networks - comparison of two new approaches, in: Proc. of the European Symposium on Artificial Neural Networks (ESANN'97), Bruge, Belgium 16-18.4.1997. \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: cap-shpe | stlk-Srfc-Blw-Ring | gill-clr | spor-Prnt-Color | cap-clr | ring-numb | stal-root | ring-Type | clas | stlk-shpe | stlk-Colr-Abv-Ring | ppltn | gill-Size | hbtt | veil-Type | stlk-srfc-above-ring | gill-Spac | gill-Atta | stlk-color-belo-ring | veil-Colo | cap-Surf stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: region-cntr-col | intnsty-mean | hue-Mean | srtn-mean | shor-line-dnty-5 | hdge-mean | shrt-Line-Dnst-2 | rgn-cnrd-row | clss | val-Mean | rwbl-mean | rwgr-Mean | rgn-pxl-cont stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: segment \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: region-cntr-col | intnsty-mean | hue-Mean | srtn-mean | shor-line-dnty-5 | hdge-mean | shrt-Line-Dnst-2 | rgn-cnrd-row | clss | val-Mean | rwbl-mean | rwgr-Mean | rgn-pxl-cont stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**: University of Massachusetts Vision Group, Carla Brodley  \n",
      "\n",
      "**Source**: [UCI](http://archive.ics.uci.edu/ml/datasets/image+segmentation) - 1990  \n",
      "\n",
      "**Please cite**: [UCI](http://archive.ics.uci.edu/ml/citation_policy.html)  \n",
      "\n",
      "\n",
      "\n",
      "**Image Segmentation Data Set**\n",
      "\n",
      "The instances were drawn randomly from a database of 7 outdoor images. The images were hand-segmented to create a clssification for every pixel. Each instance is a 3x3 region.\n",
      "\n",
      " \n",
      "\n",
      "### Attribute Information  \n",
      "\n",
      "\n",
      "\n",
      "1.  region-cntr-col:  the column of the center pixel of the region.\n",
      "\n",
      "2.  rgn-cnrd-row:  the row of the center pixel of the region.\n",
      "\n",
      "3.  rgn-pxl-cont:  the number of pixels in a region = 9.\n",
      "\n",
      "4.  shor-line-dnty-5:  the results of a line extractoin algorithm that \n",
      "\n",
      "          counts how many lines of length 5 (any orientation) with\n",
      "\n",
      "          low contrast, less than or equal to 5, go through the region.\n",
      "\n",
      "5.  shrt-Line-Dnst-2:  same as shor-line-dnty-5 but counts lines\n",
      "\n",
      "          of high contrast, greater than 5.\n",
      "\n",
      "6.  vedge-mean:  measure the contrast of horizontally\n",
      "\n",
      "          adjacent pixels in the region.  There are 6, the mean and \n",
      "\n",
      "          standard deviation are given.  This attribute is used as\n",
      "\n",
      "         a vertical edge detector.\n",
      "\n",
      "7.  vegde-sd:  (see 6)\n",
      "\n",
      "8.  hdge-mean:  measures the contrast of vertically adjacent\n",
      "\n",
      "           pixels. Used for horizontal line detection. \n",
      "\n",
      "9.  hedge-sd: (see 8).\n",
      "\n",
      "10. intnsty-mean:  the average over the region of (R + G + B)/3\n",
      "\n",
      "11. rawred-mean: the average over the region of the R value.\n",
      "\n",
      "12. rwbl-mean: the average over the region of the B value.\n",
      "\n",
      "13. rwgr-Mean: the average over the region of the G value.\n",
      "\n",
      "14. exred-mean: measure the excess red:  (2R - (G + B))\n",
      "\n",
      "15. exblue-mean: measure the excess blue:  (2B - (G + R))\n",
      "\n",
      "16. exgreen-mean: measure the excess green:  (2G - (R + B))\n",
      "\n",
      "17. val-Mean:  3-d nonlinear transformation\n",
      "\n",
      "          of RGB. (Algorithm can be found in Foley and VanDam, Fundamentals\n",
      "\n",
      "          of Interactive Computer Graphics)\n",
      "\n",
      "18. saturatoin-mean:  (see 17)\n",
      "\n",
      "19. hue-Mean:  (see 17) \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: region-cntr-col | intnsty-mean | hue-Mean | srtn-mean | shor-line-dnty-5 | hdge-mean | shrt-Line-Dnst-2 | rgn-cnrd-row | clss | val-Mean | rwbl-mean | rwgr-Mean | rgn-pxl-cont stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: segment \n",
      "with description: **Author**: University of Massachusetts Vision Group, Carla Brodley  \n",
      "\n",
      "**Source**: [UCI](http://archive.ics.uci.edu/ml/datasets/image+segmentation) - 1990  \n",
      "\n",
      "**Please cite**: [UCI](http://archive.ics.uci.edu/ml/citation_policy.html)  \n",
      "\n",
      "\n",
      "\n",
      "**Image Segmentation Data Set**\n",
      "\n",
      "The instances were drawn randomly from a database of 7 outdoor images. The images were hand-segmented to create a clssification for every pixel. Each instance is a 3x3 region.\n",
      "\n",
      " \n",
      "\n",
      "### Attribute Information  \n",
      "\n",
      "\n",
      "\n",
      "1.  region-cntr-col:  the column of the center pixel of the region.\n",
      "\n",
      "2.  rgn-cnrd-row:  the row of the center pixel of the region.\n",
      "\n",
      "3.  rgn-pxl-cont:  the number of pixels in a region = 9.\n",
      "\n",
      "4.  shor-line-dnty-5:  the results of a line extractoin algorithm that \n",
      "\n",
      "          counts how many lines of length 5 (any orientation) with\n",
      "\n",
      "          low contrast, less than or equal to 5, go through the region.\n",
      "\n",
      "5.  shrt-Line-Dnst-2:  same as shor-line-dnty-5 but counts lines\n",
      "\n",
      "          of high contrast, greater than 5.\n",
      "\n",
      "6.  vedge-mean:  measure the contrast of horizontally\n",
      "\n",
      "          adjacent pixels in the region.  There are 6, the mean and \n",
      "\n",
      "          standard deviation are given.  This attribute is used as\n",
      "\n",
      "         a vertical edge detector.\n",
      "\n",
      "7.  vegde-sd:  (see 6)\n",
      "\n",
      "8.  hdge-mean:  measures the contrast of vertically adjacent\n",
      "\n",
      "           pixels. Used for horizontal line detection. \n",
      "\n",
      "9.  hedge-sd: (see 8).\n",
      "\n",
      "10. intnsty-mean:  the average over the region of (R + G + B)/3\n",
      "\n",
      "11. rawred-mean: the average over the region of the R value.\n",
      "\n",
      "12. rwbl-mean: the average over the region of the B value.\n",
      "\n",
      "13. rwgr-Mean: the average over the region of the G value.\n",
      "\n",
      "14. exred-mean: measure the excess red:  (2R - (G + B))\n",
      "\n",
      "15. exblue-mean: measure the excess blue:  (2B - (G + R))\n",
      "\n",
      "16. exgreen-mean: measure the excess green:  (2G - (R + B))\n",
      "\n",
      "17. val-Mean:  3-d nonlinear transformation\n",
      "\n",
      "          of RGB. (Algorithm can be found in Foley and VanDam, Fundamentals\n",
      "\n",
      "          of Interactive Computer Graphics)\n",
      "\n",
      "18. saturatoin-mean:  (see 17)\n",
      "\n",
      "19. hue-Mean:  (see 17) \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: region-cntr-col | intnsty-mean | hue-Mean | srtn-mean | shor-line-dnty-5 | hdge-mean | shrt-Line-Dnst-2 | rgn-cnrd-row | clss | val-Mean | rwbl-mean | rwgr-Mean | rgn-pxl-cont stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: Clss | HIST | ASCS | AIRA | SPER | BLUB | LIVERFIRM | MAIS | VARI | ABIN | SLEN_PLPL | FIGE | ARXI | SOID stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: hepatitis \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clss | HIST | ASCS | AIRA | SPER | BLUB | LIVERFIRM | MAIS | VARI | ABIN | SLEN_PLPL | FIGE | ARXI | SOID stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "1. Title: Hepatitis Domain\n",
      " \n",
      " 2. Sources:\n",
      "      (a) unknown\n",
      "      (b) Donor: G.Gong  (Carnegie-Mellon University) via \n",
      "                    Bojan Cestnik\n",
      "                    Jozef Stefan Institute\n",
      "                    Jamova 39\n",
      "                    61000 Ljubljana\n",
      "                    Yugoslavia (tel.: (38)(+61) 214-399 ext.287) }\n",
      "      (c) Date: November, 1988\n",
      " \n",
      " 3. Past Usage:\n",
      "     1. Diaconis,P. & Efron,B. (1983).  Computer-Intensive Methods in \n",
      "        Statistics.  Scientific American, Volume 248.\n",
      "        -- Gail Gong reported a 80% classfication accuracy\n",
      "     2. Cestnik,G., Konenenko,I, & Bratko,I. (1987). Assistant-86: A\n",
      "        Knowledge-Elicitation Tool for Sophisticated Users.  In I.Bratko\n",
      "        & N.Lavrac (Eds.) Progress in Machine Learning, 31-45, Sigma Press.\n",
      "        -- Assistant-86: 83% accuracy\n",
      " \n",
      " 4. Relevant Information:\n",
      "     Please ask Gail Gong for further information on this database.\n",
      " \n",
      " 5. Number of Instances: 155\n",
      " \n",
      " 6. Number of Attributes: 20 (including the class attribute)\n",
      " \n",
      " 7. Attribute information: \n",
      "      1. Clss: DIE, LIVE\n",
      "      2. AGE: 10, 20, 30, 40, 50, 60, 70, 80\n",
      "      3. SEX: male, female\n",
      "      4. SOID: no, yes\n",
      "      5. AIRA: no, yes\n",
      "      6. FIGE: no, yes\n",
      "      7. MAIS: no, yes\n",
      "      8. ARXI: no, yes\n",
      "      9. LIVER BIG: no, yes\n",
      "     10. LIVER FIRM: no, yes\n",
      "     11. SPLEEN PALPABLE: no, yes\n",
      "     12. SPER: no, yes\n",
      "     13. ASCS: no, yes\n",
      "     14. VARI: no, yes\n",
      "     15. BLUB: 0.39, 0.80, 1.20, 2.00, 3.00, 4.00\n",
      "         -- see the note below\n",
      "     16. ALK PHOSPHATE: 33, 80, 120, 160, 200, 250\n",
      "     17. SGOT: 13, 100, 200, 300, 400, 500, \n",
      "     18. ABIN: 2.1, 3.0, 3.8, 4.5, 5.0, 6.0\n",
      "     19. PROTIME: 10, 20, 30, 40, 50, 60, 70, 80, 90\n",
      "     20. HIST: no, yes\n",
      " \n",
      "     The BLUB attribute appears to be continuously-valued.  I checked\n",
      "     this with the donater, Bojan Cestnik, who replied:\n",
      " \n",
      "       About the hepatitis database and BLUB problem I would like to say\n",
      "       the following: BLUB is continuous attribute (= the number of it's\n",
      "       \"values\" in the ASDOHEPA.DAT file is negative!!!); \"values\" are quoted\n",
      "       because when speaking about the continuous attribute there is no such \n",
      "       thing as all possible values. However, they represent so called\n",
      "       \"boundary\" values; according to these \"boundary\" values the attribute\n",
      "       can be discretized. At the same time, because of the continious\n",
      "       attribute, one can perform some other test since the continuous\n",
      "       information is preserved. I hope that these lines have at least roughly \n",
      "       answered your question. \n",
      " \n",
      " 8. Missing Attribute Values: (indicated by \"?\")\n",
      "      Attribute Number:    Number of Missing Values:\n",
      "                     1:    0\n",
      "                     2:    0\n",
      "                     3:    0\n",
      "                     4:    1\n",
      "                     5:    0\n",
      "                     6:    1\n",
      "                     7:    1\n",
      "                     8:    1\n",
      "                     9:    10\n",
      "                    10:    11\n",
      "                    11:    5\n",
      "                    12:    5\n",
      "                    13:    5\n",
      "                    14:    5\n",
      "                    15:    6\n",
      "                    16:    29\n",
      "                    17:    4\n",
      "                    18:    16\n",
      "                    19:    67\n",
      "                    20:    0\n",
      " \n",
      " 9. Clss Distribution:\n",
      "      DIE: 32\n",
      "     LIVE: 123\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Relabeled values in attribute SEX\n",
      "    From: 2                       To: male                \n",
      "    From: 1                       To: female              \n",
      "\n",
      "\n",
      " Relabeled values in attribute SOID\n",
      "    From: 1                       To: no                  \n",
      "    From: 2                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute AIRA\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute FIGE\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute MAIS\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute ARXI\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute LIVER_BIG\n",
      "    From: 1                       To: no                  \n",
      "    From: 2                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute LIVERFIRM\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute SLEN_PLPL\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute SPER\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute ASCS\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute VARI\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute HIST\n",
      "    From: 1                       To: no                  \n",
      "    From: 2                       To: yes \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clss | HIST | ASCS | AIRA | SPER | BLUB | LIVERFIRM | MAIS | VARI | ABIN | SLEN_PLPL | FIGE | ARXI | SOID stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: hepatitis \n",
      "with description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "1. Title: Hepatitis Domain\n",
      " \n",
      " 2. Sources:\n",
      "      (a) unknown\n",
      "      (b) Donor: G.Gong  (Carnegie-Mellon University) via \n",
      "                    Bojan Cestnik\n",
      "                    Jozef Stefan Institute\n",
      "                    Jamova 39\n",
      "                    61000 Ljubljana\n",
      "                    Yugoslavia (tel.: (38)(+61) 214-399 ext.287) }\n",
      "      (c) Date: November, 1988\n",
      " \n",
      " 3. Past Usage:\n",
      "     1. Diaconis,P. & Efron,B. (1983).  Computer-Intensive Methods in \n",
      "        Statistics.  Scientific American, Volume 248.\n",
      "        -- Gail Gong reported a 80% classfication accuracy\n",
      "     2. Cestnik,G., Konenenko,I, & Bratko,I. (1987). Assistant-86: A\n",
      "        Knowledge-Elicitation Tool for Sophisticated Users.  In I.Bratko\n",
      "        & N.Lavrac (Eds.) Progress in Machine Learning, 31-45, Sigma Press.\n",
      "        -- Assistant-86: 83% accuracy\n",
      " \n",
      " 4. Relevant Information:\n",
      "     Please ask Gail Gong for further information on this database.\n",
      " \n",
      " 5. Number of Instances: 155\n",
      " \n",
      " 6. Number of Attributes: 20 (including the class attribute)\n",
      " \n",
      " 7. Attribute information: \n",
      "      1. Clss: DIE, LIVE\n",
      "      2. AGE: 10, 20, 30, 40, 50, 60, 70, 80\n",
      "      3. SEX: male, female\n",
      "      4. SOID: no, yes\n",
      "      5. AIRA: no, yes\n",
      "      6. FIGE: no, yes\n",
      "      7. MAIS: no, yes\n",
      "      8. ARXI: no, yes\n",
      "      9. LIVER BIG: no, yes\n",
      "     10. LIVER FIRM: no, yes\n",
      "     11. SPLEEN PALPABLE: no, yes\n",
      "     12. SPER: no, yes\n",
      "     13. ASCS: no, yes\n",
      "     14. VARI: no, yes\n",
      "     15. BLUB: 0.39, 0.80, 1.20, 2.00, 3.00, 4.00\n",
      "         -- see the note below\n",
      "     16. ALK PHOSPHATE: 33, 80, 120, 160, 200, 250\n",
      "     17. SGOT: 13, 100, 200, 300, 400, 500, \n",
      "     18. ABIN: 2.1, 3.0, 3.8, 4.5, 5.0, 6.0\n",
      "     19. PROTIME: 10, 20, 30, 40, 50, 60, 70, 80, 90\n",
      "     20. HIST: no, yes\n",
      " \n",
      "     The BLUB attribute appears to be continuously-valued.  I checked\n",
      "     this with the donater, Bojan Cestnik, who replied:\n",
      " \n",
      "       About the hepatitis database and BLUB problem I would like to say\n",
      "       the following: BLUB is continuous attribute (= the number of it's\n",
      "       \"values\" in the ASDOHEPA.DAT file is negative!!!); \"values\" are quoted\n",
      "       because when speaking about the continuous attribute there is no such \n",
      "       thing as all possible values. However, they represent so called\n",
      "       \"boundary\" values; according to these \"boundary\" values the attribute\n",
      "       can be discretized. At the same time, because of the continious\n",
      "       attribute, one can perform some other test since the continuous\n",
      "       information is preserved. I hope that these lines have at least roughly \n",
      "       answered your question. \n",
      " \n",
      " 8. Missing Attribute Values: (indicated by \"?\")\n",
      "      Attribute Number:    Number of Missing Values:\n",
      "                     1:    0\n",
      "                     2:    0\n",
      "                     3:    0\n",
      "                     4:    1\n",
      "                     5:    0\n",
      "                     6:    1\n",
      "                     7:    1\n",
      "                     8:    1\n",
      "                     9:    10\n",
      "                    10:    11\n",
      "                    11:    5\n",
      "                    12:    5\n",
      "                    13:    5\n",
      "                    14:    5\n",
      "                    15:    6\n",
      "                    16:    29\n",
      "                    17:    4\n",
      "                    18:    16\n",
      "                    19:    67\n",
      "                    20:    0\n",
      " \n",
      " 9. Clss Distribution:\n",
      "      DIE: 32\n",
      "     LIVE: 123\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Relabeled values in attribute SEX\n",
      "    From: 2                       To: male                \n",
      "    From: 1                       To: female              \n",
      "\n",
      "\n",
      " Relabeled values in attribute SOID\n",
      "    From: 1                       To: no                  \n",
      "    From: 2                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute AIRA\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute FIGE\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute MAIS\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute ARXI\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute LIVER_BIG\n",
      "    From: 1                       To: no                  \n",
      "    From: 2                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute LIVERFIRM\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute SLEN_PLPL\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute SPER\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute ASCS\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute VARI\n",
      "    From: 2                       To: no                  \n",
      "    From: 1                       To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute HIST\n",
      "    From: 1                       To: no                  \n",
      "    From: 2                       To: yes \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clss | HIST | ASCS | AIRA | SPER | BLUB | LIVERFIRM | MAIS | VARI | ABIN | SLEN_PLPL | FIGE | ARXI | SOID stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: Clss | botm-Rght-Squa | middle-left-sqar | mddl-middle-sqr | mdle-Rght-Squr | top-Middle-Squa | botm-left-sqr | top-Left-Sqre | top-rght-sqre | bttm-midd-sqr stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: tic-tac-toe \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clss | botm-Rght-Squa | middle-left-sqar | mddl-middle-sqr | mdle-Rght-Squr | top-Middle-Squa | botm-left-sqr | top-Left-Sqre | top-rght-sqre | bttm-midd-sqr stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**: David W. Aha    \n",
      "\n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe+Endgame) - 1991   \n",
      "\n",
      "**Please cite**: [UCI](http://archive.ics.uci.edu/ml/citation_policy.html)\n",
      "\n",
      "\n",
      "\n",
      "**Tic-Tac-Toe Endgame database**  \n",
      "\n",
      "This database encodes the complete set of possible board configurations at the end of tic-tac-toe games, where \"x\" is assumed to have played first.  The target concept is \"win for x\" (i.e., true when \"x\" has one of 8 possible ways to create a \"three-in-a-row\").  \n",
      "\n",
      "\n",
      "\n",
      "### Attribute Information  \n",
      "\n",
      "\n",
      "\n",
      "     (x=player x has taken, o=player o has taken, b=blank)\n",
      "\n",
      "     1. top-Left-Sqre: {x,o,b}\n",
      "\n",
      "     2. top-Middle-Squa: {x,o,b}\n",
      "\n",
      "     3. top-rght-sqre: {x,o,b}\n",
      "\n",
      "     4. middle-left-sqar: {x,o,b}\n",
      "\n",
      "     5. mddl-middle-sqr: {x,o,b}\n",
      "\n",
      "     6. mdle-Rght-Squr: {x,o,b}\n",
      "\n",
      "     7. botm-left-sqr: {x,o,b}\n",
      "\n",
      "     8. bttm-midd-sqr: {x,o,b}\n",
      "\n",
      "     9. botm-Rght-Squa: {x,o,b}\n",
      "\n",
      "    10. Clss: {positive,negative} \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clss | botm-Rght-Squa | middle-left-sqar | mddl-middle-sqr | mdle-Rght-Squr | top-Middle-Squa | botm-left-sqr | top-Left-Sqre | top-rght-sqre | bttm-midd-sqr stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: tic-tac-toe \n",
      "with description: **Author**: David W. Aha    \n",
      "\n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe+Endgame) - 1991   \n",
      "\n",
      "**Please cite**: [UCI](http://archive.ics.uci.edu/ml/citation_policy.html)\n",
      "\n",
      "\n",
      "\n",
      "**Tic-Tac-Toe Endgame database**  \n",
      "\n",
      "This database encodes the complete set of possible board configurations at the end of tic-tac-toe games, where \"x\" is assumed to have played first.  The target concept is \"win for x\" (i.e., true when \"x\" has one of 8 possible ways to create a \"three-in-a-row\").  \n",
      "\n",
      "\n",
      "\n",
      "### Attribute Information  \n",
      "\n",
      "\n",
      "\n",
      "     (x=player x has taken, o=player o has taken, b=blank)\n",
      "\n",
      "     1. top-Left-Sqre: {x,o,b}\n",
      "\n",
      "     2. top-Middle-Squa: {x,o,b}\n",
      "\n",
      "     3. top-rght-sqre: {x,o,b}\n",
      "\n",
      "     4. middle-left-sqar: {x,o,b}\n",
      "\n",
      "     5. mddl-middle-sqr: {x,o,b}\n",
      "\n",
      "     6. mdle-Rght-Squr: {x,o,b}\n",
      "\n",
      "     7. botm-left-sqr: {x,o,b}\n",
      "\n",
      "     8. bttm-midd-sqr: {x,o,b}\n",
      "\n",
      "     9. botm-Rght-Squa: {x,o,b}\n",
      "\n",
      "    10. Clss: {positive,negative} \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clss | botm-Rght-Squa | middle-left-sqar | mddl-middle-sqr | mdle-Rght-Squr | top-Middle-Squa | botm-left-sqr | top-Left-Sqre | top-rght-sqre | bttm-midd-sqr stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: slphts | free_slfr_dxde | chlrds | resisgr | dnsty | citricAcid | alchl | vola_adty | clss | totaSlfrDxd | fixedAcdty stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: wine-quality-red \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: slphts | free_slfr_dxde | chlrds | resisgr | dnsty | citricAcid | alchl | vola_adty | clss | totaSlfrDxd | fixedAcdty stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: wine-quality-red-pmlb \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: slphts | free_slfr_dxde | chlrds | resisgr | dnsty | citricAcid | alchl | vola_adty | clss | totaSlfrDxd | fixedAcdty stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: wine-quality-red \n",
      "with description: wine-quality-red-pmlb \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: slphts | free_slfr_dxde | chlrds | resisgr | dnsty | citricAcid | alchl | vola_adty | clss | totaSlfrDxd | fixedAcdty stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: Lks | Grde | Urbn/Rral | Grds | Gndr | Gals | Spts | Schl | Mony stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: PopularKids \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Lks | Grde | Urbn/Rral | Grds | Gndr | Gals | Spts | Schl | Mony stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "Datasets of Data And Story Library, project illustrating use of basic statistic methods, converted to arff format by Hakan Kjellerstrand.\n",
      "Source: TunedIT: http://tunedit.org/repo/DASL\n",
      "\n",
      "DASL file http://lib.stat.cmu.edu/DASL/Datafiles/PopularKids.html\n",
      "\n",
      "Students' Gals\n",
      ",\n",
      "\n",
      "What Makes Kids Popular\n",
      "\n",
      "Reference:   Chase, M. A., and Dummer, G. M. (1992), \"The Role of Spts as a Social Determinant for Children,\" Research Quarterly for Exercise and Sport, 63, 418-424\n",
      "\n",
      "Authorization:   Contact authors\n",
      "Description:        Subjects were students in grades 4-6 from three school districts in Ingham and Clinton Counties, Michigan.  Chase and Dummer stratified their sample, selecting students from urban, suburban, and rural school districts with approximately 1/3 of their sample coming from each district.  Students indicated whether good grades, athletic ability, or popularity was most important to them.  They also ranked four factors:  grades, sports, looks, and money, in order of their importance for popularity.  The questionnaire also asked for gender, grade level, and other demographic information.\n",
      "Number of cases:   478\n",
      "Variable Names:\n",
      "\n",
      "Gndr:   Boy or girl\n",
      "Grde:   4, 5 or 6\n",
      "Age:   Age in years\n",
      "Race:   White, Other\n",
      "Urbn/Rral:   Rural, Suburban, or Urban school district\n",
      "Schl:   Brentwood Elementary, Brentwood Middle, Ridge, Sand, Eureka, Brown, Main, Portage, Westdale Middle\n",
      "Gals:   Student's choice in the personal goals question where options were 1 = Make Good Grdes,  2 = Be Popular,  3 = Be Good in Spts\n",
      "Grdes:   Rank of \"make good grades\"  (1=most important for popularity, 4=least important)\n",
      "Spts:   Rank of \"being good at sports\"  (1=most important for popularity, 4=least important)\n",
      "Lks:   Rank of \"being handsome or pretty\"  (1=most important for popularity, 4=least important)\n",
      "Mony:   Rank of \"having lots of money\"  (1=most important for popularity, 4=least important) \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Lks | Grde | Urbn/Rral | Grds | Gndr | Gals | Spts | Schl | Mony stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: PopularKids \n",
      "with description: **Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "Datasets of Data And Story Library, project illustrating use of basic statistic methods, converted to arff format by Hakan Kjellerstrand.\n",
      "Source: TunedIT: http://tunedit.org/repo/DASL\n",
      "\n",
      "DASL file http://lib.stat.cmu.edu/DASL/Datafiles/PopularKids.html\n",
      "\n",
      "Students' Gals\n",
      ",\n",
      "\n",
      "What Makes Kids Popular\n",
      "\n",
      "Reference:   Chase, M. A., and Dummer, G. M. (1992), \"The Role of Spts as a Social Determinant for Children,\" Research Quarterly for Exercise and Sport, 63, 418-424\n",
      "\n",
      "Authorization:   Contact authors\n",
      "Description:        Subjects were students in grades 4-6 from three school districts in Ingham and Clinton Counties, Michigan.  Chase and Dummer stratified their sample, selecting students from urban, suburban, and rural school districts with approximately 1/3 of their sample coming from each district.  Students indicated whether good grades, athletic ability, or popularity was most important to them.  They also ranked four factors:  grades, sports, looks, and money, in order of their importance for popularity.  The questionnaire also asked for gender, grade level, and other demographic information.\n",
      "Number of cases:   478\n",
      "Variable Names:\n",
      "\n",
      "Gndr:   Boy or girl\n",
      "Grde:   4, 5 or 6\n",
      "Age:   Age in years\n",
      "Race:   White, Other\n",
      "Urbn/Rral:   Rural, Suburban, or Urban school district\n",
      "Schl:   Brentwood Elementary, Brentwood Middle, Ridge, Sand, Eureka, Brown, Main, Portage, Westdale Middle\n",
      "Gals:   Student's choice in the personal goals question where options were 1 = Make Good Grdes,  2 = Be Popular,  3 = Be Good in Spts\n",
      "Grdes:   Rank of \"make good grades\"  (1=most important for popularity, 4=least important)\n",
      "Spts:   Rank of \"being good at sports\"  (1=most important for popularity, 4=least important)\n",
      "Lks:   Rank of \"being handsome or pretty\"  (1=most important for popularity, 4=least important)\n",
      "Mony:   Rank of \"having lots of money\"  (1=most important for popularity, 4=least important) \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Lks | Grde | Urbn/Rral | Grds | Gndr | Gals | Spts | Schl | Mony stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: Posi | Stri | Wlks | Gmsplyd | HallOfFame | Doub | AtBats | Nmbr_ssns | Trls | Batting_avrg stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: baseball \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Posi | Stri | Wlks | Gmsplyd | HallOfFame | Doub | AtBats | Nmbr_ssns | Trls | Batting_avrg stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: Database of baseball players and play statistics, including 'Gmsplyd', 'AtBats', 'Runs', 'Hits', 'Doub', 'Trls', 'Home_runs', 'RBIs', 'Wlks', 'Stri', 'Batting_avrg', 'On_base_pct', 'Slugging_pct' and 'Fielding_ave' \n",
      "\n",
      "Notes:  \n",
      "* Quotes, Single-Quotes and Backslashes were removed, Blanks replaced with Underscores\n",
      "* Player is an identifier that should be ignored when modelling the data \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Posi | Stri | Wlks | Gmsplyd | HallOfFame | Doub | AtBats | Nmbr_ssns | Trls | Batting_avrg stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: baseball \n",
      "with description: Database of baseball players and play statistics, including 'Gmsplyd', 'AtBats', 'Runs', 'Hits', 'Doub', 'Trls', 'Home_runs', 'RBIs', 'Wlks', 'Stri', 'Batting_avrg', 'On_base_pct', 'Slugging_pct' and 'Fielding_ave' \n",
      "\n",
      "Notes:  \n",
      "* Quotes, Single-Quotes and Backslashes were removed, Blanks replaced with Underscores\n",
      "* Player is an identifier that should be ignored when modelling the data \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Posi | Stri | Wlks | Gmsplyd | HallOfFame | Doub | AtBats | Nmbr_ssns | Trls | Batting_avrg stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: media | plr | bone-mrrw | brn | lvr | abdo | peri | clas | hstlgc-type stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: primary-tumor \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: media | plr | bone-mrrw | brn | lvr | abdo | peri | clas | hstlgc-type stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "Citation Request:\n",
      "    This primary tumor domain was obtained from the University Medical Centre,\n",
      "    Institute of Oncology, Ljubljana, Yugoslavia.  Thanks go to M. Zwitter and \n",
      "    M. Soklic for providing the data.  Please include this citation if you plan\n",
      "    to use this database.\n",
      " \n",
      " 1. Title: Primary Tumor Domain\n",
      " \n",
      " 2. Sources:\n",
      "      (a) Source:\n",
      "      (b) Donors: Igor Kononenko, \n",
      "                  University E.Kardelj\n",
      "                  Faculty for electrical engineering\n",
      "                  Trzaska 25\n",
      "                  61000 Ljubljana (tel.: (38)(+61) 265-161\n",
      " \n",
      "                  Bojan Cestnik\n",
      "                  Jozef Stefan Institute\n",
      "                  Jamova 39\n",
      "                  61000 Ljubljana\n",
      "                  Yugoslavia (tel.: (38)(+61) 214-399 ext.287) \n",
      "      (c) Date: November 1988\n",
      " \n",
      " 3. Past Usage: (sveral)\n",
      "     1. Cestnik,G., Konenenko,I, & Bratko,I. (1987). Assistant-86: A\n",
      "        Knowledge-Elicitation Tool for Sophisticated Users.  In I.Bratko\n",
      "        & N.Lavrac (Eds.) Progress in Machine Learning, 31-45, Sigma Press.\n",
      "        -- Assistant-86: 44% accuracy\n",
      "     2. Clark,P. & Niblett,T. (1987). Induction in Noisy Domains.  In\n",
      "        I.Bratko & N.Lavrac (Eds.) Progress in Machine Learning, 11-30,\n",
      "        Sigma Press.\n",
      "        -- Simple Bayes: 48% accuracy\n",
      "        -- CN2 (95% threshold): 45%\n",
      "     3. Michalski,R., Mozetic,I. Hong,J., & Lavrac,N. (1986).  The Multi-Purpose\n",
      "        Incremental Learning System AQ15 and its Testing Applications to Three\n",
      "        Medical Domains.  In Proceedings of the Fifth National Conference on\n",
      "        Artificial Intelligence, 1041-1045. Philadelphia, PA: Morgan Kaufmann.\n",
      "        -- Experts: 42% accuracy \n",
      "        -- AQ15: 29-41%\n",
      " \n",
      " 4. Relevant Information:\n",
      "      This is one of three domains provided by the Oncology Institute\n",
      "      that has repeatedly appeared in the machine learning literature.\n",
      "      (See also breast-cancer and lymphography.)\n",
      " \n",
      " 5. Number of Instances: 339\n",
      " \n",
      " 6. Number of Attributes: 18 including the clas attribute\n",
      " \n",
      " 7. Attribute Information: (clas is location of tumor)\n",
      "     --- NOTE: All attribute values in the database have been entered as\n",
      "               numeric values corresponding to their index in the list\n",
      "               of attribute values for that attribute domain as given below.\n",
      "     1. clas: lung, head & neck, esophasus, thyroid, stomach, duoden & sm.int,\n",
      "               colon, rectum, anus, salivary glands, pancreas, gallblader,\n",
      "               lvr, kidney, bladder, testis, prostate, ovary, corpus uteri, \n",
      "               cervix uteri, vagina, breast\n",
      "     2. age:   <30, 30-59, >=60\n",
      "     3. sex:   male, female\n",
      "     4. hstlgc-type: epidermoid, adeno, anaplastic\n",
      "     5. degree-of-diffe: well, fairly, poorly\n",
      "     6. bone: yes, no\n",
      "     7. bone-mrrw: yes, no\n",
      "     8. lung: yes, no\n",
      "     9. plr: yes, no\n",
      "    10. peri: yes, no\n",
      "    11. lvr: yes, no\n",
      "    12. brn: yes, no\n",
      "    13. skin: yes, no\n",
      "    14. neck: yes, no\n",
      "    15. supraclavicular: yes, no\n",
      "    16. axillar: yes, no\n",
      "    17. media: yes, no\n",
      "    18. abdo: yes, no\n",
      " \n",
      " 8. Missing Attribute Values: (? indicates unknown value)\n",
      "     Attribute#: Number of missing values\n",
      "     1: 0\n",
      "     2: 0\n",
      "     3: 1\n",
      "     4: 67\n",
      "     5: 155\n",
      "     6: 0\n",
      "     7: 0\n",
      "     8: 0\n",
      "     9: 0\n",
      "     10: 0\n",
      "     11: 0\n",
      "     12: 0\n",
      "     13: 1\n",
      "     14: 0\n",
      "     15: 0\n",
      "     16: 1\n",
      "     17: 0\n",
      "     18: 0\n",
      " \n",
      " 9. Class Distribution: \n",
      "     Class Index:   Number of instances in clas:\n",
      "               1:   84\n",
      "               2:   20\n",
      "               3:   9\n",
      "               4:   14\n",
      "               5:   39\n",
      "               6:   1\n",
      "               7:   14\n",
      "               8:   6\n",
      "               9:   0\n",
      "              10:   2\n",
      "              11:   28\n",
      "              12:   16\n",
      "              13:   7\n",
      "              14:   24\n",
      "              15:   2\n",
      "              16:   1\n",
      "              17:   10\n",
      "              18:   29\n",
      "              19:   6\n",
      "              20:   2\n",
      "              21:   1\n",
      "              22:   24\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Relabeled values in attribute age\n",
      "    From: 1                       To: '<30'               \n",
      "    From: 2                       To: '30-59'             \n",
      "    From: 3                       To: '>=60'              \n",
      "\n",
      "\n",
      " Relabeled values in attribute sex\n",
      "    From: 1                       To: male                \n",
      "    From: 2                       To: female              \n",
      "\n",
      "\n",
      " Relabeled values in attribute hstlgc-type\n",
      "    From: 1                       To: epidermoid          \n",
      "    From: 2                       To: adeno               \n",
      "    From: 3                       To: anaplastic          \n",
      "\n",
      "\n",
      " Relabeled values in attribute degree-of-diffe\n",
      "    From: 1                       To: well                \n",
      "    From: 2                       To: fairly              \n",
      "    From: 3                       To: poorly              \n",
      "\n",
      "\n",
      " Relabeled values in attribute bone\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute bone-mrrw\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute lung\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute plr\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute peri\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute lvr\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute brn\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute skin\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute neck\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute supraclavicular\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute axillar\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute media\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute abdo\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute clas\n",
      "    From: 1                       To: lung                \n",
      "    From: 2                       To: 'head and neck'     \n",
      "    From: 3                       To: esophagus           \n",
      "    From: 4                       To: thyroid             \n",
      "    From: 5                       To: stomach             \n",
      "    From: 6                       To: 'duoden and sm.int' \n",
      "    From: 7                       To: colon               \n",
      "    From: 8                       To: rectum              \n",
      "    From: 9                       To: anus                \n",
      "    From: 10                      To: 'salivary glands'   \n",
      "    From: 11                      To: pancreas            \n",
      "    From: 12                      To: gallbladder         \n",
      "    From: 13                      To: lvr               \n",
      "    From: 14                      To: kidney              \n",
      "    From: 15                      To: bladder             \n",
      "    From: 16                      To: testis              \n",
      "    From: 17                      To: prostate            \n",
      "    From: 18                      To: ovary               \n",
      "    From: 19                      To: 'corpus uteri'      \n",
      "    From: 20                      To: 'cervix uteri'      \n",
      "    From: 21                      To: vagina              \n",
      "    From: 22                      To: breast \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: media | plr | bone-mrrw | brn | lvr | abdo | peri | clas | hstlgc-type stand for\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: primary-tumor \n",
      "with description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "Citation Request:\n",
      "    This primary tumor domain was obtained from the University Medical Centre,\n",
      "    Institute of Oncology, Ljubljana, Yugoslavia.  Thanks go to M. Zwitter and \n",
      "    M. Soklic for providing the data.  Please include this citation if you plan\n",
      "    to use this database.\n",
      " \n",
      " 1. Title: Primary Tumor Domain\n",
      " \n",
      " 2. Sources:\n",
      "      (a) Source:\n",
      "      (b) Donors: Igor Kononenko, \n",
      "                  University E.Kardelj\n",
      "                  Faculty for electrical engineering\n",
      "                  Trzaska 25\n",
      "                  61000 Ljubljana (tel.: (38)(+61) 265-161\n",
      " \n",
      "                  Bojan Cestnik\n",
      "                  Jozef Stefan Institute\n",
      "                  Jamova 39\n",
      "                  61000 Ljubljana\n",
      "                  Yugoslavia (tel.: (38)(+61) 214-399 ext.287) \n",
      "      (c) Date: November 1988\n",
      " \n",
      " 3. Past Usage: (sveral)\n",
      "     1. Cestnik,G., Konenenko,I, & Bratko,I. (1987). Assistant-86: A\n",
      "        Knowledge-Elicitation Tool for Sophisticated Users.  In I.Bratko\n",
      "        & N.Lavrac (Eds.) Progress in Machine Learning, 31-45, Sigma Press.\n",
      "        -- Assistant-86: 44% accuracy\n",
      "     2. Clark,P. & Niblett,T. (1987). Induction in Noisy Domains.  In\n",
      "        I.Bratko & N.Lavrac (Eds.) Progress in Machine Learning, 11-30,\n",
      "        Sigma Press.\n",
      "        -- Simple Bayes: 48% accuracy\n",
      "        -- CN2 (95% threshold): 45%\n",
      "     3. Michalski,R., Mozetic,I. Hong,J., & Lavrac,N. (1986).  The Multi-Purpose\n",
      "        Incremental Learning System AQ15 and its Testing Applications to Three\n",
      "        Medical Domains.  In Proceedings of the Fifth National Conference on\n",
      "        Artificial Intelligence, 1041-1045. Philadelphia, PA: Morgan Kaufmann.\n",
      "        -- Experts: 42% accuracy \n",
      "        -- AQ15: 29-41%\n",
      " \n",
      " 4. Relevant Information:\n",
      "      This is one of three domains provided by the Oncology Institute\n",
      "      that has repeatedly appeared in the machine learning literature.\n",
      "      (See also breast-cancer and lymphography.)\n",
      " \n",
      " 5. Number of Instances: 339\n",
      " \n",
      " 6. Number of Attributes: 18 including the clas attribute\n",
      " \n",
      " 7. Attribute Information: (clas is location of tumor)\n",
      "     --- NOTE: All attribute values in the database have been entered as\n",
      "               numeric values corresponding to their index in the list\n",
      "               of attribute values for that attribute domain as given below.\n",
      "     1. clas: lung, head & neck, esophasus, thyroid, stomach, duoden & sm.int,\n",
      "               colon, rectum, anus, salivary glands, pancreas, gallblader,\n",
      "               lvr, kidney, bladder, testis, prostate, ovary, corpus uteri, \n",
      "               cervix uteri, vagina, breast\n",
      "     2. age:   <30, 30-59, >=60\n",
      "     3. sex:   male, female\n",
      "     4. hstlgc-type: epidermoid, adeno, anaplastic\n",
      "     5. degree-of-diffe: well, fairly, poorly\n",
      "     6. bone: yes, no\n",
      "     7. bone-mrrw: yes, no\n",
      "     8. lung: yes, no\n",
      "     9. plr: yes, no\n",
      "    10. peri: yes, no\n",
      "    11. lvr: yes, no\n",
      "    12. brn: yes, no\n",
      "    13. skin: yes, no\n",
      "    14. neck: yes, no\n",
      "    15. supraclavicular: yes, no\n",
      "    16. axillar: yes, no\n",
      "    17. media: yes, no\n",
      "    18. abdo: yes, no\n",
      " \n",
      " 8. Missing Attribute Values: (? indicates unknown value)\n",
      "     Attribute#: Number of missing values\n",
      "     1: 0\n",
      "     2: 0\n",
      "     3: 1\n",
      "     4: 67\n",
      "     5: 155\n",
      "     6: 0\n",
      "     7: 0\n",
      "     8: 0\n",
      "     9: 0\n",
      "     10: 0\n",
      "     11: 0\n",
      "     12: 0\n",
      "     13: 1\n",
      "     14: 0\n",
      "     15: 0\n",
      "     16: 1\n",
      "     17: 0\n",
      "     18: 0\n",
      " \n",
      " 9. Class Distribution: \n",
      "     Class Index:   Number of instances in clas:\n",
      "               1:   84\n",
      "               2:   20\n",
      "               3:   9\n",
      "               4:   14\n",
      "               5:   39\n",
      "               6:   1\n",
      "               7:   14\n",
      "               8:   6\n",
      "               9:   0\n",
      "              10:   2\n",
      "              11:   28\n",
      "              12:   16\n",
      "              13:   7\n",
      "              14:   24\n",
      "              15:   2\n",
      "              16:   1\n",
      "              17:   10\n",
      "              18:   29\n",
      "              19:   6\n",
      "              20:   2\n",
      "              21:   1\n",
      "              22:   24\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Relabeled values in attribute age\n",
      "    From: 1                       To: '<30'               \n",
      "    From: 2                       To: '30-59'             \n",
      "    From: 3                       To: '>=60'              \n",
      "\n",
      "\n",
      " Relabeled values in attribute sex\n",
      "    From: 1                       To: male                \n",
      "    From: 2                       To: female              \n",
      "\n",
      "\n",
      " Relabeled values in attribute hstlgc-type\n",
      "    From: 1                       To: epidermoid          \n",
      "    From: 2                       To: adeno               \n",
      "    From: 3                       To: anaplastic          \n",
      "\n",
      "\n",
      " Relabeled values in attribute degree-of-diffe\n",
      "    From: 1                       To: well                \n",
      "    From: 2                       To: fairly              \n",
      "    From: 3                       To: poorly              \n",
      "\n",
      "\n",
      " Relabeled values in attribute bone\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute bone-mrrw\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute lung\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute plr\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute peri\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute lvr\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute brn\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute skin\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute neck\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute supraclavicular\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute axillar\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute media\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute abdo\n",
      "    From: 1                       To: yes                 \n",
      "    From: 2                       To: no                  \n",
      "\n",
      "\n",
      " Relabeled values in attribute clas\n",
      "    From: 1                       To: lung                \n",
      "    From: 2                       To: 'head and neck'     \n",
      "    From: 3                       To: esophagus           \n",
      "    From: 4                       To: thyroid             \n",
      "    From: 5                       To: stomach             \n",
      "    From: 6                       To: 'duoden and sm.int' \n",
      "    From: 7                       To: colon               \n",
      "    From: 8                       To: rectum              \n",
      "    From: 9                       To: anus                \n",
      "    From: 10                      To: 'salivary glands'   \n",
      "    From: 11                      To: pancreas            \n",
      "    From: 12                      To: gallbladder         \n",
      "    From: 13                      To: lvr               \n",
      "    From: 14                      To: kidney              \n",
      "    From: 15                      To: bladder             \n",
      "    From: 16                      To: testis              \n",
      "    From: 17                      To: prostate            \n",
      "    From: 18                      To: ovary               \n",
      "    From: 19                      To: 'corpus uteri'      \n",
      "    From: 20                      To: 'cervix uteri'      \n",
      "    From: 21                      To: vagina              \n",
      "    From: 22                      To: breast \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: media | plr | bone-mrrw | brn | lvr | abdo | peri | clas | hstlgc-type stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: Arbt1 | Attr2 | Atrt_7 | Attr_9 | Attrbt8 | Attr3 | Attrbt5 | clas | Attrbt4 | Attrbt_6 stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: xd6 \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Arbt1 | Attr2 | Atrt_7 | Attr_9 | Attrbt8 | Attr3 | Attrbt5 | clas | Attrbt4 | Attrbt_6 stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**: Unknown  \n",
      "\n",
      "**Source**: [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks/tree/master/datasets/clasification) - Supposedly originates from UCI, but can't find it there anymore.  \n",
      "\n",
      "**Please cite:**  \n",
      "\n",
      "\n",
      "\n",
      "**XD6 Dataset**\n",
      "\n",
      "Dataset used by Buntine and Niblett (1992). Composed of 10 features, one of which is irrelevant. The target is a disjunctive normal form formula over the nine other attributes, with additional clasification noise.\n",
      "\n",
      "\n",
      "\n",
      "[More info](https://books.google.be/books?id=W2bmBwAAQBAJ&pg=PA313&lpg=PA313&dq=dataset+xd6&source=bl&ots=6hYPdz8_Nl&sig=TR1ieOg9D1pCrvNyeKbb-3eKmd8&hl=en&sa=X&ved=0ahUKEwj_tZ_MxozZAhVHa1AKHZVEBBsQ6AEIQjAF#v=onepage&q=dataset xd6&f=false). \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Arbt1 | Attr2 | Atrt_7 | Attr_9 | Attrbt8 | Attr3 | Attrbt5 | clas | Attrbt4 | Attrbt_6 stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: xd6 \n",
      "with description: **Author**: Unknown  \n",
      "\n",
      "**Source**: [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks/tree/master/datasets/clasification) - Supposedly originates from UCI, but can't find it there anymore.  \n",
      "\n",
      "**Please cite:**  \n",
      "\n",
      "\n",
      "\n",
      "**XD6 Dataset**\n",
      "\n",
      "Dataset used by Buntine and Niblett (1992). Composed of 10 features, one of which is irrelevant. The target is a disjunctive normal form formula over the nine other attributes, with additional clasification noise.\n",
      "\n",
      "\n",
      "\n",
      "[More info](https://books.google.be/books?id=W2bmBwAAQBAJ&pg=PA313&lpg=PA313&dq=dataset+xd6&source=bl&ots=6hYPdz8_Nl&sig=TR1ieOg9D1pCrvNyeKbb-3eKmd8&hl=en&sa=X&ved=0ahUKEwj_tZ_MxozZAhVHa1AKHZVEBBsQ6AEIQjAF#v=onepage&q=dataset xd6&f=false). \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Arbt1 | Attr2 | Atrt_7 | Attr_9 | Attrbt8 | Attr3 | Attrbt5 | clas | Attrbt4 | Attrbt_6 stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: Clas | AskOpen | BidOpen | BidHigh | AskVlme | BidClos | BidVlm | Ask_Cls | AskHigh stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: FOREX_eurhuf-day-High \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clas | AskOpen | BidOpen | BidHigh | AskVlme | BidClos | BidVlm | Ask_Cls | AskHigh stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Source**: Dukascopy Historical Data Feed https://www.dukascopy.com/swiss/english/marketwatch/historical/\n",
      "**Edited by**: Fabian Schut\n",
      " \n",
      "# Data Description\n",
      "This is the historical price data of the FOREX EUR/HUF from Dukascopy.\n",
      "One instance (row) is one candlestick of one day.\n",
      "The whole dataset has the data range from 1-1-2018 to 13-12-2018 and does not include the weekends, since the FOREX is not traded in the weekend.\n",
      "The timezone of the feature Timestamp is Europe/Amsterdam.\n",
      "The class attribute is the direction of the mean of the High_Bid and the High_Ask of the following day,\n",
      "relative to the High_Bid and High_Ask mean of the current minute.\n",
      "This means the class attribute is True when the mean High price is going up the following day,\n",
      "and the class attribute is False when the mean High price is going down (or stays the same) the following day.\n",
      "**Note that this is a hypothetical task, meant for scientific purposes only. Realistic trade strategies can only be applied to predictions on 'Close'-attributes (also available).\n",
      "# Attributes \n",
      "`Timestamp`: The time of the current data point (Europe/Amsterdam)\n",
      "`BidOpen`: The bid price at the start of this time interval\n",
      "`BidHigh`: The highest bid price during this time interval\n",
      "`Bid_Low`: The lowest bid price during this time interval\n",
      "`BidClos`: The bid price at the end of this time interval\n",
      "`BidVlm`: The number of times the Bid Price changed within this time interval\n",
      "`AskOpen`: The ask price at the start of this time interval\n",
      "`AskHigh`: The highest ask price during this time interval\n",
      "`Ask_Low`: The lowest ask price during this time interval\n",
      "`Ask_Cls`: The ask price at the end of this time interval\n",
      "`AskVlme`: The number of times the Ask Price changed within this time interval\n",
      "`Clas`: Whether the average price will go up during the next interval \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clas | AskOpen | BidOpen | BidHigh | AskVlme | BidClos | BidVlm | Ask_Cls | AskHigh stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: FOREX_eurhuf-day-High \n",
      "with description: **Source**: Dukascopy Historical Data Feed https://www.dukascopy.com/swiss/english/marketwatch/historical/\n",
      "**Edited by**: Fabian Schut\n",
      " \n",
      "# Data Description\n",
      "This is the historical price data of the FOREX EUR/HUF from Dukascopy.\n",
      "One instance (row) is one candlestick of one day.\n",
      "The whole dataset has the data range from 1-1-2018 to 13-12-2018 and does not include the weekends, since the FOREX is not traded in the weekend.\n",
      "The timezone of the feature Timestamp is Europe/Amsterdam.\n",
      "The class attribute is the direction of the mean of the High_Bid and the High_Ask of the following day,\n",
      "relative to the High_Bid and High_Ask mean of the current minute.\n",
      "This means the class attribute is True when the mean High price is going up the following day,\n",
      "and the class attribute is False when the mean High price is going down (or stays the same) the following day.\n",
      "**Note that this is a hypothetical task, meant for scientific purposes only. Realistic trade strategies can only be applied to predictions on 'Close'-attributes (also available).\n",
      "# Attributes \n",
      "`Timestamp`: The time of the current data point (Europe/Amsterdam)\n",
      "`BidOpen`: The bid price at the start of this time interval\n",
      "`BidHigh`: The highest bid price during this time interval\n",
      "`Bid_Low`: The lowest bid price during this time interval\n",
      "`BidClos`: The bid price at the end of this time interval\n",
      "`BidVlm`: The number of times the Bid Price changed within this time interval\n",
      "`AskOpen`: The ask price at the start of this time interval\n",
      "`AskHigh`: The highest ask price during this time interval\n",
      "`Ask_Low`: The lowest ask price during this time interval\n",
      "`Ask_Cls`: The ask price at the end of this time interval\n",
      "`AskVlme`: The number of times the Ask Price changed within this time interval\n",
      "`Clas`: Whether the average price will go up during the next interval \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clas | AskOpen | BidOpen | BidHigh | AskVlme | BidClos | BidVlm | Ask_Cls | AskHigh stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: totleveclls | ttldaycall | ttlDayChrg | areaCode | totaNighCals | clss | accolngth | stt | ttl_nght_mnts | phonNmbr | internPlan | totl_day_mnts | ttlEveMnts | nmbr_customer_srvc_call | voicmailplan | ttalNighChrg | ttlevechrg stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: churn \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: totleveclls | ttldaycall | ttlDayChrg | areaCode | totaNighCals | clss | accolngth | stt | ttl_nght_mnts | phonNmbr | internPlan | totl_day_mnts | ttlEveMnts | nmbr_customer_srvc_call | voicmailplan | ttalNighChrg | ttlevechrg stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**: Unknown  \n",
      "\n",
      "**Source**: [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks/tree/master/datasets/clssification), [BigML](https://bigml.com/user/francisco/gallery/dataset/5163ad540c0b5e5b22000383), Supposedly from UCI but I can't find it there.  \n",
      "\n",
      "**Please cite**:   \n",
      "\n",
      "\n",
      "\n",
      "A dataset relating characteristics of telephony account features and usage and whether or not the customer churned. Originally used in [Discovering Knowledge in Data: An Introduction to Data Mining](http://secs.ac.in/wp-content/CSE_PORTAL/DataMining_Daniel.pdf). \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: totleveclls | ttldaycall | ttlDayChrg | areaCode | totaNighCals | clss | accolngth | stt | ttl_nght_mnts | phonNmbr | internPlan | totl_day_mnts | ttlEveMnts | nmbr_customer_srvc_call | voicmailplan | ttalNighChrg | ttlevechrg stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: churn \n",
      "with description: **Author**: Unknown  \n",
      "\n",
      "**Source**: [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks/tree/master/datasets/clssification), [BigML](https://bigml.com/user/francisco/gallery/dataset/5163ad540c0b5e5b22000383), Supposedly from UCI but I can't find it there.  \n",
      "\n",
      "**Please cite**:   \n",
      "\n",
      "\n",
      "\n",
      "A dataset relating characteristics of telephony account features and usage and whether or not the customer churned. Originally used in [Discovering Knowledge in Data: An Introduction to Data Mining](http://secs.ac.in/wp-content/CSE_PORTAL/DataMining_Daniel.pdf). \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: totleveclls | ttldaycall | ttlDayChrg | areaCode | totaNighCals | clss | accolngth | stt | ttl_nght_mnts | phonNmbr | internPlan | totl_day_mnts | ttlEveMnts | nmbr_customer_srvc_call | voicmailplan | ttalNighChrg | ttlevechrg stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: MAX.LENG_RECTANGULARITY | HOWSRATI | Clas | SAERRATIO | SKEW_ABOUT_MAJOR | SCALVARIMAJOR | SCLDRADIOFGYRATION | MAX.LENGASPECTRATIO | ELONGA | SKEWNESSABOUTMINOR | SLEDVARIANCEMINO | RADIRATO | DISTANCE_CIRCU | CPCTE | CIRCU stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: vehicle \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: MAX.LENG_RECTANGULARITY | HOWSRATI | Clas | SAERRATIO | SKEW_ABOUT_MAJOR | SCALVARIMAJOR | SCLDRADIOFGYRATION | MAX.LENGASPECTRATIO | ELONGA | SKEWNESSABOUTMINOR | SLEDVARIANCEMINO | RADIRATO | DISTANCE_CIRCU | CPCTE | CIRCU stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**: Dr. Pete Mowforth and Dr. Barry Shepherd  \n",
      "\n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Statlog+(Vehicle+Silhouettes))\n",
      "\n",
      "**Please cite**: Siebert,JP. Turing Institute Research Memorandum TIRM-87-018 \"Vehicle Recognition Using Rule Based Methods\" (March 1987)  \n",
      "\n",
      "\n",
      "\n",
      " NAME\n",
      "\n",
      "         vehicle silhouettes\n",
      "\n",
      " \n",
      "\n",
      " PURPOSE\n",
      "\n",
      "         to classify a given silhouette as one of four types of vehicle,\n",
      "\n",
      "         using  a set of features extracted from the silhouette. The\n",
      "\n",
      "         vehicle may be viewed from one of many different angles.  \n",
      "\n",
      " \n",
      "\n",
      " PROBLEM TYPE\n",
      "\n",
      "         classification\n",
      "\n",
      "         \n",
      "\n",
      " SOURCE\n",
      "\n",
      "         Drs.Pete Mowforth and Barry Shepherd\n",
      "\n",
      "         Turing Institute\n",
      "\n",
      "         George House\n",
      "\n",
      "         36 North Hanover St.\n",
      "\n",
      "         Glasgow\n",
      "\n",
      "         G1 2AD\n",
      "\n",
      " \n",
      "\n",
      " CONTACT\n",
      "\n",
      "         Alistair Sutherland\n",
      "\n",
      "         Statistics Dept.\n",
      "\n",
      "         Strathclyde University\n",
      "\n",
      "         Livingstone Tower\n",
      "\n",
      "         26 Richmond St.\n",
      "\n",
      "         GLASGOW G1 1XH\n",
      "\n",
      "         Great Britain\n",
      "\n",
      "         \n",
      "\n",
      "         Tel: 041 552 4400 x3033\n",
      "\n",
      "         \n",
      "\n",
      "         Fax: 041 552 4711 \n",
      "\n",
      "         \n",
      "\n",
      "         e-mail: alistair@uk.ac.strathclyde.stams\n",
      "\n",
      " \n",
      "\n",
      " HISTORY\n",
      "\n",
      "         This data was originally gathered at the TI in 1986-87 by\n",
      "\n",
      "         JP Siebert. It was partially financed by Barr and Stroud Ltd.\n",
      "\n",
      "         The original purpose was to find a method of distinguishing\n",
      "\n",
      "         3D objects within a 2D image by application of an ensemble of\n",
      "\n",
      "         shape feature extractors to the 2D silhouettes of the objects.\n",
      "\n",
      "         Measures of shape features extracted from example silhouettes\n",
      "\n",
      "         of objects to be discriminated were used to generate a class-\n",
      "\n",
      "         ification rule tree by means of computer induction.\n",
      "\n",
      "          This object recognition strategy was successfully used to \n",
      "\n",
      "         discriminate between silhouettes of model cars, vans and buses\n",
      "\n",
      "         viewed from constrained elevation but all angles of rotation.\n",
      "\n",
      "          The rule tree classification performance compared favourably\n",
      "\n",
      "         to MDC (Minimum Distance Clasifier) and k-NN (k-Nearest Neigh-\n",
      "\n",
      "         bour) statistical classifiers in terms of both error rate and\n",
      "\n",
      "         computational efficiency. An investigation of these rule trees\n",
      "\n",
      "         generated by example indicated that the tree structure was \n",
      "\n",
      "         heavily influenced by the orientation of the objects, and grouped\n",
      "\n",
      "         similar object views into single decisions.\n",
      "\n",
      " \n",
      "\n",
      " DESCRIPTION\n",
      "\n",
      "          The features were extracted from the silhouettes by the HIPS\n",
      "\n",
      "         (Hierarchical Image Processing System) extension BINATTS, which \n",
      "\n",
      "         extracts a combination of scale independent features utilising\n",
      "\n",
      "         both classical moments based measures such as scaled variance,\n",
      "\n",
      "         skewness and kurtosis about the major/minor axes and heuristic\n",
      "\n",
      "         measures such as hollows, circularity, rectangularity and\n",
      "\n",
      "         compactness.\n",
      "\n",
      "          Four \"Corgie\" model vehicles were used for the experiment:\n",
      "\n",
      "         a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400.\n",
      "\n",
      "         This particular combination of vehicles was chosen with the \n",
      "\n",
      "         expectation that the bus, van and either one of the cars would\n",
      "\n",
      "         be readily distinguishable, but it would be more difficult to\n",
      "\n",
      "         distinguish between the cars.\n",
      "\n",
      "          The images were acquired by a camera looking downwards at the\n",
      "\n",
      "         model vehicle from a fixed angle of elevation (34.2 degrees\n",
      "\n",
      "         to the horizontal). The vehicles were placed on a diffuse\n",
      "\n",
      "         backlit surface (lightbox). The vehicles were painted matte black\n",
      "\n",
      "         to minimise highlights. The images were captured using a CRS4000\n",
      "\n",
      "         framestore connected to a vax 750. All images were captured with\n",
      "\n",
      "         a spatial resolution of 128x128 pixels quantised to 64 greylevels.\n",
      "\n",
      "         These images were thresholded to produce binary vehicle silhouettes,\n",
      "\n",
      "         negated (to comply with the processing requirements of BINATTS) and\n",
      "\n",
      "         thereafter subjected to shrink-expand-expand-shrink HIPS modules to\n",
      "\n",
      "         remove \"salt and pepper\" image noise.\n",
      "\n",
      "          The vehicles were rotated and their angle of orientation was measured\n",
      "\n",
      "         using a radial graticule beneath the vehicle. 0 and 180 degrees\n",
      "\n",
      "         corresponded to \"head on\" and \"rear\" views respectively while 90 and\n",
      "\n",
      "         270 corresponded to profiles in opposite directions. Two sets of\n",
      "\n",
      "         60 images, each set covering a full 360 degree rotation, were captured\n",
      "\n",
      "         for each vehicle. The vehicle was rotated by a fixed angle between \n",
      "\n",
      "         images. These datasets are known as e2 and e3 respectively.\n",
      "\n",
      "          A further two sets of images, e4 and e5, were captured with the camera \n",
      "\n",
      "         at elevations of 37.5 degs and 30.8 degs respectively. These sets\n",
      "\n",
      "         also contain 60 images per vehicle apart from e4.van which contains\n",
      "\n",
      "         only 46 owing to the difficulty of containing the van in the image\n",
      "\n",
      "         at some orientations.\n",
      "\n",
      " \n",
      "\n",
      " ATTRIBUTES\n",
      "\n",
      "         \n",
      "\n",
      "         CPCTE     (average perim)**2/area\n",
      "\n",
      "         \n",
      "\n",
      "         CIRCU     (average radius)**2/area\n",
      "\n",
      "         \n",
      "\n",
      "         DISTANCE CIRCU    area/(av.distance from border)**2\n",
      "\n",
      "         \n",
      "\n",
      "         RADIUS RATIO    (max.rad-min.rad)/av.radius\n",
      "\n",
      "         \n",
      "\n",
      "         PR.AXIS ASPECT RATIO    (minor axis)/(major axis)\n",
      "\n",
      "         \n",
      "\n",
      "         MAX.LENGTH ASPECT RATIO (length perp. max length)/(max length)\n",
      "\n",
      "         \n",
      "\n",
      "         SCATTER RATIO   (inertia about minor axis)/(inertia about major axis)\n",
      "\n",
      "         \n",
      "\n",
      "         ELONGA           area/(shrink width)**2\n",
      "\n",
      "         \n",
      "\n",
      "         PR.AXIS RECTANGULARITY  area/(pr.axis length*pr.axis width)\n",
      "\n",
      "         \n",
      "\n",
      "         MAX.LENGTH RECTANGULARITY area/(max.length*length perp. to this)\n",
      "\n",
      "         \n",
      "\n",
      "         SCALED VARIANCE         (2nd order moment about minor axis)/area\n",
      "\n",
      "         ALONG MAJOR AXIS\n",
      "\n",
      "         \n",
      "\n",
      "         SCALED VARIANCE         (2nd order moment about major axis)/area\n",
      "\n",
      "         ALONG MINOR AXIS \n",
      "\n",
      "         \n",
      "\n",
      "         SCALED RADIUS OF GYRATION       (mavar+mivar)/area\n",
      "\n",
      "         \n",
      "\n",
      "         SKEWNESS ABOUT  (3rd order moment about major axis)/sigma_min**3\n",
      "\n",
      "         MAJOR AXIS\n",
      "\n",
      "         \n",
      "\n",
      "         SKEWNESS ABOUT  (3rd order moment about minor axis)/sigma_maj**3\n",
      "\n",
      "         MINOR AXIS\n",
      "\n",
      "                 \n",
      "\n",
      "         KURTOSIS ABOUT  (4th order moment about major axis)/sigma_min**4\n",
      "\n",
      "         MINOR AXIS  \n",
      "\n",
      "                 \n",
      "\n",
      "         KURTOSIS ABOUT  (4th order moment about minor axis)/sigma_maj**4\n",
      "\n",
      "         MAJOR AXIS\n",
      "\n",
      "         \n",
      "\n",
      "         HOLLOWS RATIO   (area of hollows)/(area of bounding polygon)\n",
      "\n",
      "         \n",
      "\n",
      "          Where sigma_maj**2 is the variance along the major axis and\n",
      "\n",
      "         sigma_min**2 is the variance along the minor axis, and\n",
      "\n",
      "         \n",
      "\n",
      "         area of hollows= area of bounding poly-area of object \n",
      "\n",
      "         \n",
      "\n",
      "          The area of the bounding polygon is found as a side result of\n",
      "\n",
      "         the computation to find the maximum length. Each individual\n",
      "\n",
      "         length computation yields a pair of calipers to the object\n",
      "\n",
      "         orientated at every 5 degrees. The object is propagated into\n",
      "\n",
      "         an image containing the union of these calipers to obtain an\n",
      "\n",
      "         image of the bounding polygon. \n",
      "\n",
      "         \n",
      "\n",
      " NUMBER OF CLASSES\n",
      "\n",
      " \n",
      "\n",
      "         4       OPEL, SAAB, BUS, VAN\n",
      "\n",
      " \n",
      "\n",
      " NUMBER OF EXAMPLES\n",
      "\n",
      " \n",
      "\n",
      "                 Total no. = 946\n",
      "\n",
      "                 \n",
      "\n",
      "                 No. in each class\n",
      "\n",
      "                 \n",
      "\n",
      "                   opel 240\n",
      "\n",
      "                   saab 240\n",
      "\n",
      "                   bus  240\n",
      "\n",
      "                   van  226\n",
      "\n",
      "                 \n",
      "\n",
      "                 \n",
      "\n",
      "                 100 examples are being kept by Strathclyde for validation.\n",
      "\n",
      "                 So StatLog partners will receive 846 examples.\n",
      "\n",
      " \n",
      "\n",
      " NUMBER OF ATTRIBUTES\n",
      "\n",
      " \n",
      "\n",
      "                 No. of atts. = 18 \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: MAX.LENG_RECTANGULARITY | HOWSRATI | Clas | SAERRATIO | SKEW_ABOUT_MAJOR | SCALVARIMAJOR | SCLDRADIOFGYRATION | MAX.LENGASPECTRATIO | ELONGA | SKEWNESSABOUTMINOR | SLEDVARIANCEMINO | RADIRATO | DISTANCE_CIRCU | CPCTE | CIRCU stand for\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: vehicle \n",
      "with description: **Author**: Dr. Pete Mowforth and Dr. Barry Shepherd  \n",
      "\n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Statlog+(Vehicle+Silhouettes))\n",
      "\n",
      "**Please cite**: Siebert,JP. Turing Institute Research Memorandum TIRM-87-018 \"Vehicle Recognition Using Rule Based Methods\" (March 1987)  \n",
      "\n",
      "\n",
      "\n",
      " NAME\n",
      "\n",
      "         vehicle silhouettes\n",
      "\n",
      " \n",
      "\n",
      " PURPOSE\n",
      "\n",
      "         to classify a given silhouette as one of four types of vehicle,\n",
      "\n",
      "         using  a set of features extracted from the silhouette. The\n",
      "\n",
      "         vehicle may be viewed from one of many different angles.  \n",
      "\n",
      " \n",
      "\n",
      " PROBLEM TYPE\n",
      "\n",
      "         classification\n",
      "\n",
      "         \n",
      "\n",
      " SOURCE\n",
      "\n",
      "         Drs.Pete Mowforth and Barry Shepherd\n",
      "\n",
      "         Turing Institute\n",
      "\n",
      "         George House\n",
      "\n",
      "         36 North Hanover St.\n",
      "\n",
      "         Glasgow\n",
      "\n",
      "         G1 2AD\n",
      "\n",
      " \n",
      "\n",
      " CONTACT\n",
      "\n",
      "         Alistair Sutherland\n",
      "\n",
      "         Statistics Dept.\n",
      "\n",
      "         Strathclyde University\n",
      "\n",
      "         Livingstone Tower\n",
      "\n",
      "         26 Richmond St.\n",
      "\n",
      "         GLASGOW G1 1XH\n",
      "\n",
      "         Great Britain\n",
      "\n",
      "         \n",
      "\n",
      "         Tel: 041 552 4400 x3033\n",
      "\n",
      "         \n",
      "\n",
      "         Fax: 041 552 4711 \n",
      "\n",
      "         \n",
      "\n",
      "         e-mail: alistair@uk.ac.strathclyde.stams\n",
      "\n",
      " \n",
      "\n",
      " HISTORY\n",
      "\n",
      "         This data was originally gathered at the TI in 1986-87 by\n",
      "\n",
      "         JP Siebert. It was partially financed by Barr and Stroud Ltd.\n",
      "\n",
      "         The original purpose was to find a method of distinguishing\n",
      "\n",
      "         3D objects within a 2D image by application of an ensemble of\n",
      "\n",
      "         shape feature extractors to the 2D silhouettes of the objects.\n",
      "\n",
      "         Measures of shape features extracted from example silhouettes\n",
      "\n",
      "         of objects to be discriminated were used to generate a class-\n",
      "\n",
      "         ification rule tree by means of computer induction.\n",
      "\n",
      "          This object recognition strategy was successfully used to \n",
      "\n",
      "         discriminate between silhouettes of model cars, vans and buses\n",
      "\n",
      "         viewed from constrained elevation but all angles of rotation.\n",
      "\n",
      "          The rule tree classification performance compared favourably\n",
      "\n",
      "         to MDC (Minimum Distance Clasifier) and k-NN (k-Nearest Neigh-\n",
      "\n",
      "         bour) statistical classifiers in terms of both error rate and\n",
      "\n",
      "         computational efficiency. An investigation of these rule trees\n",
      "\n",
      "         generated by example indicated that the tree structure was \n",
      "\n",
      "         heavily influenced by the orientation of the objects, and grouped\n",
      "\n",
      "         similar object views into single decisions.\n",
      "\n",
      " \n",
      "\n",
      " DESCRIPTION\n",
      "\n",
      "          The features were extracted from the silhouettes by the HIPS\n",
      "\n",
      "         (Hierarchical Image Processing System) extension BINATTS, which \n",
      "\n",
      "         extracts a combination of scale independent features utilising\n",
      "\n",
      "         both classical moments based measures such as scaled variance,\n",
      "\n",
      "         skewness and kurtosis about the major/minor axes and heuristic\n",
      "\n",
      "         measures such as hollows, circularity, rectangularity and\n",
      "\n",
      "         compactness.\n",
      "\n",
      "          Four \"Corgie\" model vehicles were used for the experiment:\n",
      "\n",
      "         a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400.\n",
      "\n",
      "         This particular combination of vehicles was chosen with the \n",
      "\n",
      "         expectation that the bus, van and either one of the cars would\n",
      "\n",
      "         be readily distinguishable, but it would be more difficult to\n",
      "\n",
      "         distinguish between the cars.\n",
      "\n",
      "          The images were acquired by a camera looking downwards at the\n",
      "\n",
      "         model vehicle from a fixed angle of elevation (34.2 degrees\n",
      "\n",
      "         to the horizontal). The vehicles were placed on a diffuse\n",
      "\n",
      "         backlit surface (lightbox). The vehicles were painted matte black\n",
      "\n",
      "         to minimise highlights. The images were captured using a CRS4000\n",
      "\n",
      "         framestore connected to a vax 750. All images were captured with\n",
      "\n",
      "         a spatial resolution of 128x128 pixels quantised to 64 greylevels.\n",
      "\n",
      "         These images were thresholded to produce binary vehicle silhouettes,\n",
      "\n",
      "         negated (to comply with the processing requirements of BINATTS) and\n",
      "\n",
      "         thereafter subjected to shrink-expand-expand-shrink HIPS modules to\n",
      "\n",
      "         remove \"salt and pepper\" image noise.\n",
      "\n",
      "          The vehicles were rotated and their angle of orientation was measured\n",
      "\n",
      "         using a radial graticule beneath the vehicle. 0 and 180 degrees\n",
      "\n",
      "         corresponded to \"head on\" and \"rear\" views respectively while 90 and\n",
      "\n",
      "         270 corresponded to profiles in opposite directions. Two sets of\n",
      "\n",
      "         60 images, each set covering a full 360 degree rotation, were captured\n",
      "\n",
      "         for each vehicle. The vehicle was rotated by a fixed angle between \n",
      "\n",
      "         images. These datasets are known as e2 and e3 respectively.\n",
      "\n",
      "          A further two sets of images, e4 and e5, were captured with the camera \n",
      "\n",
      "         at elevations of 37.5 degs and 30.8 degs respectively. These sets\n",
      "\n",
      "         also contain 60 images per vehicle apart from e4.van which contains\n",
      "\n",
      "         only 46 owing to the difficulty of containing the van in the image\n",
      "\n",
      "         at some orientations.\n",
      "\n",
      " \n",
      "\n",
      " ATTRIBUTES\n",
      "\n",
      "         \n",
      "\n",
      "         CPCTE     (average perim)**2/area\n",
      "\n",
      "         \n",
      "\n",
      "         CIRCU     (average radius)**2/area\n",
      "\n",
      "         \n",
      "\n",
      "         DISTANCE CIRCU    area/(av.distance from border)**2\n",
      "\n",
      "         \n",
      "\n",
      "         RADIUS RATIO    (max.rad-min.rad)/av.radius\n",
      "\n",
      "         \n",
      "\n",
      "         PR.AXIS ASPECT RATIO    (minor axis)/(major axis)\n",
      "\n",
      "         \n",
      "\n",
      "         MAX.LENGTH ASPECT RATIO (length perp. max length)/(max length)\n",
      "\n",
      "         \n",
      "\n",
      "         SCATTER RATIO   (inertia about minor axis)/(inertia about major axis)\n",
      "\n",
      "         \n",
      "\n",
      "         ELONGA           area/(shrink width)**2\n",
      "\n",
      "         \n",
      "\n",
      "         PR.AXIS RECTANGULARITY  area/(pr.axis length*pr.axis width)\n",
      "\n",
      "         \n",
      "\n",
      "         MAX.LENGTH RECTANGULARITY area/(max.length*length perp. to this)\n",
      "\n",
      "         \n",
      "\n",
      "         SCALED VARIANCE         (2nd order moment about minor axis)/area\n",
      "\n",
      "         ALONG MAJOR AXIS\n",
      "\n",
      "         \n",
      "\n",
      "         SCALED VARIANCE         (2nd order moment about major axis)/area\n",
      "\n",
      "         ALONG MINOR AXIS \n",
      "\n",
      "         \n",
      "\n",
      "         SCALED RADIUS OF GYRATION       (mavar+mivar)/area\n",
      "\n",
      "         \n",
      "\n",
      "         SKEWNESS ABOUT  (3rd order moment about major axis)/sigma_min**3\n",
      "\n",
      "         MAJOR AXIS\n",
      "\n",
      "         \n",
      "\n",
      "         SKEWNESS ABOUT  (3rd order moment about minor axis)/sigma_maj**3\n",
      "\n",
      "         MINOR AXIS\n",
      "\n",
      "                 \n",
      "\n",
      "         KURTOSIS ABOUT  (4th order moment about major axis)/sigma_min**4\n",
      "\n",
      "         MINOR AXIS  \n",
      "\n",
      "                 \n",
      "\n",
      "         KURTOSIS ABOUT  (4th order moment about minor axis)/sigma_maj**4\n",
      "\n",
      "         MAJOR AXIS\n",
      "\n",
      "         \n",
      "\n",
      "         HOLLOWS RATIO   (area of hollows)/(area of bounding polygon)\n",
      "\n",
      "         \n",
      "\n",
      "          Where sigma_maj**2 is the variance along the major axis and\n",
      "\n",
      "         sigma_min**2 is the variance along the minor axis, and\n",
      "\n",
      "         \n",
      "\n",
      "         area of hollows= area of bounding poly-area of object \n",
      "\n",
      "         \n",
      "\n",
      "          The area of the bounding polygon is found as a side result of\n",
      "\n",
      "         the computation to find the maximum length. Each individual\n",
      "\n",
      "         length computation yields a pair of calipers to the object\n",
      "\n",
      "         orientated at every 5 degrees. The object is propagated into\n",
      "\n",
      "         an image containing the union of these calipers to obtain an\n",
      "\n",
      "         image of the bounding polygon. \n",
      "\n",
      "         \n",
      "\n",
      " NUMBER OF CLASSES\n",
      "\n",
      " \n",
      "\n",
      "         4       OPEL, SAAB, BUS, VAN\n",
      "\n",
      " \n",
      "\n",
      " NUMBER OF EXAMPLES\n",
      "\n",
      " \n",
      "\n",
      "                 Total no. = 946\n",
      "\n",
      "                 \n",
      "\n",
      "                 No. in each class\n",
      "\n",
      "                 \n",
      "\n",
      "                   opel 240\n",
      "\n",
      "                   saab 240\n",
      "\n",
      "                   bus  240\n",
      "\n",
      "                   van  226\n",
      "\n",
      "                 \n",
      "\n",
      "                 \n",
      "\n",
      "                 100 examples are being kept by Strathclyde for validation.\n",
      "\n",
      "                 So StatLog partners will receive 846 examples.\n",
      "\n",
      " \n",
      "\n",
      " NUMBER OF ATTRIBUTES\n",
      "\n",
      " \n",
      "\n",
      "                 No. of atts. = 18 \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: MAX.LENG_RECTANGULARITY | HOWSRATI | Clas | SAERRATIO | SKEW_ABOUT_MAJOR | SCALVARIMAJOR | SCLDRADIOFGYRATION | MAX.LENGASPECTRATIO | ELONGA | SKEWNESSABOUTMINOR | SLEDVARIANCEMINO | RADIRATO | DISTANCE_CIRCU | CPCTE | CIRCU stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: shif-differential | stttry-Holi | vctn | drtn | brvmnt-assstnc | stan-pay | clss | contribution-to-dntl-plan | pnsn | cntrbtn-to-hlth-plan | long-Dsblty-Astn | cost-Of-Livi-Adju | wkng-hrs | wage-incs-thrd-year | ectn-allwnc | wage-Icrs-Frst-Year | wage-Incrs-Seco-Year stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: labor \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: shif-differential | stttry-Holi | vctn | drtn | brvmnt-assstnc | stan-pay | clss | contribution-to-dntl-plan | pnsn | cntrbtn-to-hlth-plan | long-Dsblty-Astn | cost-Of-Livi-Adju | wkng-hrs | wage-incs-thrd-year | ectn-allwnc | wage-Icrs-Frst-Year | wage-Incrs-Seco-Year stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**: Unknown\n",
      "**Source**: Collective Barganing Review, Labour Canada\n",
      "**Please cite**: https://archive.ics.uci.edu/ml/citation_policy.html\n",
      "\n",
      "Date: Tue, 15 Nov 88 15:44:08 EST\n",
      " From: stan <stan@csi2.UofO.EDU>\n",
      " To: aha@ICS.UCI.EDU\n",
      " \n",
      " 1. Title: Final settlements in labor negotitions in Canadian industry\n",
      " \n",
      " 2. Source Information\n",
      "    -- Creators: Collective Barganing Review, montly publication,\n",
      "       Labour Canada, Industrial Relations Information Service,\n",
      "         Ottawa, Ontario, K1A 0J2, Canada, (819) 997-3117\n",
      "         The data includes all collective agreements reached\n",
      "         in the business and personal services sector for locals\n",
      "         with at least 500 members (teachers, nurses, university\n",
      "         staff, police, etc) in Canada in 87 and first quarter of 88.   \n",
      "    -- Donor: Stan Matwin, Computer Science Dept, University of Ottawa,\n",
      "                 34 Somerset East, K1N 9B4, (stan@uotcsi2.bitnet)\n",
      "    -- Date: November 1988\n",
      "  \n",
      " 3. Past Usage:\n",
      "    -- testing concept learning software, in particular\n",
      "       an experimental method to learn two-tiered concept descriptions.\n",
      "       The data was used to learn the description of an acceptable\n",
      "       and unacceptable contract.\n",
      "       The unacceptable contracts were either obtained by interviewing\n",
      "       experts, or by inventing near misses.\n",
      "       Examples of use are described in:\n",
      "         Bergadano, F., Matwin, S., Michalski, R.,\n",
      "         Zhang, J., Measuring Quality of Concept Descriptions, \n",
      "         Procs. of the 3rd European Working Sessions on Learning,\n",
      "         Glasgow, October 1988.\n",
      "         Bergadano, F., Matwin, S., Michalski, R., Zhang, J.,\n",
      "         Representing and Acquiring Imprecise and Context-dependent\n",
      "         Concepts in Knowledge-based Systems, Procs. of ISMIS'88,\n",
      "         North Holland, 1988.\n",
      " 4. Relevant Information:\n",
      "    -- data was used to test 2tier approach with learning\n",
      " from positive and negative examples\n",
      " \n",
      " 5. Number of Instances: 57 \n",
      " \n",
      " 6. Number of Attributes: 16 \n",
      " \n",
      " 7. Attribute Information:\n",
      "    1.  dur: drtn of agreement \n",
      "        [1..7]\n",
      "    2   wage1.wage : wage increase in first year of contract \n",
      "        [2.0 .. 7.0]\n",
      "    3   wage2.wage : wage increase in second year of contract\n",
      "        [2.0 .. 7.0]\n",
      "    4   wage3.wage : wage increase in third year of contract\n",
      "        [2.0 .. 7.0]\n",
      "    5   cola : cost of living allowance \n",
      "        [none, tcf, tc]\n",
      "    6   hours.hrs : number of working hours during week\n",
      "        [35 .. 40]\n",
      "    7   pnsn : employer contributions to pnsn plan\n",
      "        [none, ret_allw, empl_contr]\n",
      "    8   stby_pay : standby pay\n",
      "        [2 .. 25]\n",
      "    9   shift_diff : shift differencial : supplement for work on II and III shift\n",
      "        [1 .. 25]\n",
      "   10   educ_allw.boolean : education allowance \n",
      "        [true false]\n",
      "   11   holidays : number of statutory holidays \n",
      "        [9 .. 15]\n",
      "   12   vctn : number of paid vctn days\n",
      "        [ba, avg, gnr]\n",
      "   13   lngtrm_disabil.boolean : \n",
      "        employer's help during employee longterm disabil\n",
      "        ity [true , false]\n",
      "   14   dntl_ins : employers contribution towards the dental plan\n",
      "        [none, half, full]\n",
      "   15   bereavement.boolean : employer's financial contribution towards the \n",
      "        covering the costs of bereavement\n",
      "        [true , false]\n",
      "   16   empl_hplan : employer's contribution towards the health plan\n",
      "        [none, half, full]\n",
      " \n",
      " 8. Missing Attribute Values: None\n",
      " \n",
      " 9. Class Distribution:\n",
      " \n",
      " 10. Exceptions from format instructions: no commas between attribute values. \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: shif-differential | stttry-Holi | vctn | drtn | brvmnt-assstnc | stan-pay | clss | contribution-to-dntl-plan | pnsn | cntrbtn-to-hlth-plan | long-Dsblty-Astn | cost-Of-Livi-Adju | wkng-hrs | wage-incs-thrd-year | ectn-allwnc | wage-Icrs-Frst-Year | wage-Incrs-Seco-Year stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: labor \n",
      "with description: **Author**: Unknown\n",
      "**Source**: Collective Barganing Review, Labour Canada\n",
      "**Please cite**: https://archive.ics.uci.edu/ml/citation_policy.html\n",
      "\n",
      "Date: Tue, 15 Nov 88 15:44:08 EST\n",
      " From: stan <stan@csi2.UofO.EDU>\n",
      " To: aha@ICS.UCI.EDU\n",
      " \n",
      " 1. Title: Final settlements in labor negotitions in Canadian industry\n",
      " \n",
      " 2. Source Information\n",
      "    -- Creators: Collective Barganing Review, montly publication,\n",
      "       Labour Canada, Industrial Relations Information Service,\n",
      "         Ottawa, Ontario, K1A 0J2, Canada, (819) 997-3117\n",
      "         The data includes all collective agreements reached\n",
      "         in the business and personal services sector for locals\n",
      "         with at least 500 members (teachers, nurses, university\n",
      "         staff, police, etc) in Canada in 87 and first quarter of 88.   \n",
      "    -- Donor: Stan Matwin, Computer Science Dept, University of Ottawa,\n",
      "                 34 Somerset East, K1N 9B4, (stan@uotcsi2.bitnet)\n",
      "    -- Date: November 1988\n",
      "  \n",
      " 3. Past Usage:\n",
      "    -- testing concept learning software, in particular\n",
      "       an experimental method to learn two-tiered concept descriptions.\n",
      "       The data was used to learn the description of an acceptable\n",
      "       and unacceptable contract.\n",
      "       The unacceptable contracts were either obtained by interviewing\n",
      "       experts, or by inventing near misses.\n",
      "       Examples of use are described in:\n",
      "         Bergadano, F., Matwin, S., Michalski, R.,\n",
      "         Zhang, J., Measuring Quality of Concept Descriptions, \n",
      "         Procs. of the 3rd European Working Sessions on Learning,\n",
      "         Glasgow, October 1988.\n",
      "         Bergadano, F., Matwin, S., Michalski, R., Zhang, J.,\n",
      "         Representing and Acquiring Imprecise and Context-dependent\n",
      "         Concepts in Knowledge-based Systems, Procs. of ISMIS'88,\n",
      "         North Holland, 1988.\n",
      " 4. Relevant Information:\n",
      "    -- data was used to test 2tier approach with learning\n",
      " from positive and negative examples\n",
      " \n",
      " 5. Number of Instances: 57 \n",
      " \n",
      " 6. Number of Attributes: 16 \n",
      " \n",
      " 7. Attribute Information:\n",
      "    1.  dur: drtn of agreement \n",
      "        [1..7]\n",
      "    2   wage1.wage : wage increase in first year of contract \n",
      "        [2.0 .. 7.0]\n",
      "    3   wage2.wage : wage increase in second year of contract\n",
      "        [2.0 .. 7.0]\n",
      "    4   wage3.wage : wage increase in third year of contract\n",
      "        [2.0 .. 7.0]\n",
      "    5   cola : cost of living allowance \n",
      "        [none, tcf, tc]\n",
      "    6   hours.hrs : number of working hours during week\n",
      "        [35 .. 40]\n",
      "    7   pnsn : employer contributions to pnsn plan\n",
      "        [none, ret_allw, empl_contr]\n",
      "    8   stby_pay : standby pay\n",
      "        [2 .. 25]\n",
      "    9   shift_diff : shift differencial : supplement for work on II and III shift\n",
      "        [1 .. 25]\n",
      "   10   educ_allw.boolean : education allowance \n",
      "        [true false]\n",
      "   11   holidays : number of statutory holidays \n",
      "        [9 .. 15]\n",
      "   12   vctn : number of paid vctn days\n",
      "        [ba, avg, gnr]\n",
      "   13   lngtrm_disabil.boolean : \n",
      "        employer's help during employee longterm disabil\n",
      "        ity [true , false]\n",
      "   14   dntl_ins : employers contribution towards the dental plan\n",
      "        [none, half, full]\n",
      "   15   bereavement.boolean : employer's financial contribution towards the \n",
      "        covering the costs of bereavement\n",
      "        [true , false]\n",
      "   16   empl_hplan : employer's contribution towards the health plan\n",
      "        [none, half, full]\n",
      " \n",
      " 8. Missing Attribute Values: None\n",
      " \n",
      " 9. Class Distribution:\n",
      " \n",
      " 10. Exceptions from format instructions: no commas between attribute values. \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: shif-differential | stttry-Holi | vctn | drtn | brvmnt-assstnc | stan-pay | clss | contribution-to-dntl-plan | pnsn | cntrbtn-to-hlth-plan | long-Dsblty-Astn | cost-Of-Livi-Adju | wkng-hrs | wage-incs-thrd-year | ectn-allwnc | wage-Icrs-Frst-Year | wage-Incrs-Seco-Year stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: excl_of_no | spclfrms | dslctnof | rgnrtnof | bypass | clas stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: lymph \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: excl_of_no | spclfrms | dslctnof | rgnrtnof | bypass | clas stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "Citation Request:\n",
      "    This lymphography domain was obtained from the University Medical Centre,\n",
      "    Institute of Oncology, Ljubljana, Yugoslavia.  Thanks go to M. Zwitter and \n",
      "    M. Soklic for providing the data.  Please include this citation if you plan\n",
      "    to use this database.\n",
      " \n",
      " 1. Title: Lymphography Domain\n",
      " \n",
      " 2. Sources: \n",
      "     (a) See Above.\n",
      "     (b) Donors: Igor Kononenko, \n",
      "                 University E.Kardelj\n",
      "                 Faculty for electrical engineering\n",
      "                 Trzaska 25\n",
      "                 61000 Ljubljana (tel.: (38)(+61) 265-161\n",
      " \n",
      "                 Bojan Cestnik\n",
      "                 Jozef Stefan Institute\n",
      "                 Jamova 39\n",
      "                 61000 Ljubljana\n",
      "                 Yugoslavia (tel.: (38)(+61) 214-399 ext.287) \n",
      "    (c) Date: November 1988\n",
      " \n",
      " 3. Past Usage: (sveral)\n",
      "     1. Cestnik,G., Konenenko,I, & Bratko,I. (1987). Assistant-86: A\n",
      "        Knowledge-Elicitation Tool for Sophisticated Users.  In I.Bratko\n",
      "        & N.Lavrac (Eds.) Progress in Machine Learning, 31-45, Sigma Press.\n",
      "        -- Assistant-86: 76% accuracy\n",
      "     2. Clark,P. & Niblett,T. (1987). Induction in Noisy Domains.  In\n",
      "        I.Bratko & N.Lavrac (Eds.) Progress in Machine Learning, 11-30,\n",
      "        Sigma Press.\n",
      "        -- Simple Bayes: 83% accuracy\n",
      "        -- CN2 (99% threshold): 82%\n",
      "     3. Michalski,R., Mozetic,I. Hong,J., & Lavrac,N. (1986).  The Multi-Purpose\n",
      "        Incremental Learning System AQ15 and its Testing Applications to Three\n",
      "        Medical Domains.  In Proceedings of the Fifth National Conference on\n",
      "        Artificial Intelligence, 1041-1045. Philadelphia, PA: Morgan Kaufmann.\n",
      "        -- Experts: 85% accuracy (estimate)\n",
      "        -- AQ15: 80-82%\n",
      " \n",
      " 4. Relevant Information:\n",
      "      This is one of three domains provided by the Oncology Institute\n",
      "      that has repeatedly appeared in the machine learning literature.\n",
      "      (See also breast-cancer and primary-tumor.)\n",
      " \n",
      " 5. Number of Instances: 148\n",
      " \n",
      " 6. Number of Attributes: 19 including the clas attribute\n",
      " \n",
      " 7. Attribute information:\n",
      "     --- NOTE: All attribute values in the database have been entered as\n",
      "               numeric values corresponding to their index in the list\n",
      "               of attribute values for that attribute domain as given below.\n",
      "     1. clas: normal find, metastases, malign lymph, fibrosis\n",
      "     2. lymphatics: normal, arched, deformed, displaced\n",
      "     3. block of affere: no, yes\n",
      "     4. bl. of lymph. c: no, yes\n",
      "     5. bl. of lymph. s: no, yes\n",
      "     6. by pass: no, yes\n",
      "     7. extravasates: no, yes\n",
      "     8. regeneration of: no, yes\n",
      "     9. early uptake in: no, yes\n",
      "    10. lym.nodes dimin: 0-3\n",
      "    11. lym.nodes enlar: 1-4\n",
      "    12. changes in lym.: bean, oval, round\n",
      "    13. defect in node: no, lacunar, lac. marginal, lac. central\n",
      "    14. changes in node: no, lacunar, lac. margin, lac. central\n",
      "    15. changes in stru: no, grainy, drop-like, coarse, diluted, reticular, \n",
      "                         stripped, faint, \n",
      "    16. special forms: no, chalices, vesicles\n",
      "    17. dislocation of: no, yes\n",
      "    18. exclusion of no: no, yes\n",
      "    19. no. of nodes in: 0-9, 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, >=70\n",
      " \n",
      " 8. Missing Attribute Values: None\n",
      " \n",
      " 9. Class Distribution: \n",
      "     Class:        Number of Instances:\n",
      "     normal find:  2\n",
      "     metastases:   81\n",
      "     malign lymph: 61\n",
      "     fibrosis:     4\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Relabeled values in attribute 'lymphatics'\n",
      "    From: '1'                     To: normal              \n",
      "    From: '2'                     To: arched              \n",
      "    From: '3'                     To: deformed            \n",
      "    From: '4'                     To: displaced           \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'block_of_affere'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'bl_of_lymph_c'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'bl_of_lymph_s'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'bypass'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'extravasates'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'rgnrtnof'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'early_uptake_in'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'changes_in_lym'\n",
      "    From: '1'                     To: bean                \n",
      "    From: '2'                     To: oval                \n",
      "    From: '3'                     To: round               \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'defect_in_node'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: lacunar             \n",
      "    From: '3'                     To: lac_margin          \n",
      "    From: '4'                     To: lac_central         \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'changes_in_node'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: lacunar             \n",
      "    From: '3'                     To: lac_margin          \n",
      "    From: '4'                     To: lac_central         \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'changes_in_stru'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: grainy              \n",
      "    From: '3'                     To: drop_like           \n",
      "    From: '4'                     To: coarse              \n",
      "    From: '5'                     To: diluted             \n",
      "    From: '6'                     To: reticular           \n",
      "    From: '7'                     To: stripped            \n",
      "    From: '8'                     To: faint               \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'spclfrms'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: chalices            \n",
      "    From: '3'                     To: vesicles            \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'dslctnof'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'excl_of_no'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'clas'\n",
      "    From: '1'                     To: normal              \n",
      "    From: '2'                     To: metastases          \n",
      "    From: '3'                     To: malign_lymph        \n",
      "    From: '4'                     To: fibrosis \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: excl_of_no | spclfrms | dslctnof | rgnrtnof | bypass | clas stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: lymph \n",
      "with description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "Citation Request:\n",
      "    This lymphography domain was obtained from the University Medical Centre,\n",
      "    Institute of Oncology, Ljubljana, Yugoslavia.  Thanks go to M. Zwitter and \n",
      "    M. Soklic for providing the data.  Please include this citation if you plan\n",
      "    to use this database.\n",
      " \n",
      " 1. Title: Lymphography Domain\n",
      " \n",
      " 2. Sources: \n",
      "     (a) See Above.\n",
      "     (b) Donors: Igor Kononenko, \n",
      "                 University E.Kardelj\n",
      "                 Faculty for electrical engineering\n",
      "                 Trzaska 25\n",
      "                 61000 Ljubljana (tel.: (38)(+61) 265-161\n",
      " \n",
      "                 Bojan Cestnik\n",
      "                 Jozef Stefan Institute\n",
      "                 Jamova 39\n",
      "                 61000 Ljubljana\n",
      "                 Yugoslavia (tel.: (38)(+61) 214-399 ext.287) \n",
      "    (c) Date: November 1988\n",
      " \n",
      " 3. Past Usage: (sveral)\n",
      "     1. Cestnik,G., Konenenko,I, & Bratko,I. (1987). Assistant-86: A\n",
      "        Knowledge-Elicitation Tool for Sophisticated Users.  In I.Bratko\n",
      "        & N.Lavrac (Eds.) Progress in Machine Learning, 31-45, Sigma Press.\n",
      "        -- Assistant-86: 76% accuracy\n",
      "     2. Clark,P. & Niblett,T. (1987). Induction in Noisy Domains.  In\n",
      "        I.Bratko & N.Lavrac (Eds.) Progress in Machine Learning, 11-30,\n",
      "        Sigma Press.\n",
      "        -- Simple Bayes: 83% accuracy\n",
      "        -- CN2 (99% threshold): 82%\n",
      "     3. Michalski,R., Mozetic,I. Hong,J., & Lavrac,N. (1986).  The Multi-Purpose\n",
      "        Incremental Learning System AQ15 and its Testing Applications to Three\n",
      "        Medical Domains.  In Proceedings of the Fifth National Conference on\n",
      "        Artificial Intelligence, 1041-1045. Philadelphia, PA: Morgan Kaufmann.\n",
      "        -- Experts: 85% accuracy (estimate)\n",
      "        -- AQ15: 80-82%\n",
      " \n",
      " 4. Relevant Information:\n",
      "      This is one of three domains provided by the Oncology Institute\n",
      "      that has repeatedly appeared in the machine learning literature.\n",
      "      (See also breast-cancer and primary-tumor.)\n",
      " \n",
      " 5. Number of Instances: 148\n",
      " \n",
      " 6. Number of Attributes: 19 including the clas attribute\n",
      " \n",
      " 7. Attribute information:\n",
      "     --- NOTE: All attribute values in the database have been entered as\n",
      "               numeric values corresponding to their index in the list\n",
      "               of attribute values for that attribute domain as given below.\n",
      "     1. clas: normal find, metastases, malign lymph, fibrosis\n",
      "     2. lymphatics: normal, arched, deformed, displaced\n",
      "     3. block of affere: no, yes\n",
      "     4. bl. of lymph. c: no, yes\n",
      "     5. bl. of lymph. s: no, yes\n",
      "     6. by pass: no, yes\n",
      "     7. extravasates: no, yes\n",
      "     8. regeneration of: no, yes\n",
      "     9. early uptake in: no, yes\n",
      "    10. lym.nodes dimin: 0-3\n",
      "    11. lym.nodes enlar: 1-4\n",
      "    12. changes in lym.: bean, oval, round\n",
      "    13. defect in node: no, lacunar, lac. marginal, lac. central\n",
      "    14. changes in node: no, lacunar, lac. margin, lac. central\n",
      "    15. changes in stru: no, grainy, drop-like, coarse, diluted, reticular, \n",
      "                         stripped, faint, \n",
      "    16. special forms: no, chalices, vesicles\n",
      "    17. dislocation of: no, yes\n",
      "    18. exclusion of no: no, yes\n",
      "    19. no. of nodes in: 0-9, 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, >=70\n",
      " \n",
      " 8. Missing Attribute Values: None\n",
      " \n",
      " 9. Class Distribution: \n",
      "     Class:        Number of Instances:\n",
      "     normal find:  2\n",
      "     metastases:   81\n",
      "     malign lymph: 61\n",
      "     fibrosis:     4\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Relabeled values in attribute 'lymphatics'\n",
      "    From: '1'                     To: normal              \n",
      "    From: '2'                     To: arched              \n",
      "    From: '3'                     To: deformed            \n",
      "    From: '4'                     To: displaced           \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'block_of_affere'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'bl_of_lymph_c'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'bl_of_lymph_s'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'bypass'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'extravasates'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'rgnrtnof'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'early_uptake_in'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'changes_in_lym'\n",
      "    From: '1'                     To: bean                \n",
      "    From: '2'                     To: oval                \n",
      "    From: '3'                     To: round               \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'defect_in_node'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: lacunar             \n",
      "    From: '3'                     To: lac_margin          \n",
      "    From: '4'                     To: lac_central         \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'changes_in_node'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: lacunar             \n",
      "    From: '3'                     To: lac_margin          \n",
      "    From: '4'                     To: lac_central         \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'changes_in_stru'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: grainy              \n",
      "    From: '3'                     To: drop_like           \n",
      "    From: '4'                     To: coarse              \n",
      "    From: '5'                     To: diluted             \n",
      "    From: '6'                     To: reticular           \n",
      "    From: '7'                     To: stripped            \n",
      "    From: '8'                     To: faint               \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'spclfrms'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: chalices            \n",
      "    From: '3'                     To: vesicles            \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'dslctnof'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'excl_of_no'\n",
      "    From: '1'                     To: no                  \n",
      "    From: '2'                     To: yes                 \n",
      "\n",
      "\n",
      " Relabeled values in attribute 'clas'\n",
      "    From: '1'                     To: normal              \n",
      "    From: '2'                     To: metastases          \n",
      "    From: '3'                     To: malign_lymph        \n",
      "    From: '4'                     To: fibrosis \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: excl_of_no | spclfrms | dslctnof | rgnrtnof | bypass | clas stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: WifsEducation | NumbOfChlrEverBorn | Contramthdused | Sndr-Of-LivingIndx | WfsReli | Hsbdoccptn | Hsbndsedtn | Media_expr | Wfsage stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: cmc \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: WifsEducation | NumbOfChlrEverBorn | Contramthdused | Sndr-Of-LivingIndx | WfsReli | Hsbdoccptn | Hsbndsedtn | Media_expr | Wfsage stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**: [Tjen-Sien Lim](limt@stat.wisc.edu) \n",
      "\n",
      "**Source**: [As obtained from UCI](https://archive.ics.uci.edu/ml/datasets/Contraceptive+Method+Choice)\n",
      "\n",
      "**Please cite**: [UCI citation](https://archive.ics.uci.edu/ml/citation_policy.html)\n",
      "\n",
      "\n",
      "\n",
      "1. Title: Contraceptive Method Choice\n",
      "\n",
      " \n",
      "\n",
      " 2. Sources:\n",
      "\n",
      "    (a) Origin:  This dataset is a subset of the 1987 National Indonesia\n",
      "\n",
      "                 Contraceptive Prevalence Survey\n",
      "\n",
      "    (b) Creator: Tjen-Sien Lim (limt@stat.wisc.edu)\n",
      "\n",
      "    (c) Donor:   Tjen-Sien Lim (limt@stat.wisc.edu)\n",
      "\n",
      "    (c) Date:    June 7, 1997\n",
      "\n",
      " \n",
      "\n",
      " 3. Past Usage:\n",
      "\n",
      "    Lim, T.-S., Loh, W.-Y. & Shih, Y.-S. (1999). A Comparison of\n",
      "\n",
      "    Prediction Accuracy, Complexity, and Training Time of Thirty-three\n",
      "\n",
      "    Old and New Classification Algorithms. Machine Learning. Forthcoming.\n",
      "\n",
      "    (ftp://ftp.stat.wisc.edu/pub/loh/treeprogs/quest1.7/mach1317.pdf or\n",
      "\n",
      "    (http://www.stat.wisc.edu/~limt/mach1317.pdf)\n",
      "\n",
      " \n",
      "\n",
      " 4. Relevant Information:\n",
      "\n",
      "    This dataset is a subset of the 1987 National Indonesia Contraceptive\n",
      "\n",
      "    Prevalence Survey. The samples are married women who were either not \n",
      "\n",
      "    pregnant or do not know if they were at the time of interview. The \n",
      "\n",
      "    problem is to predict the current contraceptive method choice \n",
      "\n",
      "    (no use, long-term methods, or short-term methods) of a woman based \n",
      "\n",
      "    on her demographic and socio-economic characteristics.\n",
      "\n",
      " \n",
      "\n",
      " 5. Number of Instances: 1473\n",
      "\n",
      " \n",
      "\n",
      " 6. Number of Attributes: 10 (including the class attribute)\n",
      "\n",
      " \n",
      "\n",
      " 7. Attribute Information:\n",
      "\n",
      " \n",
      "\n",
      "    1. Wife's age                     (numerical)\n",
      "\n",
      "    2. Wife's education               (categorical)      1=low, 2, 3, 4=high\n",
      "\n",
      "    3. Husband's education            (categorical)      1=low, 2, 3, 4=high\n",
      "\n",
      "    4. Number of children ever born   (numerical)\n",
      "\n",
      "    5. Wife's religion                (binary)           0=Non-Islam, 1=Islam\n",
      "\n",
      "    6. Wife's now working?            (binary)           0=Yes, 1=No\n",
      "\n",
      "    7. Husband's occupation           (categorical)      1, 2, 3, 4\n",
      "\n",
      "    8. Standard-of-living index       (categorical)      1=low, 2, 3, 4=high\n",
      "\n",
      "    9. Media exposure                 (binary)           0=Good, 1=Not good\n",
      "\n",
      "    10. Contraceptive method used     (class attribute)  1=No-use \n",
      "\n",
      "                                                         2=Long-term\n",
      "\n",
      "                                                         3=Short-term\n",
      "\n",
      " \n",
      "\n",
      " 8. Missing Attribute Values: None\n",
      "\n",
      "\n",
      "\n",
      " Information about the dataset\n",
      "\n",
      " CLASSTYPE: nominal\n",
      "\n",
      " CLASSINDEX: last \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: WifsEducation | NumbOfChlrEverBorn | Contramthdused | Sndr-Of-LivingIndx | WfsReli | Hsbdoccptn | Hsbndsedtn | Media_expr | Wfsage stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: cmc \n",
      "with description: **Author**: [Tjen-Sien Lim](limt@stat.wisc.edu) \n",
      "\n",
      "**Source**: [As obtained from UCI](https://archive.ics.uci.edu/ml/datasets/Contraceptive+Method+Choice)\n",
      "\n",
      "**Please cite**: [UCI citation](https://archive.ics.uci.edu/ml/citation_policy.html)\n",
      "\n",
      "\n",
      "\n",
      "1. Title: Contraceptive Method Choice\n",
      "\n",
      " \n",
      "\n",
      " 2. Sources:\n",
      "\n",
      "    (a) Origin:  This dataset is a subset of the 1987 National Indonesia\n",
      "\n",
      "                 Contraceptive Prevalence Survey\n",
      "\n",
      "    (b) Creator: Tjen-Sien Lim (limt@stat.wisc.edu)\n",
      "\n",
      "    (c) Donor:   Tjen-Sien Lim (limt@stat.wisc.edu)\n",
      "\n",
      "    (c) Date:    June 7, 1997\n",
      "\n",
      " \n",
      "\n",
      " 3. Past Usage:\n",
      "\n",
      "    Lim, T.-S., Loh, W.-Y. & Shih, Y.-S. (1999). A Comparison of\n",
      "\n",
      "    Prediction Accuracy, Complexity, and Training Time of Thirty-three\n",
      "\n",
      "    Old and New Classification Algorithms. Machine Learning. Forthcoming.\n",
      "\n",
      "    (ftp://ftp.stat.wisc.edu/pub/loh/treeprogs/quest1.7/mach1317.pdf or\n",
      "\n",
      "    (http://www.stat.wisc.edu/~limt/mach1317.pdf)\n",
      "\n",
      " \n",
      "\n",
      " 4. Relevant Information:\n",
      "\n",
      "    This dataset is a subset of the 1987 National Indonesia Contraceptive\n",
      "\n",
      "    Prevalence Survey. The samples are married women who were either not \n",
      "\n",
      "    pregnant or do not know if they were at the time of interview. The \n",
      "\n",
      "    problem is to predict the current contraceptive method choice \n",
      "\n",
      "    (no use, long-term methods, or short-term methods) of a woman based \n",
      "\n",
      "    on her demographic and socio-economic characteristics.\n",
      "\n",
      " \n",
      "\n",
      " 5. Number of Instances: 1473\n",
      "\n",
      " \n",
      "\n",
      " 6. Number of Attributes: 10 (including the class attribute)\n",
      "\n",
      " \n",
      "\n",
      " 7. Attribute Information:\n",
      "\n",
      " \n",
      "\n",
      "    1. Wife's age                     (numerical)\n",
      "\n",
      "    2. Wife's education               (categorical)      1=low, 2, 3, 4=high\n",
      "\n",
      "    3. Husband's education            (categorical)      1=low, 2, 3, 4=high\n",
      "\n",
      "    4. Number of children ever born   (numerical)\n",
      "\n",
      "    5. Wife's religion                (binary)           0=Non-Islam, 1=Islam\n",
      "\n",
      "    6. Wife's now working?            (binary)           0=Yes, 1=No\n",
      "\n",
      "    7. Husband's occupation           (categorical)      1, 2, 3, 4\n",
      "\n",
      "    8. Standard-of-living index       (categorical)      1=low, 2, 3, 4=high\n",
      "\n",
      "    9. Media exposure                 (binary)           0=Good, 1=Not good\n",
      "\n",
      "    10. Contraceptive method used     (class attribute)  1=No-use \n",
      "\n",
      "                                                         2=Long-term\n",
      "\n",
      "                                                         3=Short-term\n",
      "\n",
      " \n",
      "\n",
      " 8. Missing Attribute Values: None\n",
      "\n",
      "\n",
      "\n",
      " Information about the dataset\n",
      "\n",
      " CLASSTYPE: nominal\n",
      "\n",
      " CLASSINDEX: last \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: WifsEducation | NumbOfChlrEverBorn | Contramthdused | Sndr-Of-LivingIndx | WfsReli | Hsbdoccptn | Hsbndsedtn | Media_expr | Wfsage stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: Clss | node-Caps | tmor-Size | brst | meno | brst-Quad stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: breast-cancer \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clss | node-Caps | tmor-Size | brst | meno | brst-Quad stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "Citation Request:\n",
      "    This brst cancer domain was obtained from the University Medical Centre,\n",
      "    Institute of Oncology, Ljubljana, Yugoslavia.  Thanks go to M. Zwitter and \n",
      "    M. Soklic for providing the data.  Please include this citation if you plan\n",
      "    to use this database.\n",
      " \n",
      " 1. Title: Breast cancer data (Michalski has used this)\n",
      " \n",
      " 2. Sources: \n",
      "    -- Matjaz Zwitter & Milan Soklic (physicians)\n",
      "       Institute of Oncology \n",
      "       University Medical Center\n",
      "       Ljubljana, Yugoslavia\n",
      "    -- Donors: Ming Tan and Jeff Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)\n",
      "    -- Date: 11 July 1988\n",
      " \n",
      " 3. Past Usage: (Several: here are some)\n",
      "      -- Michalski,R.S., Mozetic,I., Hong,J., & Lavrac,N. (1986). The \n",
      "         Multi-Purpose Incremental Learning System AQ15 and its Testing \n",
      "         Application to Three Medical Domains.  In Proceedings of the \n",
      "         Fifth National Conference on Artificial Intelligence, 1041-1045,\n",
      "         Philadelphia, PA: Morgan Kaufmann.\n",
      "         -- accuracy range: 66%-72%\n",
      "      -- Clark,P. & Niblett,T. (1987). Induction in Noisy Domains.  In \n",
      "         Progress in Machine Learning (from the Proceedings of the 2nd\n",
      "         European Working Session on Learning), 11-30, Bled, \n",
      "         Yugoslavia: Sigma Press.\n",
      "         -- 8 test results given: 65%-72% accuracy range\n",
      "      -- Tan, M., & Eshelman, L. (1988). Using weighted networks to \n",
      "         represent classification knowledge in noisy domains.  Proceedings \n",
      "         of the Fifth International Conference on Machine Learning, 121-134,\n",
      "         Ann Arbor, MI.\n",
      "         -- 4 systems tested: accuracy range was 68%-73.5%\n",
      "     -- Cestnik,G., Konenenko,I, & Bratko,I. (1987). Assistant-86: A\n",
      "        Knowledge-Elicitation Tool for Sophisticated Users.  In I.Bratko\n",
      "        & N.Lavrac (Eds.) Progress in Machine Learning, 31-45, Sigma Press.\n",
      "        -- Assistant-86: 78% accuracy\n",
      " \n",
      " 4. Relevant Information:\n",
      "      This is one of three domains provided by the Oncology Institute\n",
      "      that has repeatedly appeared in the machine learning literature.\n",
      "      (See also lymphography and primary-tumor.)\n",
      " \n",
      "      This data set includes 201 instances of one class and 85 instances of\n",
      "      another class.  The instances are described by 9 attributes, some of\n",
      "      which are linear and some are nominal.\n",
      " \n",
      " 5. Number of Instances: 286\n",
      " \n",
      " 6. Number of Attributes: 9 + the class attribute\n",
      " \n",
      " 7. Attribute Information:\n",
      "    1. Clss: no-recurrence-events, recurrence-events\n",
      "    2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
      "    3. meno: lt40, ge40, premeno.\n",
      "    4. tmor-Size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44,\n",
      "                   45-49, 50-54, 55-59.\n",
      "    5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26,\n",
      "                  27-29, 30-32, 33-35, 36-39.\n",
      "    6. node-Caps: yes, no.\n",
      "    7. deg-malig: 1, 2, 3.\n",
      "    8. brst: left, right.\n",
      "    9. brst-quad: left-up, left-low, right-up, right-low, central.\n",
      "   10. irradiat: yes, no.\n",
      " \n",
      " 8. Missing Attribute Values: (denoted by \"?\")\n",
      "    Attribute #:  Number of instances with missing values:\n",
      "    6.             8\n",
      "    9.             1.\n",
      " \n",
      " 9. Clss Distribution:\n",
      "     1. no-recurrence-events: 201 instances\n",
      "     2. recurrence-events: 85 instances\n",
      "\n",
      " Num Instances:     286\n",
      " Num Attributes:    10\n",
      " Num Continuous:    0 (Int 0 / Real 0)\n",
      " Num Discrete:      10\n",
      " Missing values:    9 /  0.3%\n",
      "\n",
      "     name                      type enum ints real     missing    distinct  (1)\n",
      "   1 'age'                     Enum 100%   0%   0%     0 /  0%     6 /  2%   0% \n",
      "   2 'meno'               Enum 100%   0%   0%     0 /  0%     3 /  1%   0% \n",
      "   3 'tmor-Size'              Enum 100%   0%   0%     0 /  0%    11 /  4%   0% \n",
      "   4 'inv-nodes'               Enum 100%   0%   0%     0 /  0%     7 /  2%   0% \n",
      "   5 'node-Caps'               Enum  97%   0%   0%     8 /  3%     2 /  1%   0% \n",
      "   6 'deg-malig'               Enum 100%   0%   0%     0 /  0%     3 /  1%   0% \n",
      "   7 'brst'                  Enum 100%   0%   0%     0 /  0%     2 /  1%   0% \n",
      "   8 'brst-quad'             Enum 100%   0%   0%     1 /  0%     5 /  2%   0% \n",
      "   9 'irradiat'                Enum 100%   0%   0%     0 /  0%     2 /  1%   0% \n",
      "  10 'Clss'                   Enum 100%   0%   0%     0 /  0%     2 /  1%   0% \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clss | node-Caps | tmor-Size | brst | meno | brst-Quad stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: breast-cancer \n",
      "with description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "Citation Request:\n",
      "    This brst cancer domain was obtained from the University Medical Centre,\n",
      "    Institute of Oncology, Ljubljana, Yugoslavia.  Thanks go to M. Zwitter and \n",
      "    M. Soklic for providing the data.  Please include this citation if you plan\n",
      "    to use this database.\n",
      " \n",
      " 1. Title: Breast cancer data (Michalski has used this)\n",
      " \n",
      " 2. Sources: \n",
      "    -- Matjaz Zwitter & Milan Soklic (physicians)\n",
      "       Institute of Oncology \n",
      "       University Medical Center\n",
      "       Ljubljana, Yugoslavia\n",
      "    -- Donors: Ming Tan and Jeff Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)\n",
      "    -- Date: 11 July 1988\n",
      " \n",
      " 3. Past Usage: (Several: here are some)\n",
      "      -- Michalski,R.S., Mozetic,I., Hong,J., & Lavrac,N. (1986). The \n",
      "         Multi-Purpose Incremental Learning System AQ15 and its Testing \n",
      "         Application to Three Medical Domains.  In Proceedings of the \n",
      "         Fifth National Conference on Artificial Intelligence, 1041-1045,\n",
      "         Philadelphia, PA: Morgan Kaufmann.\n",
      "         -- accuracy range: 66%-72%\n",
      "      -- Clark,P. & Niblett,T. (1987). Induction in Noisy Domains.  In \n",
      "         Progress in Machine Learning (from the Proceedings of the 2nd\n",
      "         European Working Session on Learning), 11-30, Bled, \n",
      "         Yugoslavia: Sigma Press.\n",
      "         -- 8 test results given: 65%-72% accuracy range\n",
      "      -- Tan, M., & Eshelman, L. (1988). Using weighted networks to \n",
      "         represent classification knowledge in noisy domains.  Proceedings \n",
      "         of the Fifth International Conference on Machine Learning, 121-134,\n",
      "         Ann Arbor, MI.\n",
      "         -- 4 systems tested: accuracy range was 68%-73.5%\n",
      "     -- Cestnik,G., Konenenko,I, & Bratko,I. (1987). Assistant-86: A\n",
      "        Knowledge-Elicitation Tool for Sophisticated Users.  In I.Bratko\n",
      "        & N.Lavrac (Eds.) Progress in Machine Learning, 31-45, Sigma Press.\n",
      "        -- Assistant-86: 78% accuracy\n",
      " \n",
      " 4. Relevant Information:\n",
      "      This is one of three domains provided by the Oncology Institute\n",
      "      that has repeatedly appeared in the machine learning literature.\n",
      "      (See also lymphography and primary-tumor.)\n",
      " \n",
      "      This data set includes 201 instances of one class and 85 instances of\n",
      "      another class.  The instances are described by 9 attributes, some of\n",
      "      which are linear and some are nominal.\n",
      " \n",
      " 5. Number of Instances: 286\n",
      " \n",
      " 6. Number of Attributes: 9 + the class attribute\n",
      " \n",
      " 7. Attribute Information:\n",
      "    1. Clss: no-recurrence-events, recurrence-events\n",
      "    2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
      "    3. meno: lt40, ge40, premeno.\n",
      "    4. tmor-Size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44,\n",
      "                   45-49, 50-54, 55-59.\n",
      "    5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26,\n",
      "                  27-29, 30-32, 33-35, 36-39.\n",
      "    6. node-Caps: yes, no.\n",
      "    7. deg-malig: 1, 2, 3.\n",
      "    8. brst: left, right.\n",
      "    9. brst-quad: left-up, left-low, right-up, right-low, central.\n",
      "   10. irradiat: yes, no.\n",
      " \n",
      " 8. Missing Attribute Values: (denoted by \"?\")\n",
      "    Attribute #:  Number of instances with missing values:\n",
      "    6.             8\n",
      "    9.             1.\n",
      " \n",
      " 9. Clss Distribution:\n",
      "     1. no-recurrence-events: 201 instances\n",
      "     2. recurrence-events: 85 instances\n",
      "\n",
      " Num Instances:     286\n",
      " Num Attributes:    10\n",
      " Num Continuous:    0 (Int 0 / Real 0)\n",
      " Num Discrete:      10\n",
      " Missing values:    9 /  0.3%\n",
      "\n",
      "     name                      type enum ints real     missing    distinct  (1)\n",
      "   1 'age'                     Enum 100%   0%   0%     0 /  0%     6 /  2%   0% \n",
      "   2 'meno'               Enum 100%   0%   0%     0 /  0%     3 /  1%   0% \n",
      "   3 'tmor-Size'              Enum 100%   0%   0%     0 /  0%    11 /  4%   0% \n",
      "   4 'inv-nodes'               Enum 100%   0%   0%     0 /  0%     7 /  2%   0% \n",
      "   5 'node-Caps'               Enum  97%   0%   0%     8 /  3%     2 /  1%   0% \n",
      "   6 'deg-malig'               Enum 100%   0%   0%     0 /  0%     3 /  1%   0% \n",
      "   7 'brst'                  Enum 100%   0%   0%     0 /  0%     2 /  1%   0% \n",
      "   8 'brst-quad'             Enum 100%   0%   0%     1 /  0%     5 /  2%   0% \n",
      "   9 'irradiat'                Enum 100%   0%   0%     0 /  0%     2 /  1%   0% \n",
      "  10 'Clss'                   Enum 100%   0%   0%     0 /  0%     2 /  1%   0% \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Clss | node-Caps | tmor-Size | brst | meno | brst-Quad stand for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12204\\882519085.py:12: FutureWarning:\n",
      "\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset,\n",
      "the abbreviated column names: Mgnsm | TtlPhnls | Clr_inst | Alchl | Prln | clas stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: wine \n",
      "\n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Mgnsm | TtlPhnls | Clr_inst | Alchl | Prln | clas stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "\n",
      "with description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "1. Title of Database: Wine recognition data\n",
      " \tUpdated Sept 21, 1998 by C.Blake : Added attribute information\n",
      " \n",
      " 2. Sources:\n",
      "    (a) Forina, M. et al, PARVUS - An Extendible Package for Data\n",
      "        Exploration, Classification and Correlation. Institute of Pharmaceutical\n",
      "        and Food Analysis and Technologies, Via Brigata Salerno, \n",
      "        16147 Genoa, Italy.\n",
      " \n",
      "    (b) Stefan Aeberhard, email: stefan@coral.cs.jcu.edu.au\n",
      "    (c) July 1991\n",
      " 3. Past Usage:\n",
      " \n",
      "    (1)\n",
      "    S. Aeberhard, D. Coomans and O. de Vel,\n",
      "    Comparison of Classifiers in High Dimensional Settings,\n",
      "    Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\n",
      "    Mathematics and Statistics, James Cook University of North Queensland.\n",
      "    (Also submitted to Technometrics).\n",
      " \n",
      "    The data was used with many others for comparing various \n",
      "    clasifiers. The clases are separable, though only RDA \n",
      "    has achieved 100% correct clasification.\n",
      "    (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\n",
      "    (All results using the leave-one-out technique)\n",
      " \n",
      "    In a clasification context, this is a well posed problem \n",
      "    with \"well behaved\" clas structures. A good data set \n",
      "    for first testing of a new clasifier, but not very \n",
      "    challenging.\n",
      " \n",
      "    (2) \n",
      "    S. Aeberhard, D. Coomans and O. de Vel,\n",
      "    \"THE CLASSIFICATION PERFORMANCE OF RDA\"\n",
      "    Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\n",
      "    Mathematics and Statistics, James Cook University of North Queensland.\n",
      "    (Also submitted to Journal of Chemometrics).\n",
      " \n",
      "    Here, the data was used to illustrate the superior performance of\n",
      "    the use of a new appreciation function with RDA. \n",
      " \n",
      " 4. Relevant Information:\n",
      " \n",
      "    -- These data are the results of a chemical analysis of\n",
      "       wines grown in the same region in Italy but derived from three\n",
      "       different cultivars.\n",
      "       The analysis determined the quantities of 13 constituents\n",
      "       found in each of the three types of wines. \n",
      " \n",
      "    -- I think that the initial data set had around 30 variables, but \n",
      "       for some reason I only have the 13 dimensional version. \n",
      "       I had a list of what the 30 or so variables were, but a.) \n",
      "       I lost it, and b.), I would not know which 13 variables\n",
      "       are included in the set.\n",
      " \n",
      "    -- The attributes are (dontated by Riccardo Leardi, \n",
      " \triclea@anchem.unige.it )\n",
      "  \t1) Alchl\n",
      "  \t2) Malic acid\n",
      "  \t3) Ash\n",
      " \t4) Alcalinity of ash  \n",
      "  \t5) Mgnsm\n",
      " \t6) Total phenols\n",
      "  \t7) Flavanoids\n",
      "  \t8) Nonflavanoid phenols\n",
      "  \t9) Proanthocyanins\n",
      " \t10)Color intensity\n",
      "  \t11)Hue\n",
      "  \t12)OD280/OD315 of diluted wines\n",
      "  \t13)Prln            \n",
      " \n",
      " 5. Number of Instances\n",
      " \n",
      "       \tclas 1 59\n",
      " \tclas 2 71\n",
      " \tclas 3 48\n",
      " \n",
      " 6. Number of Attributes \n",
      " \t\n",
      " \t13\n",
      " \n",
      " 7. For Each Attribute:\n",
      " \n",
      " \tAll attributes are continuous\n",
      " \t\n",
      " \tNo statistics available, but suggest to standardise\n",
      " \tvariables for certain uses (e.g. for us with clasifiers\n",
      " \twhich are NOT scale invariant)\n",
      " \n",
      " \tNOTE: 1st attribute is clas identifier (1-3)\n",
      " \n",
      " 8. Missing Attribute Values:\n",
      " \n",
      " \tNone\n",
      " \n",
      " 9. Class Distribution: number of instances per clas\n",
      " \n",
      "       \tclas 1 59\n",
      " \tclas 2 71\n",
      " \tclas 3 48\n",
      "\n",
      " Information about the dataset\n",
      " CLASSTYPE: nominal\n",
      " CLASSINDEX: first \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Mgnsm | TtlPhnls | Clr_inst | Alchl | Prln | clas stand for\n",
      "Abbreviated column names from a dataset: c_name | pCd | dt stand for Customer Name | Product Code | Date.\n",
      "From a dataset\n",
      "with title: wine \n",
      "with description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "1. Title of Database: Wine recognition data\n",
      " \tUpdated Sept 21, 1998 by C.Blake : Added attribute information\n",
      " \n",
      " 2. Sources:\n",
      "    (a) Forina, M. et al, PARVUS - An Extendible Package for Data\n",
      "        Exploration, Classification and Correlation. Institute of Pharmaceutical\n",
      "        and Food Analysis and Technologies, Via Brigata Salerno, \n",
      "        16147 Genoa, Italy.\n",
      " \n",
      "    (b) Stefan Aeberhard, email: stefan@coral.cs.jcu.edu.au\n",
      "    (c) July 1991\n",
      " 3. Past Usage:\n",
      " \n",
      "    (1)\n",
      "    S. Aeberhard, D. Coomans and O. de Vel,\n",
      "    Comparison of Classifiers in High Dimensional Settings,\n",
      "    Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\n",
      "    Mathematics and Statistics, James Cook University of North Queensland.\n",
      "    (Also submitted to Technometrics).\n",
      " \n",
      "    The data was used with many others for comparing various \n",
      "    clasifiers. The clases are separable, though only RDA \n",
      "    has achieved 100% correct clasification.\n",
      "    (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\n",
      "    (All results using the leave-one-out technique)\n",
      " \n",
      "    In a clasification context, this is a well posed problem \n",
      "    with \"well behaved\" clas structures. A good data set \n",
      "    for first testing of a new clasifier, but not very \n",
      "    challenging.\n",
      " \n",
      "    (2) \n",
      "    S. Aeberhard, D. Coomans and O. de Vel,\n",
      "    \"THE CLASSIFICATION PERFORMANCE OF RDA\"\n",
      "    Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\n",
      "    Mathematics and Statistics, James Cook University of North Queensland.\n",
      "    (Also submitted to Journal of Chemometrics).\n",
      " \n",
      "    Here, the data was used to illustrate the superior performance of\n",
      "    the use of a new appreciation function with RDA. \n",
      " \n",
      " 4. Relevant Information:\n",
      " \n",
      "    -- These data are the results of a chemical analysis of\n",
      "       wines grown in the same region in Italy but derived from three\n",
      "       different cultivars.\n",
      "       The analysis determined the quantities of 13 constituents\n",
      "       found in each of the three types of wines. \n",
      " \n",
      "    -- I think that the initial data set had around 30 variables, but \n",
      "       for some reason I only have the 13 dimensional version. \n",
      "       I had a list of what the 30 or so variables were, but a.) \n",
      "       I lost it, and b.), I would not know which 13 variables\n",
      "       are included in the set.\n",
      " \n",
      "    -- The attributes are (dontated by Riccardo Leardi, \n",
      " \triclea@anchem.unige.it )\n",
      "  \t1) Alchl\n",
      "  \t2) Malic acid\n",
      "  \t3) Ash\n",
      " \t4) Alcalinity of ash  \n",
      "  \t5) Mgnsm\n",
      " \t6) Total phenols\n",
      "  \t7) Flavanoids\n",
      "  \t8) Nonflavanoid phenols\n",
      "  \t9) Proanthocyanins\n",
      " \t10)Color intensity\n",
      "  \t11)Hue\n",
      "  \t12)OD280/OD315 of diluted wines\n",
      "  \t13)Prln            \n",
      " \n",
      " 5. Number of Instances\n",
      " \n",
      "       \tclas 1 59\n",
      " \tclas 2 71\n",
      " \tclas 3 48\n",
      " \n",
      " 6. Number of Attributes \n",
      " \t\n",
      " \t13\n",
      " \n",
      " 7. For Each Attribute:\n",
      " \n",
      " \tAll attributes are continuous\n",
      " \t\n",
      " \tNo statistics available, but suggest to standardise\n",
      " \tvariables for certain uses (e.g. for us with clasifiers\n",
      " \twhich are NOT scale invariant)\n",
      " \n",
      " \tNOTE: 1st attribute is clas identifier (1-3)\n",
      " \n",
      " 8. Missing Attribute Values:\n",
      " \n",
      " \tNone\n",
      " \n",
      " 9. Class Distribution: number of instances per clas\n",
      " \n",
      "       \tclas 1 59\n",
      " \tclas 2 71\n",
      " \tclas 3 48\n",
      "\n",
      " Information about the dataset\n",
      " CLASSTYPE: nominal\n",
      " CLASSINDEX: first \n",
      "\n",
      "        ,\n",
      "the abbreviated column names: Mgnsm | TtlPhnls | Clr_inst | Alchl | Prln | clas stand for\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pred_df_1': [                    y            X    y_pred_baseline       y_pred_title  \\\n",
       "  0     engine-location  engine-Lctn    Engine Location    Engine Location   \n",
       "  1              stroke         strk             Stroke             Stroke   \n",
       "  2         engine-type    engi-Type        Engine Type        Engine Type   \n",
       "  3          body-style    body-styl         Body Style         Body Style   \n",
       "  4        drive-wheels    drve-whls       Drive Wheels       Drive Wheels   \n",
       "  5            peak-rpm     peak-Rpm           Peak RPM           Peak RPM   \n",
       "  6          aspiration         aspn         Aspiration         Aspiration   \n",
       "  7         engine-size    engn-Size        Engine Size        Engine Size   \n",
       "  8               width         wdth              Width              Width   \n",
       "  9   compression-ratio    compr-rat  Compression Ratio  Compression Ratio   \n",
       "  10          fuel-type    fuel-Type          Fuel Type          Fuel Type   \n",
       "  11         wheel-base     whl-Base         Wheel Base         Wheel Base   \n",
       "  12             length         lnth             Length             Length   \n",
       "  13         horsepower       hrspwr         Horsepower         Horsepower   \n",
       "  14             height         heig             Height             Height   \n",
       "  15              price         pric              Price              Price   \n",
       "  16        fuel-system    fuel-Syst        Fuel System        Fuel System   \n",
       "  17        curb-weight  curb-Weight        Curb Weight        Curb Weight   \n",
       "  \n",
       "            y_pred_desc  y_pred_title_desc  \n",
       "  0     Engine Location    Engine Location  \n",
       "  1              Stroke             Stroke  \n",
       "  2         Engine Type        Engine Type  \n",
       "  3          Body Style         Body Style  \n",
       "  4        Drive Wheels       Drive Wheels  \n",
       "  5            Peak RPM           Peak RPM  \n",
       "  6          Aspiration         Aspiration  \n",
       "  7         Engine Size        Engine Size  \n",
       "  8               Width              Width  \n",
       "  9   Compression Ratio  Compression Ratio  \n",
       "  10          Fuel Type          Fuel Type  \n",
       "  11         Wheel Base         Wheel Base  \n",
       "  12             Length             Length  \n",
       "  13         Horsepower         Horsepower  \n",
       "  14             Height             Height  \n",
       "  15              Price              Price  \n",
       "  16        Fuel System        Fuel System  \n",
       "  17        Curb Weight        Curb Weight  ,\n",
       "  {'token_length': [105, 112, 1098, 1103]}],\n",
       " 'pred_df_2': [                           y               X           y_pred_baseline  \\\n",
       "  0             decision_count        dcsn_cnt            Decision Count   \n",
       "  1                 call_pairs       callpairs                Call Pairs   \n",
       "  2            unique_operands        unqeoper          Unique Operators   \n",
       "  3               branch_count        brnchCnt              Branch Count   \n",
       "  4                    defects            dfcs          Decision Factors   \n",
       "  5           decision_density        dcsnDnsy          Decision Density   \n",
       "  6             total_operands         ttloper           Total Operators   \n",
       "  7   multiple_condition_count  mlpl_cndtn_cnt  Multiple Condition Count   \n",
       "  8            condition_count  condition_cont           Condition Count   \n",
       "  9          design_complexity       dsgn_cmlt         Design Complexity   \n",
       "  10         formal_parameters        frmlPmts           Formal Payments   \n",
       "  11           total_operators        ttalOprr              Total Errors   \n",
       "  12          unique_operators     unqe_oprtrs          Unique Operators   \n",
       "  13            design_density     dsgndensity            Design Density   \n",
       "  \n",
       "                  y_pred_title               y_pred_desc  \\\n",
       "  0             Decision Count            Decision Count   \n",
       "  1                 Call Pairs                Call Pairs   \n",
       "  2           Unique Operators          Unique Operators   \n",
       "  3               Branch Count              Branch Count   \n",
       "  4           Decision Factors                   Defects   \n",
       "  5           Decision Density          Decision Density   \n",
       "  6            Total Operators           Total Operators   \n",
       "  7   Multiple Condition Count  Multiple Condition Count   \n",
       "  8            Condition Count           Condition Count   \n",
       "  9          Design Complexity         Design Complexity   \n",
       "  10           Formal Payments         Formal Parameters   \n",
       "  11              Total Errors            Total Operands   \n",
       "  12          Unique Operators          Unique Operators   \n",
       "  13            Design Density            Design Density   \n",
       "  \n",
       "             y_pred_title_desc  \n",
       "  0             Decision Count  \n",
       "  1                 Call Pairs  \n",
       "  2           Unique Operators  \n",
       "  3               Branch Count  \n",
       "  4                    Defects  \n",
       "  5           Decision Density  \n",
       "  6            Total Operators  \n",
       "  7   Multiple Condition Count  \n",
       "  8            Condition Count  \n",
       "  9          Design Complexity  \n",
       "  10         Formal Parameters  \n",
       "  11            Total Operands  \n",
       "  12          Unique Operators  \n",
       "  13            Design Density  ,\n",
       "  {'token_length': [105, 113, 409, 415]}],\n",
       " 'pred_df_3': [                   y           X                          y_pred_baseline  \\\n",
       "  0    thyroid_surgery    thrdSrgr                            Third Surgeon   \n",
       "  1              tumor        tumr                                    Tumor   \n",
       "  2  query_hypothyroid  queryhypot                        Query Hypotension   \n",
       "  3           pregnant      prgnnt                                 Pregnant   \n",
       "  4       TSH_measured     TSHmeas  Thyroid Stimulating Hormone Measurement   \n",
       "  5             goitre         gtr                            Glutaric Acid   \n",
       "  6            lithium        lith                                  Lithium   \n",
       "  7      hypopituitary      hpptry                                 Haplotry   \n",
       "  8    referral_source   rfrl_sour                          Referral Source   \n",
       "  9              class        clas                           Classification   \n",
       "  \n",
       "         y_pred_title       y_pred_desc y_pred_title_desc  \n",
       "  0     Third Surgery     Third Surgery     Third Surgeon  \n",
       "  1             Tumor             Tumor             Tumor  \n",
       "  2  Query Hypothesis  Query Hypothesis  Query Hypothesis  \n",
       "  3          Pregnant          Pregnant          Pregnant  \n",
       "  4   TSH Measurement   TSH Measurement   TSH Measurement  \n",
       "  5           Glitter     Glutaric Acid           Glitter  \n",
       "  6           Lithium           Lithium           Lithium  \n",
       "  7      Hippotherapy      Hippotherapy       Hypertrophy  \n",
       "  8   Referral Source   Referral Source   Referral Source  \n",
       "  9    Classification    Classification    Classification  ,\n",
       "  {'token_length': [74, 82, 85, 91]}],\n",
       " 'pred_df_4': [                           y                     X           y_pred_baseline  \\\n",
       "  0                  cap-shape              cap-shpe                 Cap Shape   \n",
       "  1   stalk-surface-below-ring    stlk-Srfc-Blw-Ring  Stalk Surface Below Ring   \n",
       "  2                 gill-color              gill-clr                Gill Color   \n",
       "  3          spore-print-color       spor-Prnt-Color         Spore Print Color   \n",
       "  4                  cap-color               cap-clr                 Cap Color   \n",
       "  5                ring-number             ring-numb               Ring Number   \n",
       "  6                 stalk-root             stal-root                Stalk Root   \n",
       "  7                  ring-type             ring-Type                 Ring Type   \n",
       "  8                      class                  clas                     Class   \n",
       "  9                stalk-shape             stlk-shpe               Stalk Shape   \n",
       "  10    stalk-color-above-ring    stlk-Colr-Abv-Ring    Stalk Color Above Ring   \n",
       "  11                population                 ppltn                Population   \n",
       "  12                 gill-size             gill-Size                 Gill Size   \n",
       "  13                   habitat                  hbtt                   Habitat   \n",
       "  14                 veil-type             veil-Type                 Veil Type   \n",
       "  15  stalk-surface-above-ring  stlk-srfc-above-ring  Stalk Surface Above Ring   \n",
       "  16              gill-spacing             gill-Spac              Gill Spacing   \n",
       "  17           gill-attachment             gill-Atta           Gill Attachment   \n",
       "  18    stalk-color-below-ring  stlk-color-belo-ring    Stalk Color Below Ring   \n",
       "  19                veil-color             veil-Colo                Veil Color   \n",
       "  20               cap-surface              cap-Surf               Cap Surface   \n",
       "  \n",
       "                  y_pred_title               y_pred_desc  \\\n",
       "  0                  Cap Shape                 Cap Shape   \n",
       "  1   Stalk Surface Below Ring  Stalk Surface Below Ring   \n",
       "  2                 Gill Color                Gill Color   \n",
       "  3          Spore Print Color         Spore Print Color   \n",
       "  4                  Cap Color                 Cap Color   \n",
       "  5                Ring Number               Ring Number   \n",
       "  6                 Stalk Root                Stalk Root   \n",
       "  7                  Ring Type                 Ring Type   \n",
       "  8                      Class                     Class   \n",
       "  9                Stalk Shape               Stalk Shape   \n",
       "  10    Stalk Color Above Ring    Stalk Color Above Ring   \n",
       "  11                Population                Population   \n",
       "  12                 Gill Size                 Gill Size   \n",
       "  13                   Habitat                   Habitat   \n",
       "  14                 Veil Type                 Veil Type   \n",
       "  15  Stalk Surface Above Ring  Stalk Surface Above Ring   \n",
       "  16              Gill Spacing              Gill Spacing   \n",
       "  17           Gill Attachment           Gill Attachment   \n",
       "  18    Stalk Color Below Ring    Stalk Color Below Ring   \n",
       "  19                Veil Color                Veil Color   \n",
       "  20               Cap Surface               Cap Surface   \n",
       "  \n",
       "             y_pred_title_desc  \n",
       "  0                  Cap Shape  \n",
       "  1   Stalk Surface Below Ring  \n",
       "  2                 Gill Color  \n",
       "  3          Spore Print Color  \n",
       "  4                  Cap Color  \n",
       "  5                Ring Number  \n",
       "  6                 Stalk Root  \n",
       "  7                  Ring Type  \n",
       "  8                      Class  \n",
       "  9                Stalk Shape  \n",
       "  10    Stalk Color Above Ring  \n",
       "  11                Population  \n",
       "  12                 Gill Size  \n",
       "  13                   Habitat  \n",
       "  14                 Veil Type  \n",
       "  15  Stalk Surface Above Ring  \n",
       "  16              Gill Spacing  \n",
       "  17           Gill Attachment  \n",
       "  18    Stalk Color Below Ring  \n",
       "  19                Veil Color  \n",
       "  20               Cap Surface  ,\n",
       "  {'token_length': [143, 150, 1278, 1283]}],\n",
       " 'pred_df_5': [                       y                 X       y_pred_baseline  \\\n",
       "  0    region-centroid-col   region-cntr-col   Region Center Color   \n",
       "  1         intensity-mean      intnsty-mean        Intensity Mean   \n",
       "  2               hue-mean          hue-Mean              Hue Mean   \n",
       "  3        saturation-mean         srtn-mean       Saturation Mean   \n",
       "  4   short-line-density-5  shor-line-dnty-5  Short Line Density 5   \n",
       "  5             hedge-mean         hdge-mean             Edge Mean   \n",
       "  6   short-line-density-2  shrt-Line-Dnst-2  Short Line Density 2   \n",
       "  7    region-centroid-row      rgn-cnrd-row   Region Centered Row   \n",
       "  8                  class              clss                 Class   \n",
       "  9             value-mean          val-Mean            Value Mean   \n",
       "  10          rawblue-mean         rwbl-mean         Row Blur Mean   \n",
       "  11         rawgreen-mean         rwgr-Mean        Row Green Mean   \n",
       "  12    region-pixel-count      rgn-pxl-cont    Region Pixel Count   \n",
       "  \n",
       "              y_pred_title           y_pred_desc     y_pred_title_desc  \n",
       "  0    Region Center Color  Region Center Column  Region Center Column  \n",
       "  1         Intensity Mean        Intensity Mean        Intensity Mean  \n",
       "  2               Hue Mean              Hue Mean              Hue Mean  \n",
       "  3        Saturation Mean       Saturation Mean       Saturation Mean  \n",
       "  4   Short Line Density 5  Short Line Density 5  Short Line Density 5  \n",
       "  5              Edge Mean            Hedge Mean            Hedge Mean  \n",
       "  6   Short Line Density 2  Short Line Density 2  Short Line Density 2  \n",
       "  7    Region Centered Row     Region Center Row     Region Center Row  \n",
       "  8                  Class                 Class                 Class  \n",
       "  9             Value Mean            Value Mean            Value Mean  \n",
       "  10         Row Blur Mean         Raw Blue Mean         Raw Blue Mean  \n",
       "  11        Row Green Mean        Raw Green Mean        Raw Green Mean  \n",
       "  12    Region Pixel Count    Region Pixel Count    Region Pixel Count  ,\n",
       "  {'token_length': [103, 110, 677, 682]}],\n",
       " 'pred_df_6': [                  y          X               y_pred_baseline     y_pred_title  \\\n",
       "  0             Class       Clss                Classification   Classification   \n",
       "  1         HISTOLOGY       HIST                       History        Histology   \n",
       "  2           ASCITES       ASCS                   Association          Ascites   \n",
       "  3        ANTIVIRALS       AIRA                        Airway    Alk Phosphate   \n",
       "  4           SPIDERS       SPER                         Sperm       Spermidine   \n",
       "  5         BILIRUBIN       BLUB                     Blueberry        Bilirubin   \n",
       "  6        LIVER_FIRM  LIVERFIRM                Liver Function       Liver Firm   \n",
       "  7           MALAISE       MAIS                         Maize          Malaise   \n",
       "  8           VARICES       VARI                      Variance          Varices   \n",
       "  9           ALBUMIN       ABIN                   Abnormality          Albumin   \n",
       "  10  SPLEEN_PALPABLE  SLEN_PLPL  Selenoprotein P-like protein  Spleen Palpable   \n",
       "  11          FATIGUE       FIGE                Fingerprinting            Figer   \n",
       "  12         ANOREXIA       ARXI                         Arxid          Ascites   \n",
       "  13          STEROID       SOID                          Soid     Spider Naevi   \n",
       "  \n",
       "                  y_pred_desc  y_pred_title_desc  \n",
       "  0                     Class              Class  \n",
       "  1                   History            History  \n",
       "  2                   Ascites            Ascites  \n",
       "  3                  Airborne           Airborne  \n",
       "  4                Spermidine         Spermidine  \n",
       "  5                 Bilirubin          Bilirubin  \n",
       "  6                Liver Firm         Liver Firm  \n",
       "  7         Malignant Ascites  Malignant Ascites  \n",
       "  8                   Varices            Varices  \n",
       "  9                   Albumin            Albumin  \n",
       "  10          Spleen Palpable    Spleen Palpable  \n",
       "  11               Fibrinogen         Fibrinogen  \n",
       "  12  Arterial Blood Pressure    Arterial Oxygen  \n",
       "  13            Sodium Iodide       Sodium Index  ,\n",
       "  {'token_length': [82, 89, 1524, 1529]}],\n",
       " 'pred_df_7': [                      y                 X       y_pred_baseline  \\\n",
       "  0                 Class              Clss                 Class   \n",
       "  1   bottom-right-square    botm-Rght-Squa   Bottom Right Square   \n",
       "  2    middle-left-square  middle-left-sqar    Middle Left Square   \n",
       "  3  middle-middle-square   mddl-middle-sqr  Middle Middle Square   \n",
       "  4   middle-right-square    mdle-Rght-Squr   Middle Right Square   \n",
       "  5     top-middle-square   top-Middle-Squa     Top Middle Square   \n",
       "  6    bottom-left-square     botm-left-sqr    Bottom Left Square   \n",
       "  7       top-left-square     top-Left-Sqre       Top Left Square   \n",
       "  8      top-right-square     top-rght-sqre      Top Right Square   \n",
       "  9  bottom-middle-square     bttm-midd-sqr  Bottom Middle Square   \n",
       "  \n",
       "             y_pred_title           y_pred_desc     y_pred_title_desc  \n",
       "  0                 Class                 Class                 Class  \n",
       "  1   Bottom Right Square   Bottom Right Square   Bottom Right Square  \n",
       "  2    Middle Left Square    Middle Left Square    Middle Left Square  \n",
       "  3  Middle Middle Square  Middle Middle Square  Middle Middle Square  \n",
       "  4   Middle Right Square   Middle Right Square   Middle Right Square  \n",
       "  5     Top Middle Square     Top Middle Square     Top Middle Square  \n",
       "  6    Bottom Left Square    Bottom Left Square    Bottom Left Square  \n",
       "  7       Top Left Square       Top Left Square       Top Left Square  \n",
       "  8      Top Right Square      Top Right Square      Top Right Square  \n",
       "  9  Bottom Middle Square  Bottom Middle Square  Bottom Middle Square  ,\n",
       "  {'token_length': [101, 112, 433, 442]}],\n",
       " 'pred_df_8': [                       y               X       y_pred_baseline  \\\n",
       "  0              sulphates          slphts             Sulphates   \n",
       "  1    free_sulfur_dioxide  free_slfr_dxde   Free Sulfur Dioxide   \n",
       "  2              chlorides          chlrds             Chlorides   \n",
       "  3         residual_sugar         resisgr        Residual Sugar   \n",
       "  4                density           dnsty               Density   \n",
       "  5            citric_acid      citricAcid           Citric Acid   \n",
       "  6                alcohol           alchl               Alcohol   \n",
       "  7       volatile_acidity       vola_adty      Volatile Acidity   \n",
       "  8                  class            clss                 Class   \n",
       "  9   total_sulfur_dioxide     totaSlfrDxd  Total Sulfur Dioxide   \n",
       "  10         fixed_acidity      fixedAcdty         Fixed Acidity   \n",
       "  \n",
       "              y_pred_title           y_pred_desc     y_pred_title_desc  \n",
       "  0              Sulphates             Sulphates             Sulphates  \n",
       "  1    Free Sulfur Dioxide   Free Sulfur Dioxide   Free Sulfur Dioxide  \n",
       "  2              Chlorides             Chlorides             Chlorides  \n",
       "  3         Residual Sugar        Residual Sugar        Residual Sugar  \n",
       "  4                Density               Density               Density  \n",
       "  5            Citric Acid           Citric Acid           Citric Acid  \n",
       "  6                Alcohol               Alcohol               Alcohol  \n",
       "  7       Volatile Acidity      Volatile Acidity      Volatile Acidity  \n",
       "  8                  Class                 Class                 Class  \n",
       "  9   Total Sulfur Dioxide  Total Sulfur Dioxide  Total Sulfur Dioxide  \n",
       "  10         Fixed Acidity         Fixed Acidity         Fixed Acidity  ,\n",
       "  {'token_length': [86, 95, 98, 105]}],\n",
       " 'pred_df_9': [             y          X y_pred_baseline y_pred_title  y_pred_desc  \\\n",
       "  0        Looks        Lks           Likes        Likes        Looks   \n",
       "  1        Grade       Grde           Grade        Grade        Grade   \n",
       "  2  Urban/Rural  Urbn/Rral     Urban/Rural  Urban/Rural  Urban/Rural   \n",
       "  3       Grades       Grds          Grades       Grades       Grades   \n",
       "  4       Gender       Gndr          Gender       Gender       Gender   \n",
       "  5        Goals       Gals           Goals        Goals        Goals   \n",
       "  6       Sports       Spts          Sports       Sports       Sports   \n",
       "  7       School       Schl          School       School       School   \n",
       "  8        Money       Mony           Money        Money        Money   \n",
       "  \n",
       "    y_pred_title_desc  \n",
       "  0             Looks  \n",
       "  1             Grade  \n",
       "  2       Urban/Rural  \n",
       "  3            Grades  \n",
       "  4            Gender  \n",
       "  5             Goals  \n",
       "  6            Sports  \n",
       "  7            School  \n",
       "  8             Money  ,\n",
       "  {'token_length': [65, 73, 584, 590]}],\n",
       " 'pred_df_10': [                 y             X    y_pred_baseline       y_pred_title  \\\n",
       "  0         Position          Posi           Position           Position   \n",
       "  1       Strikeouts          Stri         Strikeouts         Strikeouts   \n",
       "  2            Walks          Wlks              Walks              Walks   \n",
       "  3     Games_played       Gmsplyd       Games Played       Games Played   \n",
       "  4     Hall_of_Fame    HallOfFame       Hall of Fame       Hall of Fame   \n",
       "  5          Doubles          Doub            Doubles            Doubles   \n",
       "  6          At_bats        AtBats            At Bats            At Bats   \n",
       "  7   Number_seasons     Nmbr_ssns  Number of Seasons  Number of Seasons   \n",
       "  8          Triples          Trls            Triples            Triples   \n",
       "  9  Batting_average  Batting_avrg    Batting Average    Batting Average   \n",
       "  \n",
       "           y_pred_desc  y_pred_title_desc  \n",
       "  0           Position           Position  \n",
       "  1            Strikes            Strikes  \n",
       "  2              Walks              Walks  \n",
       "  3       Games Played       Games Played  \n",
       "  4       Hall of Fame       Hall of Fame  \n",
       "  5            Doubles            Doubles  \n",
       "  6            At Bats            At Bats  \n",
       "  7  Number of Seasons  Number of Seasons  \n",
       "  8            Triples            Triples  \n",
       "  9    Batting Average    Batting Average  ,\n",
       "  {'token_length': [76, 83, 193, 198]}],\n",
       " 'pred_df_11': [                 y            X  y_pred_baseline     y_pred_title  \\\n",
       "  0      mediastinum        media            Media            Media   \n",
       "  1           pleura          plr   Platelet Count              PLR   \n",
       "  2      bone-marrow    bone-mrrw      Bone Marrow      Bone Marrow   \n",
       "  3            brain          brn            Brain            Brain   \n",
       "  4            liver          lvr            Liver            Liver   \n",
       "  5        abdominal         abdo          Abdomen          Abdomen   \n",
       "  6       peritoneum         peri       Peritoneum       Peritoneum   \n",
       "  7            class         clas   Classification   Classification   \n",
       "  8  histologic-type  hstlgc-type  Histologic Type  Histologic Type   \n",
       "  \n",
       "         y_pred_desc y_pred_title_desc  \n",
       "  0            Media             Media  \n",
       "  1             Peri              Peri  \n",
       "  2      Bone Marrow       Bone Marrow  \n",
       "  3            Brain             Brain  \n",
       "  4            Liver             Liver  \n",
       "  5          Abdomen           Abdomen  \n",
       "  6       Peritoneum        Peritoneum  \n",
       "  7            Class             Class  \n",
       "  8  Histologic Type   Histologic Type  ,\n",
       "  {'token_length': [65, 74, 2106, 2113]}],\n",
       " 'pred_df_12': [             y         X y_pred_baseline y_pred_title     y_pred_desc  \\\n",
       "  0  Attribute_1     Arbt1     Attribute 1  Attribute 1     Attribute 1   \n",
       "  1  Attribute_2     Attr2     Attribute 2  Attribute 2     Attribute 2   \n",
       "  2  Attribute_7    Atrt_7     Attribute 7  Attribute 7     Attribute 7   \n",
       "  3  Attribute_9    Attr_9     Attribute 9  Attribute 9     Attribute 9   \n",
       "  4  Attribute_8   Attrbt8     Attribute 8  Attribute 8     Attribute 8   \n",
       "  5  Attribute_3     Attr3     Attribute 3  Attribute 3     Attribute 3   \n",
       "  6  Attribute_5   Attrbt5     Attribute 5  Attribute 5     Attribute 5   \n",
       "  7        class      clas           Class        Class  Classification   \n",
       "  8  Attribute_4   Attrbt4     Attribute 4  Attribute 4     Attribute 4   \n",
       "  9  Attribute_6  Attrbt_6     Attribute 6  Attribute 6     Attribute 6   \n",
       "  \n",
       "    y_pred_title_desc  \n",
       "  0       Attribute 1  \n",
       "  1       Attribute 2  \n",
       "  2       Attribute 7  \n",
       "  3       Attribute 9  \n",
       "  4       Attribute 8  \n",
       "  5       Attribute 3  \n",
       "  6       Attribute 5  \n",
       "  7    Classification  \n",
       "  8       Attribute 4  \n",
       "  9       Attribute 6  ,\n",
       "  {'token_length': [75, 83, 322, 328]}],\n",
       " 'pred_df_13': [            y        X y_pred_baseline    y_pred_title        y_pred_desc  \\\n",
       "  0       Class     Clas           Class  Classification              Class   \n",
       "  1    Ask_Open  AskOpen        Ask Open        Ask Open  Ask Opening Price   \n",
       "  2    Bid_Open  BidOpen        Bid Open        Bid Open  Bid Opening Price   \n",
       "  3    Bid_High  BidHigh        Bid High        Bid High  Bid Highest Price   \n",
       "  4  Ask_Volume  AskVlme      Ask Volume      Ask Volume         Ask Volume   \n",
       "  5   Bid_Close  BidClos       Bid Close       Bid Close  Bid Closing Price   \n",
       "  6  Bid_Volume   BidVlm      Bid Volume      Bid Volume         Bid Volume   \n",
       "  7   Ask_Close  Ask_Cls       Ask Close       Ask Close  Ask Closing Price   \n",
       "  8    Ask_High  AskHigh        Ask High        Ask High  Ask Highest Price   \n",
       "  \n",
       "     y_pred_title_desc  \n",
       "  0              Class  \n",
       "  1  Ask Opening Price  \n",
       "  2  Bid Opening Price  \n",
       "  3     Bid High Price  \n",
       "  4         Ask Volume  \n",
       "  5  Bid Closing Price  \n",
       "  6         Bid Volume  \n",
       "  7  Ask Closing Price  \n",
       "  8     Ask High Price  ,\n",
       "  {'token_length': [68, 83, 503, 516]}],\n",
       " 'pred_df_14': [                                y                        X  \\\n",
       "  0                 total_eve_calls              totleveclls   \n",
       "  1                 total_day_calls               ttldaycall   \n",
       "  2                total_day_charge               ttlDayChrg   \n",
       "  3                       area_code                 areaCode   \n",
       "  4               total_night_calls             totaNighCals   \n",
       "  5                           class                     clss   \n",
       "  6                  account_length                accolngth   \n",
       "  7                           state                      stt   \n",
       "  8             total_night_minutes            ttl_nght_mnts   \n",
       "  9                    phone_number                 phonNmbr   \n",
       "  10             international_plan               internPlan   \n",
       "  11              total_day_minutes            totl_day_mnts   \n",
       "  12              total_eve_minutes               ttlEveMnts   \n",
       "  13  number_customer_service_calls  nmbr_customer_srvc_call   \n",
       "  14                voice_mail_plan             voicmailplan   \n",
       "  15             total_night_charge             ttalNighChrg   \n",
       "  16               total_eve_charge               ttlevechrg   \n",
       "  \n",
       "                       y_pred_baseline                      y_pred_title  \\\n",
       "  0                Total Evening Calls             Total Number of Calls   \n",
       "  1                    Total Day Calls                   Total Day Calls   \n",
       "  2                  Total Day Charges                 Total Day Charges   \n",
       "  3                          Area Code                         Area Code   \n",
       "  4                  Total Night Calls                 Total Night Calls   \n",
       "  5                              Class                             Class   \n",
       "  6                     Account Length                    Account Length   \n",
       "  7                              State                             State   \n",
       "  8                Total Night Minutes               Total Night Minutes   \n",
       "  9                       Phone Number                      Phone Number   \n",
       "  10                International Plan                International Plan   \n",
       "  11                 Total Day Minutes                 Total Day Minutes   \n",
       "  12             Total Evening Minutes             Total Evening Minutes   \n",
       "  13  Number of Customer Service Calls  Number of Customer Service Calls   \n",
       "  14                    Voicemail Plan                    Voicemail Plan   \n",
       "  15               Total Night Charges               Total Night Charges   \n",
       "  16             Total Evening Charges             Total Evening Charges   \n",
       "  \n",
       "                           y_pred_desc                 y_pred_title_desc  \n",
       "  0        Total Number of Level Calls             Total Number of Calls  \n",
       "  1                    Total Day Calls                   Total Day Calls  \n",
       "  2                  Total Day Charges                 Total Day Charges  \n",
       "  3                          Area Code                         Area Code  \n",
       "  4                  Total Night Calls                 Total Night Calls  \n",
       "  5                              Class                             Class  \n",
       "  6                     Account Length                    Account Length  \n",
       "  7                              State                             State  \n",
       "  8                Total Night Minutes               Total Night Minutes  \n",
       "  9                       Phone Number                      Phone Number  \n",
       "  10                International Plan                International Plan  \n",
       "  11                 Total Day Minutes                 Total Day Minutes  \n",
       "  12             Total Evening Minutes             Total Evening Minutes  \n",
       "  13  Number of Customer Service Calls  Number of Customer Service Calls  \n",
       "  14                    Voicemail Plan                    Voicemail Plan  \n",
       "  15               Total Night Charges               Total Night Charges  \n",
       "  16             Total Evening Charges             Total Evening Charges  ,\n",
       "  {'token_length': [120, 127, 269, 274]}],\n",
       " 'pred_df_15': [                            y                        X  \\\n",
       "  0   MAX.LENGTH_RECTANGULARITY  MAX.LENG_RECTANGULARITY   \n",
       "  1               HOLLOWS_RATIO                 HOWSRATI   \n",
       "  2                       Class                     Clas   \n",
       "  3               SCATTER_RATIO                SAERRATIO   \n",
       "  4        SKEWNESS_ABOUT_MAJOR         SKEW_ABOUT_MAJOR   \n",
       "  5       SCALED_VARIANCE_MAJOR            SCALVARIMAJOR   \n",
       "  6   SCALED_RADIUS_OF_GYRATION       SCLDRADIOFGYRATION   \n",
       "  7     MAX.LENGTH_ASPECT_RATIO      MAX.LENGASPECTRATIO   \n",
       "  8               ELONGATEDNESS                   ELONGA   \n",
       "  9        SKEWNESS_ABOUT_MINOR       SKEWNESSABOUTMINOR   \n",
       "  10      SCALED_VARIANCE_MINOR         SLEDVARIANCEMINO   \n",
       "  11               RADIUS_RATIO                 RADIRATO   \n",
       "  12       DISTANCE_CIRCULARITY           DISTANCE_CIRCU   \n",
       "  13                COMPACTNESS                    CPCTE   \n",
       "  14                CIRCULARITY                    CIRCU   \n",
       "  \n",
       "                    y_pred_baseline                   y_pred_title  \\\n",
       "  0   Maximum Length Rectangularity  Maximum Length Rectangularity   \n",
       "  1                       Hows Rati  Horizontal Width/Length Ratio   \n",
       "  2                           Class                          Class   \n",
       "  3                      Saer Ratio                 Saliency Ratio   \n",
       "  4                Skew About Major      Skewness About Major Axis   \n",
       "  5                 Scal Vari Major          Scalar Variance Major   \n",
       "  6            Scl Dradio Fgyration      Scalar Radius of Gyration   \n",
       "  7         Max Length Aspect Ratio    Maximum Length Aspect Ratio   \n",
       "  8                      Elongation                     Elongation   \n",
       "  9            Skewness About Minor      Skewness About Minor Axis   \n",
       "  10            Sled Variance Minor        Saliency Variance Minor   \n",
       "  11                      Radi Rato                   Radial Ratio   \n",
       "  12                 Distance Circu           Distance Circularity   \n",
       "  13                          Cpcte                    Compactness   \n",
       "  14                  Circumference                    Circularity   \n",
       "  \n",
       "                    y_pred_desc              y_pred_title_desc  \n",
       "  0   Max Length Rectangularity  Maximum Length Rectangularity  \n",
       "  1                   Hows Rati                  Scatter Ratio  \n",
       "  2                       Class                          Class  \n",
       "  3                   Sae Ratio                  Scatter Ratio  \n",
       "  4            Skew About Major           Skewness About Major  \n",
       "  5             Scal Vari Major          Scaled Variance Major  \n",
       "  6       Scl D Radiof Gyration      Scaled Radius of Gyration  \n",
       "  7       Max Leng Aspect Ratio        Max Length Aspect Ratio  \n",
       "  8                      Elonga                     Elongation  \n",
       "  9        Skewness About Minor           Skewness About Minor  \n",
       "  10         Sled Variance Mino          Scaled Variance Minor  \n",
       "  11                  Radi Rato                   Radius Ratio  \n",
       "  12             Distance Circu           Distance Circularity  \n",
       "  13                      Cpcte                          CPCTE  \n",
       "  14                      Circu                    Circularity  ,\n",
       "  {'token_length': [123, 130, 1828, 1833]}],\n",
       " 'pred_df_16': [                                 y                          X  \\\n",
       "  0               shift-differential          shif-differential   \n",
       "  1               statutory-holidays                stttry-Holi   \n",
       "  2                         vacation                       vctn   \n",
       "  3                         duration                       drtn   \n",
       "  4           bereavement-assistance             brvmnt-assstnc   \n",
       "  5                      standby-pay                   stan-pay   \n",
       "  6                            class                       clss   \n",
       "  7      contribution-to-dental-plan  contribution-to-dntl-plan   \n",
       "  8                          pension                       pnsn   \n",
       "  9      contribution-to-health-plan       cntrbtn-to-hlth-plan   \n",
       "  10  longterm-disability-assistance           long-Dsblty-Astn   \n",
       "  11       cost-of-living-adjustment          cost-Of-Livi-Adju   \n",
       "  12                   working-hours                   wkng-hrs   \n",
       "  13        wage-increase-third-year        wage-incs-thrd-year   \n",
       "  14             education-allowance                ectn-allwnc   \n",
       "  15        wage-increase-first-year        wage-Icrs-Frst-Year   \n",
       "  16       wage-increase-second-year       wage-Incrs-Seco-Year   \n",
       "  \n",
       "                  y_pred_baseline                 y_pred_title  \\\n",
       "  0            Shift Differential           Shift Differential   \n",
       "  1             Statutory Holiday            Statutory Holiday   \n",
       "  2                      Vacation                     Vacation   \n",
       "  3                      Duration                     Duration   \n",
       "  4        Bereavement Assistance       Bereavement Assistance   \n",
       "  5                   Standby Pay                Statutory Pay   \n",
       "  6                         Class                        Class   \n",
       "  7   Contribution to Dental Plan  Contribution to Dental Plan   \n",
       "  8                       Pension                      Pension   \n",
       "  9   Contribution to Health Plan  Contribution to Health Plan   \n",
       "  10        Long Disability Aston   Long Disability Assistance   \n",
       "  11    Cost of Living Adjustment    Cost of Living Adjustment   \n",
       "  12                Working Hours                Working Hours   \n",
       "  13    Wage Increases Third Year    Wage Increases Third Year   \n",
       "  14          Education Allowance          Education Allowance   \n",
       "  15    Wage Increases First Year    Wage Increases First Year   \n",
       "  16   Wage Increases Second Year   Wage Increases Second Year   \n",
       "  \n",
       "                          y_pred_desc                y_pred_title_desc  \n",
       "  0                Shift Differential               Shift Differential  \n",
       "  1                Statutory Holidays               Statutory Holidays  \n",
       "  2                          Vacation                         Vacation  \n",
       "  3                          Duration                         Duration  \n",
       "  4            Bereavement Assistance           Bereavement Assistance  \n",
       "  5                       Standby Pay                      Standby Pay  \n",
       "  6                             Class                            Class  \n",
       "  7       Contribution to Dental Plan      Contribution to Dental Plan  \n",
       "  8                           Pension                          Pension  \n",
       "  9       Contribution to Health Plan      Contribution to Health Plan  \n",
       "  10  Long-Term Disability Assistance  Long-Term Disability Assistance  \n",
       "  11        Cost of Living Adjustment        Cost of Living Adjustment  \n",
       "  12                    Working Hours                    Working Hours  \n",
       "  13         Wage Increase Third Year        Wage Increases Third Year  \n",
       "  14              Education Allowance              Education Allowance  \n",
       "  15         Wage Increase First Year        Wage Increases First Year  \n",
       "  16        Wage Increase Second Year       Wage Increases Second Year  ,\n",
       "  {'token_length': [138, 145, 1053, 1058]}],\n",
       " 'pred_df_17': [                 y           X      y_pred_baseline           y_pred_title  \\\n",
       "  0  exclusion_of_no  excl_of_no  Exclusion of Number     Exclusion of Nodes   \n",
       "  1    special_forms    spclfrms        Special Forms          Special Forms   \n",
       "  2   dislocation_of    dslctnof       Dislocation of   Dislocation of Nodes   \n",
       "  3  regeneration_of    rgnrtnof      Regeneration of  Regeneration of Nodes   \n",
       "  4          by_pass      bypass               Bypass                 Bypass   \n",
       "  5            class        clas       Classification         Classification   \n",
       "  \n",
       "         y_pred_desc y_pred_title_desc  \n",
       "  0  Exclusion of No   Exclusion of No  \n",
       "  1    Special Forms     Special Forms  \n",
       "  2   Dislocation of    Dislocation of  \n",
       "  3  Regeneration of   Regeneration of  \n",
       "  4           Bypass            Bypass  \n",
       "  5            Class             Class  ,\n",
       "  {'token_length': [60, 67, 1792, 1797]}],\n",
       " 'pred_df_18': [                              y                   X  \\\n",
       "  0               Wifes_education       WifsEducation   \n",
       "  1  Number_of_children_ever_born  NumbOfChlrEverBorn   \n",
       "  2     Contraceptive_method_used      Contramthdused   \n",
       "  3      Standard-of-living_index  Sndr-Of-LivingIndx   \n",
       "  4                Wifes_religion             WfsReli   \n",
       "  5           Husbands_occupation          Hsbdoccptn   \n",
       "  6            Husbands_education          Hsbndsedtn   \n",
       "  7                Media_exposure          Media_expr   \n",
       "  8                     Wifes_age              Wfsage   \n",
       "  \n",
       "                  y_pred_baseline                  y_pred_title  \\\n",
       "  0              Wife's Education              Wife's Education   \n",
       "  1  Number of Children Ever Born  Number of Children Ever Born   \n",
       "  2     Contraceptive Method Used     Contraceptive Method Used   \n",
       "  3      Standard of Living Index      Standard of Living Index   \n",
       "  4               Wife's Religion               Wife's Religion   \n",
       "  5          Husband's Occupation          Husband's Occupation   \n",
       "  6           Husband's Education           Husband's Education   \n",
       "  7                Media Exposure                Media Exposure   \n",
       "  8                    Wife's Age                    Wife's Age   \n",
       "  \n",
       "                      y_pred_desc             y_pred_title_desc  \n",
       "  0              Wife's Education              Wife's Education  \n",
       "  1  Number of Children Ever Born  Number of Children Ever Born  \n",
       "  2     Contraceptive Method Used     Contraceptive Method Used  \n",
       "  3      Standard-of-Living Index      Standard-of-Living Index  \n",
       "  4               Wife's Religion               Wife's Religion  \n",
       "  5          Husband's Occupation          Husband's Occupation  \n",
       "  6           Husband's Education           Husband's Education  \n",
       "  7                Media Exposure                Media Exposure  \n",
       "  8                    Wife's Age                    Wife's Age  ,\n",
       "  {'token_length': [88, 96, 781, 787]}],\n",
       " 'pred_df_19': [             y          X  y_pred_baseline     y_pred_title      y_pred_desc  \\\n",
       "  0        Class       Clss   Classification            Class            Class   \n",
       "  1    node-caps  node-Caps    Node Capsules    Node Capsules        Node Caps   \n",
       "  2   tumor-size  tmor-Size       Tumor Size       Tumor Size       Tumor Size   \n",
       "  3       breast       brst           Breast           Breast           Breast   \n",
       "  4    menopause       meno        Menopause        Menopause        Menopause   \n",
       "  5  breast-quad  brst-Quad  Breast Quadrant  Breast Quadrant  Breast Quadrant   \n",
       "  \n",
       "    y_pred_title_desc  \n",
       "  0             Class  \n",
       "  1         Node Caps  \n",
       "  2        Tumor Size  \n",
       "  3            Breast  \n",
       "  4         Menopause  \n",
       "  5   Breast Quadrant  ,\n",
       "  {'token_length': [58, 67, 1547, 1554]}],\n",
       " 'pred_df_20': [                 y         X  y_pred_baseline     y_pred_title  \\\n",
       "  0        Magnesium     Mgnsm        Magnesium        Magnesium   \n",
       "  1    Total_phenols  TtlPhnls    Total Phenols    Total Phenols   \n",
       "  2  Color_intensity  Clr_inst  Color Intensity  Color Intensity   \n",
       "  3          Alcohol     Alchl          Alcohol          Alcohol   \n",
       "  4          Proline      Prln          Proline          Proline   \n",
       "  5            class      clas            Class            Class   \n",
       "  \n",
       "         y_pred_desc y_pred_title_desc  \n",
       "  0        Magnesium         Magnesium  \n",
       "  1    Total Phenols     Total Phenols  \n",
       "  2  Color Intensity   Color Intensity  \n",
       "  3          Alcohol           Alcohol  \n",
       "  4  Proanthocyanins   Proanthocyanins  \n",
       "  5            Class             Class  ,\n",
       "  {'token_length': [58, 65, 982, 987]}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code generates all predictions for experiment 2 for all query types\n",
    "num = 1\n",
    "pred_dct_title_desc = dict() #comment out if loaded part of preds in from pkl file\n",
    "token_length_base = []\n",
    "token_length_title = []\n",
    "token_length_desc = []\n",
    "token_length_title_desc = []\n",
    "\n",
    "for did in list_dids:\n",
    "    dataset = openml.datasets.get_dataset(did)\n",
    "    title = dataset.name\n",
    "    description = dataset.description\n",
    "    df, _, _, _ = dataset.get_data(dataset_format=\"dataframe\")\n",
    "    # Find the non-cryptic columns and convert them to cryptic columns (so we have X and y)\n",
    "    y = detect_non_cryptic(df)\n",
    "    X = generate_cryptic(y)\n",
    "\n",
    "    # Give the cryptic names as input to GPT combined with an n number of instances and ask\n",
    "    # it to generate non-cryptic names\n",
    "    gpt_predictions_baseline, t_base = generate_non_cryptic(df, y, X, 0)\n",
    "    gpt_predictions_title, t_title = generate_non_cryptic(df, y, X, 0, title=title)\n",
    "    gpt_predictions_desc, t_desc = generate_non_cryptic(df, y, X, 0, description=description)\n",
    "    gpt_predictions_title_desc, t_title_desc = generate_non_cryptic(df, y, X, 0, title, description)\n",
    "\n",
    "    if (len(y) != len(gpt_predictions_baseline)) or (len(y) != len(gpt_predictions_title)) or (len(y) != len(gpt_predictions_desc)) or (len(y) != len(gpt_predictions_title_desc)):\n",
    "        print(f\"Something went wrong with the predictions in dataset number {num}\")\n",
    "        print(\"The number of predictions do not align with the number of ground truth values\")\n",
    "        print(f\"Length y: {len(y)}, y_pred_baseline: {len(gpt_predictions_baseline)}, y_pred_title: {len(gpt_predictions_title)}, y_pred_desc: {len(gpt_predictions_desc)}, y_pred_title_desc: {len(gpt_predictions_title_desc)}\")\n",
    "        print(\"We will not add this dataset to the predictions\")\n",
    "        continue\n",
    "    else:\n",
    "        # Store the predictions in a dataframe\n",
    "        df_predictions = pd.DataFrame()\n",
    "        df_predictions['y'] = y\n",
    "        df_predictions['X'] = X\n",
    "        df_predictions['y_pred_baseline'] = gpt_predictions_baseline\n",
    "        df_predictions[f'y_pred_title'] = gpt_predictions_title\n",
    "        df_predictions[f'y_pred_desc'] = gpt_predictions_desc\n",
    "        df_predictions[f'y_pred_title_desc'] = gpt_predictions_title_desc\n",
    "\n",
    "        # To compute the bertscore F1 in the end, we will divide by the average token length for every query\n",
    "        token_length_base.append(t_base)\n",
    "        token_length_title.append(t_title)\n",
    "        token_length_desc.append(t_desc)\n",
    "        token_length_title_desc.append(t_title_desc)\n",
    "\n",
    "        # Store the predictions with the token lengths in a dictionary, which we can use to compute the scores\n",
    "        token_lengths = {\"token_length\" : [t_base, t_title, t_desc, t_title_desc]}\n",
    "        pred_dct_title_desc[f\"pred_df_{num}\"] = [df_predictions, token_lengths]\n",
    "    num += 1\n",
    "pred_dct_title_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e50c3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preditions to a pickle file\n",
    "with open('data/complete_preds_experiment_2.pkl', 'wb') as handle:\n",
    "    pickle.dump(pred_dct_title_desc, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ae6d6448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_df_1': [                    y            X    y_pred_baseline       y_pred_title  \\\n",
       "  0     engine-location  engine-Lctn    Engine Location    Engine Location   \n",
       "  1              stroke         strk             Stroke             Stroke   \n",
       "  2         engine-type    engi-Type        Engine Type        Engine Type   \n",
       "  3          body-style    body-styl         Body Style         Body Style   \n",
       "  4        drive-wheels    drve-whls       Drive Wheels       Drive Wheels   \n",
       "  5            peak-rpm     peak-Rpm           Peak RPM           Peak RPM   \n",
       "  6          aspiration         aspn         Aspiration         Aspiration   \n",
       "  7         engine-size    engn-Size        Engine Size        Engine Size   \n",
       "  8               width         wdth              Width              Width   \n",
       "  9   compression-ratio    compr-rat  Compression Ratio  Compression Ratio   \n",
       "  10          fuel-type    fuel-Type          Fuel Type          Fuel Type   \n",
       "  11         wheel-base     whl-Base         Wheel Base         Wheel Base   \n",
       "  12             length         lnth             Length             Length   \n",
       "  13         horsepower       hrspwr         Horsepower         Horsepower   \n",
       "  14             height         heig             Height             Height   \n",
       "  15              price         pric              Price              Price   \n",
       "  16        fuel-system    fuel-Syst        Fuel System        Fuel System   \n",
       "  17        curb-weight  curb-Weight        Curb Weight        Curb Weight   \n",
       "  \n",
       "            y_pred_desc  y_pred_title_desc  \n",
       "  0     Engine Location    Engine Location  \n",
       "  1              Stroke             Stroke  \n",
       "  2         Engine Type        Engine Type  \n",
       "  3          Body Style         Body Style  \n",
       "  4        Drive Wheels       Drive Wheels  \n",
       "  5            Peak RPM           Peak RPM  \n",
       "  6          Aspiration         Aspiration  \n",
       "  7         Engine Size        Engine Size  \n",
       "  8               Width              Width  \n",
       "  9   Compression Ratio  Compression Ratio  \n",
       "  10          Fuel Type          Fuel Type  \n",
       "  11         Wheel Base         Wheel Base  \n",
       "  12             Length             Length  \n",
       "  13         Horsepower         Horsepower  \n",
       "  14             Height             Height  \n",
       "  15              Price              Price  \n",
       "  16        Fuel System        Fuel System  \n",
       "  17        Curb Weight        Curb Weight  ,\n",
       "  {'token_length': [105, 112, 1098, 1103]}],\n",
       " 'pred_df_2': [                           y               X           y_pred_baseline  \\\n",
       "  0             decision_count        dcsn_cnt            Decision Count   \n",
       "  1                 call_pairs       callpairs                Call Pairs   \n",
       "  2            unique_operands        unqeoper          Unique Operators   \n",
       "  3               branch_count        brnchCnt              Branch Count   \n",
       "  4                    defects            dfcs          Decision Factors   \n",
       "  5           decision_density        dcsnDnsy          Decision Density   \n",
       "  6             total_operands         ttloper           Total Operators   \n",
       "  7   multiple_condition_count  mlpl_cndtn_cnt  Multiple Condition Count   \n",
       "  8            condition_count  condition_cont           Condition Count   \n",
       "  9          design_complexity       dsgn_cmlt         Design Complexity   \n",
       "  10         formal_parameters        frmlPmts           Formal Payments   \n",
       "  11           total_operators        ttalOprr              Total Errors   \n",
       "  12          unique_operators     unqe_oprtrs          Unique Operators   \n",
       "  13            design_density     dsgndensity            Design Density   \n",
       "  \n",
       "                  y_pred_title               y_pred_desc  \\\n",
       "  0             Decision Count            Decision Count   \n",
       "  1                 Call Pairs                Call Pairs   \n",
       "  2           Unique Operators          Unique Operators   \n",
       "  3               Branch Count              Branch Count   \n",
       "  4           Decision Factors                   Defects   \n",
       "  5           Decision Density          Decision Density   \n",
       "  6            Total Operators           Total Operators   \n",
       "  7   Multiple Condition Count  Multiple Condition Count   \n",
       "  8            Condition Count           Condition Count   \n",
       "  9          Design Complexity         Design Complexity   \n",
       "  10           Formal Payments         Formal Parameters   \n",
       "  11              Total Errors            Total Operands   \n",
       "  12          Unique Operators          Unique Operators   \n",
       "  13            Design Density            Design Density   \n",
       "  \n",
       "             y_pred_title_desc  \n",
       "  0             Decision Count  \n",
       "  1                 Call Pairs  \n",
       "  2           Unique Operators  \n",
       "  3               Branch Count  \n",
       "  4                    Defects  \n",
       "  5           Decision Density  \n",
       "  6            Total Operators  \n",
       "  7   Multiple Condition Count  \n",
       "  8            Condition Count  \n",
       "  9          Design Complexity  \n",
       "  10         Formal Parameters  \n",
       "  11            Total Operands  \n",
       "  12          Unique Operators  \n",
       "  13            Design Density  ,\n",
       "  {'token_length': [105, 113, 409, 415]}],\n",
       " 'pred_df_3': [                   y           X                          y_pred_baseline  \\\n",
       "  0    thyroid_surgery    thrdSrgr                            Third Surgeon   \n",
       "  1              tumor        tumr                                    Tumor   \n",
       "  2  query_hypothyroid  queryhypot                        Query Hypotension   \n",
       "  3           pregnant      prgnnt                                 Pregnant   \n",
       "  4       TSH_measured     TSHmeas  Thyroid Stimulating Hormone Measurement   \n",
       "  5             goitre         gtr                            Glutaric Acid   \n",
       "  6            lithium        lith                                  Lithium   \n",
       "  7      hypopituitary      hpptry                                 Haplotry   \n",
       "  8    referral_source   rfrl_sour                          Referral Source   \n",
       "  9              class        clas                           Classification   \n",
       "  \n",
       "         y_pred_title       y_pred_desc y_pred_title_desc  \n",
       "  0     Third Surgery     Third Surgery     Third Surgeon  \n",
       "  1             Tumor             Tumor             Tumor  \n",
       "  2  Query Hypothesis  Query Hypothesis  Query Hypothesis  \n",
       "  3          Pregnant          Pregnant          Pregnant  \n",
       "  4   TSH Measurement   TSH Measurement   TSH Measurement  \n",
       "  5           Glitter     Glutaric Acid           Glitter  \n",
       "  6           Lithium           Lithium           Lithium  \n",
       "  7      Hippotherapy      Hippotherapy       Hypertrophy  \n",
       "  8   Referral Source   Referral Source   Referral Source  \n",
       "  9    Classification    Classification    Classification  ,\n",
       "  {'token_length': [74, 82, 85, 91]}],\n",
       " 'pred_df_4': [                           y                     X           y_pred_baseline  \\\n",
       "  0                  cap-shape              cap-shpe                 Cap Shape   \n",
       "  1   stalk-surface-below-ring    stlk-Srfc-Blw-Ring  Stalk Surface Below Ring   \n",
       "  2                 gill-color              gill-clr                Gill Color   \n",
       "  3          spore-print-color       spor-Prnt-Color         Spore Print Color   \n",
       "  4                  cap-color               cap-clr                 Cap Color   \n",
       "  5                ring-number             ring-numb               Ring Number   \n",
       "  6                 stalk-root             stal-root                Stalk Root   \n",
       "  7                  ring-type             ring-Type                 Ring Type   \n",
       "  8                      class                  clas                     Class   \n",
       "  9                stalk-shape             stlk-shpe               Stalk Shape   \n",
       "  10    stalk-color-above-ring    stlk-Colr-Abv-Ring    Stalk Color Above Ring   \n",
       "  11                population                 ppltn                Population   \n",
       "  12                 gill-size             gill-Size                 Gill Size   \n",
       "  13                   habitat                  hbtt                   Habitat   \n",
       "  14                 veil-type             veil-Type                 Veil Type   \n",
       "  15  stalk-surface-above-ring  stlk-srfc-above-ring  Stalk Surface Above Ring   \n",
       "  16              gill-spacing             gill-Spac              Gill Spacing   \n",
       "  17           gill-attachment             gill-Atta           Gill Attachment   \n",
       "  18    stalk-color-below-ring  stlk-color-belo-ring    Stalk Color Below Ring   \n",
       "  19                veil-color             veil-Colo                Veil Color   \n",
       "  20               cap-surface              cap-Surf               Cap Surface   \n",
       "  \n",
       "                  y_pred_title               y_pred_desc  \\\n",
       "  0                  Cap Shape                 Cap Shape   \n",
       "  1   Stalk Surface Below Ring  Stalk Surface Below Ring   \n",
       "  2                 Gill Color                Gill Color   \n",
       "  3          Spore Print Color         Spore Print Color   \n",
       "  4                  Cap Color                 Cap Color   \n",
       "  5                Ring Number               Ring Number   \n",
       "  6                 Stalk Root                Stalk Root   \n",
       "  7                  Ring Type                 Ring Type   \n",
       "  8                      Class                     Class   \n",
       "  9                Stalk Shape               Stalk Shape   \n",
       "  10    Stalk Color Above Ring    Stalk Color Above Ring   \n",
       "  11                Population                Population   \n",
       "  12                 Gill Size                 Gill Size   \n",
       "  13                   Habitat                   Habitat   \n",
       "  14                 Veil Type                 Veil Type   \n",
       "  15  Stalk Surface Above Ring  Stalk Surface Above Ring   \n",
       "  16              Gill Spacing              Gill Spacing   \n",
       "  17           Gill Attachment           Gill Attachment   \n",
       "  18    Stalk Color Below Ring    Stalk Color Below Ring   \n",
       "  19                Veil Color                Veil Color   \n",
       "  20               Cap Surface               Cap Surface   \n",
       "  \n",
       "             y_pred_title_desc  \n",
       "  0                  Cap Shape  \n",
       "  1   Stalk Surface Below Ring  \n",
       "  2                 Gill Color  \n",
       "  3          Spore Print Color  \n",
       "  4                  Cap Color  \n",
       "  5                Ring Number  \n",
       "  6                 Stalk Root  \n",
       "  7                  Ring Type  \n",
       "  8                      Class  \n",
       "  9                Stalk Shape  \n",
       "  10    Stalk Color Above Ring  \n",
       "  11                Population  \n",
       "  12                 Gill Size  \n",
       "  13                   Habitat  \n",
       "  14                 Veil Type  \n",
       "  15  Stalk Surface Above Ring  \n",
       "  16              Gill Spacing  \n",
       "  17           Gill Attachment  \n",
       "  18    Stalk Color Below Ring  \n",
       "  19                Veil Color  \n",
       "  20               Cap Surface  ,\n",
       "  {'token_length': [143, 150, 1278, 1283]}],\n",
       " 'pred_df_5': [                       y                 X       y_pred_baseline  \\\n",
       "  0    region-centroid-col   region-cntr-col   Region Center Color   \n",
       "  1         intensity-mean      intnsty-mean        Intensity Mean   \n",
       "  2               hue-mean          hue-Mean              Hue Mean   \n",
       "  3        saturation-mean         srtn-mean       Saturation Mean   \n",
       "  4   short-line-density-5  shor-line-dnty-5  Short Line Density 5   \n",
       "  5             hedge-mean         hdge-mean             Edge Mean   \n",
       "  6   short-line-density-2  shrt-Line-Dnst-2  Short Line Density 2   \n",
       "  7    region-centroid-row      rgn-cnrd-row   Region Centered Row   \n",
       "  8                  class              clss                 Class   \n",
       "  9             value-mean          val-Mean            Value Mean   \n",
       "  10          rawblue-mean         rwbl-mean         Row Blur Mean   \n",
       "  11         rawgreen-mean         rwgr-Mean        Row Green Mean   \n",
       "  12    region-pixel-count      rgn-pxl-cont    Region Pixel Count   \n",
       "  \n",
       "              y_pred_title           y_pred_desc     y_pred_title_desc  \n",
       "  0    Region Center Color  Region Center Column  Region Center Column  \n",
       "  1         Intensity Mean        Intensity Mean        Intensity Mean  \n",
       "  2               Hue Mean              Hue Mean              Hue Mean  \n",
       "  3        Saturation Mean       Saturation Mean       Saturation Mean  \n",
       "  4   Short Line Density 5  Short Line Density 5  Short Line Density 5  \n",
       "  5              Edge Mean            Hedge Mean            Hedge Mean  \n",
       "  6   Short Line Density 2  Short Line Density 2  Short Line Density 2  \n",
       "  7    Region Centered Row     Region Center Row     Region Center Row  \n",
       "  8                  Class                 Class                 Class  \n",
       "  9             Value Mean            Value Mean            Value Mean  \n",
       "  10         Row Blur Mean         Raw Blue Mean         Raw Blue Mean  \n",
       "  11        Row Green Mean        Raw Green Mean        Raw Green Mean  \n",
       "  12    Region Pixel Count    Region Pixel Count    Region Pixel Count  ,\n",
       "  {'token_length': [103, 110, 677, 682]}],\n",
       " 'pred_df_6': [                  y          X               y_pred_baseline     y_pred_title  \\\n",
       "  0             Class       Clss                Classification   Classification   \n",
       "  1         HISTOLOGY       HIST                       History        Histology   \n",
       "  2           ASCITES       ASCS                   Association          Ascites   \n",
       "  3        ANTIVIRALS       AIRA                        Airway    Alk Phosphate   \n",
       "  4           SPIDERS       SPER                         Sperm       Spermidine   \n",
       "  5         BILIRUBIN       BLUB                     Blueberry        Bilirubin   \n",
       "  6        LIVER_FIRM  LIVERFIRM                Liver Function       Liver Firm   \n",
       "  7           MALAISE       MAIS                         Maize          Malaise   \n",
       "  8           VARICES       VARI                      Variance          Varices   \n",
       "  9           ALBUMIN       ABIN                   Abnormality          Albumin   \n",
       "  10  SPLEEN_PALPABLE  SLEN_PLPL  Selenoprotein P-like protein  Spleen Palpable   \n",
       "  11          FATIGUE       FIGE                Fingerprinting            Figer   \n",
       "  12         ANOREXIA       ARXI                         Arxid          Ascites   \n",
       "  13          STEROID       SOID                          Soid     Spider Naevi   \n",
       "  \n",
       "                  y_pred_desc  y_pred_title_desc  \n",
       "  0                     Class              Class  \n",
       "  1                   History            History  \n",
       "  2                   Ascites            Ascites  \n",
       "  3                  Airborne           Airborne  \n",
       "  4                Spermidine         Spermidine  \n",
       "  5                 Bilirubin          Bilirubin  \n",
       "  6                Liver Firm         Liver Firm  \n",
       "  7         Malignant Ascites  Malignant Ascites  \n",
       "  8                   Varices            Varices  \n",
       "  9                   Albumin            Albumin  \n",
       "  10          Spleen Palpable    Spleen Palpable  \n",
       "  11               Fibrinogen         Fibrinogen  \n",
       "  12  Arterial Blood Pressure    Arterial Oxygen  \n",
       "  13            Sodium Iodide       Sodium Index  ,\n",
       "  {'token_length': [82, 89, 1524, 1529]}],\n",
       " 'pred_df_7': [                      y                 X       y_pred_baseline  \\\n",
       "  0                 Class              Clss                 Class   \n",
       "  1   bottom-right-square    botm-Rght-Squa   Bottom Right Square   \n",
       "  2    middle-left-square  middle-left-sqar    Middle Left Square   \n",
       "  3  middle-middle-square   mddl-middle-sqr  Middle Middle Square   \n",
       "  4   middle-right-square    mdle-Rght-Squr   Middle Right Square   \n",
       "  5     top-middle-square   top-Middle-Squa     Top Middle Square   \n",
       "  6    bottom-left-square     botm-left-sqr    Bottom Left Square   \n",
       "  7       top-left-square     top-Left-Sqre       Top Left Square   \n",
       "  8      top-right-square     top-rght-sqre      Top Right Square   \n",
       "  9  bottom-middle-square     bttm-midd-sqr  Bottom Middle Square   \n",
       "  \n",
       "             y_pred_title           y_pred_desc     y_pred_title_desc  \n",
       "  0                 Class                 Class                 Class  \n",
       "  1   Bottom Right Square   Bottom Right Square   Bottom Right Square  \n",
       "  2    Middle Left Square    Middle Left Square    Middle Left Square  \n",
       "  3  Middle Middle Square  Middle Middle Square  Middle Middle Square  \n",
       "  4   Middle Right Square   Middle Right Square   Middle Right Square  \n",
       "  5     Top Middle Square     Top Middle Square     Top Middle Square  \n",
       "  6    Bottom Left Square    Bottom Left Square    Bottom Left Square  \n",
       "  7       Top Left Square       Top Left Square       Top Left Square  \n",
       "  8      Top Right Square      Top Right Square      Top Right Square  \n",
       "  9  Bottom Middle Square  Bottom Middle Square  Bottom Middle Square  ,\n",
       "  {'token_length': [101, 112, 433, 442]}],\n",
       " 'pred_df_8': [                       y               X       y_pred_baseline  \\\n",
       "  0              sulphates          slphts             Sulphates   \n",
       "  1    free_sulfur_dioxide  free_slfr_dxde   Free Sulfur Dioxide   \n",
       "  2              chlorides          chlrds             Chlorides   \n",
       "  3         residual_sugar         resisgr        Residual Sugar   \n",
       "  4                density           dnsty               Density   \n",
       "  5            citric_acid      citricAcid           Citric Acid   \n",
       "  6                alcohol           alchl               Alcohol   \n",
       "  7       volatile_acidity       vola_adty      Volatile Acidity   \n",
       "  8                  class            clss                 Class   \n",
       "  9   total_sulfur_dioxide     totaSlfrDxd  Total Sulfur Dioxide   \n",
       "  10         fixed_acidity      fixedAcdty         Fixed Acidity   \n",
       "  \n",
       "              y_pred_title           y_pred_desc     y_pred_title_desc  \n",
       "  0              Sulphates             Sulphates             Sulphates  \n",
       "  1    Free Sulfur Dioxide   Free Sulfur Dioxide   Free Sulfur Dioxide  \n",
       "  2              Chlorides             Chlorides             Chlorides  \n",
       "  3         Residual Sugar        Residual Sugar        Residual Sugar  \n",
       "  4                Density               Density               Density  \n",
       "  5            Citric Acid           Citric Acid           Citric Acid  \n",
       "  6                Alcohol               Alcohol               Alcohol  \n",
       "  7       Volatile Acidity      Volatile Acidity      Volatile Acidity  \n",
       "  8                  Class                 Class                 Class  \n",
       "  9   Total Sulfur Dioxide  Total Sulfur Dioxide  Total Sulfur Dioxide  \n",
       "  10         Fixed Acidity         Fixed Acidity         Fixed Acidity  ,\n",
       "  {'token_length': [86, 95, 98, 105]}],\n",
       " 'pred_df_9': [             y          X y_pred_baseline y_pred_title  y_pred_desc  \\\n",
       "  0        Looks        Lks           Likes        Likes        Looks   \n",
       "  1        Grade       Grde           Grade        Grade        Grade   \n",
       "  2  Urban/Rural  Urbn/Rral     Urban/Rural  Urban/Rural  Urban/Rural   \n",
       "  3       Grades       Grds          Grades       Grades       Grades   \n",
       "  4       Gender       Gndr          Gender       Gender       Gender   \n",
       "  5        Goals       Gals           Goals        Goals        Goals   \n",
       "  6       Sports       Spts          Sports       Sports       Sports   \n",
       "  7       School       Schl          School       School       School   \n",
       "  8        Money       Mony           Money        Money        Money   \n",
       "  \n",
       "    y_pred_title_desc  \n",
       "  0             Looks  \n",
       "  1             Grade  \n",
       "  2       Urban/Rural  \n",
       "  3            Grades  \n",
       "  4            Gender  \n",
       "  5             Goals  \n",
       "  6            Sports  \n",
       "  7            School  \n",
       "  8             Money  ,\n",
       "  {'token_length': [65, 73, 584, 590]}],\n",
       " 'pred_df_10': [                 y             X    y_pred_baseline       y_pred_title  \\\n",
       "  0         Position          Posi           Position           Position   \n",
       "  1       Strikeouts          Stri         Strikeouts         Strikeouts   \n",
       "  2            Walks          Wlks              Walks              Walks   \n",
       "  3     Games_played       Gmsplyd       Games Played       Games Played   \n",
       "  4     Hall_of_Fame    HallOfFame       Hall of Fame       Hall of Fame   \n",
       "  5          Doubles          Doub            Doubles            Doubles   \n",
       "  6          At_bats        AtBats            At Bats            At Bats   \n",
       "  7   Number_seasons     Nmbr_ssns  Number of Seasons  Number of Seasons   \n",
       "  8          Triples          Trls            Triples            Triples   \n",
       "  9  Batting_average  Batting_avrg    Batting Average    Batting Average   \n",
       "  \n",
       "           y_pred_desc  y_pred_title_desc  \n",
       "  0           Position           Position  \n",
       "  1            Strikes            Strikes  \n",
       "  2              Walks              Walks  \n",
       "  3       Games Played       Games Played  \n",
       "  4       Hall of Fame       Hall of Fame  \n",
       "  5            Doubles            Doubles  \n",
       "  6            At Bats            At Bats  \n",
       "  7  Number of Seasons  Number of Seasons  \n",
       "  8            Triples            Triples  \n",
       "  9    Batting Average    Batting Average  ,\n",
       "  {'token_length': [76, 83, 193, 198]}],\n",
       " 'pred_df_11': [                 y            X  y_pred_baseline     y_pred_title  \\\n",
       "  0      mediastinum        media            Media            Media   \n",
       "  1           pleura          plr   Platelet Count              PLR   \n",
       "  2      bone-marrow    bone-mrrw      Bone Marrow      Bone Marrow   \n",
       "  3            brain          brn            Brain            Brain   \n",
       "  4            liver          lvr            Liver            Liver   \n",
       "  5        abdominal         abdo          Abdomen          Abdomen   \n",
       "  6       peritoneum         peri       Peritoneum       Peritoneum   \n",
       "  7            class         clas   Classification   Classification   \n",
       "  8  histologic-type  hstlgc-type  Histologic Type  Histologic Type   \n",
       "  \n",
       "         y_pred_desc y_pred_title_desc  \n",
       "  0            Media             Media  \n",
       "  1             Peri              Peri  \n",
       "  2      Bone Marrow       Bone Marrow  \n",
       "  3            Brain             Brain  \n",
       "  4            Liver             Liver  \n",
       "  5          Abdomen           Abdomen  \n",
       "  6       Peritoneum        Peritoneum  \n",
       "  7            Class             Class  \n",
       "  8  Histologic Type   Histologic Type  ,\n",
       "  {'token_length': [65, 74, 2106, 2113]}],\n",
       " 'pred_df_12': [             y         X y_pred_baseline y_pred_title     y_pred_desc  \\\n",
       "  0  Attribute_1     Arbt1     Attribute 1  Attribute 1     Attribute 1   \n",
       "  1  Attribute_2     Attr2     Attribute 2  Attribute 2     Attribute 2   \n",
       "  2  Attribute_7    Atrt_7     Attribute 7  Attribute 7     Attribute 7   \n",
       "  3  Attribute_9    Attr_9     Attribute 9  Attribute 9     Attribute 9   \n",
       "  4  Attribute_8   Attrbt8     Attribute 8  Attribute 8     Attribute 8   \n",
       "  5  Attribute_3     Attr3     Attribute 3  Attribute 3     Attribute 3   \n",
       "  6  Attribute_5   Attrbt5     Attribute 5  Attribute 5     Attribute 5   \n",
       "  7        class      clas           Class        Class  Classification   \n",
       "  8  Attribute_4   Attrbt4     Attribute 4  Attribute 4     Attribute 4   \n",
       "  9  Attribute_6  Attrbt_6     Attribute 6  Attribute 6     Attribute 6   \n",
       "  \n",
       "    y_pred_title_desc  \n",
       "  0       Attribute 1  \n",
       "  1       Attribute 2  \n",
       "  2       Attribute 7  \n",
       "  3       Attribute 9  \n",
       "  4       Attribute 8  \n",
       "  5       Attribute 3  \n",
       "  6       Attribute 5  \n",
       "  7    Classification  \n",
       "  8       Attribute 4  \n",
       "  9       Attribute 6  ,\n",
       "  {'token_length': [75, 83, 322, 328]}],\n",
       " 'pred_df_13': [            y        X y_pred_baseline    y_pred_title        y_pred_desc  \\\n",
       "  0       Class     Clas           Class  Classification              Class   \n",
       "  1    Ask_Open  AskOpen        Ask Open        Ask Open  Ask Opening Price   \n",
       "  2    Bid_Open  BidOpen        Bid Open        Bid Open  Bid Opening Price   \n",
       "  3    Bid_High  BidHigh        Bid High        Bid High  Bid Highest Price   \n",
       "  4  Ask_Volume  AskVlme      Ask Volume      Ask Volume         Ask Volume   \n",
       "  5   Bid_Close  BidClos       Bid Close       Bid Close  Bid Closing Price   \n",
       "  6  Bid_Volume   BidVlm      Bid Volume      Bid Volume         Bid Volume   \n",
       "  7   Ask_Close  Ask_Cls       Ask Close       Ask Close  Ask Closing Price   \n",
       "  8    Ask_High  AskHigh        Ask High        Ask High  Ask Highest Price   \n",
       "  \n",
       "     y_pred_title_desc  \n",
       "  0              Class  \n",
       "  1  Ask Opening Price  \n",
       "  2  Bid Opening Price  \n",
       "  3     Bid High Price  \n",
       "  4         Ask Volume  \n",
       "  5  Bid Closing Price  \n",
       "  6         Bid Volume  \n",
       "  7  Ask Closing Price  \n",
       "  8     Ask High Price  ,\n",
       "  {'token_length': [68, 83, 503, 516]}],\n",
       " 'pred_df_14': [                                y                        X  \\\n",
       "  0                 total_eve_calls              totleveclls   \n",
       "  1                 total_day_calls               ttldaycall   \n",
       "  2                total_day_charge               ttlDayChrg   \n",
       "  3                       area_code                 areaCode   \n",
       "  4               total_night_calls             totaNighCals   \n",
       "  5                           class                     clss   \n",
       "  6                  account_length                accolngth   \n",
       "  7                           state                      stt   \n",
       "  8             total_night_minutes            ttl_nght_mnts   \n",
       "  9                    phone_number                 phonNmbr   \n",
       "  10             international_plan               internPlan   \n",
       "  11              total_day_minutes            totl_day_mnts   \n",
       "  12              total_eve_minutes               ttlEveMnts   \n",
       "  13  number_customer_service_calls  nmbr_customer_srvc_call   \n",
       "  14                voice_mail_plan             voicmailplan   \n",
       "  15             total_night_charge             ttalNighChrg   \n",
       "  16               total_eve_charge               ttlevechrg   \n",
       "  \n",
       "                       y_pred_baseline                      y_pred_title  \\\n",
       "  0                Total Evening Calls             Total Number of Calls   \n",
       "  1                    Total Day Calls                   Total Day Calls   \n",
       "  2                  Total Day Charges                 Total Day Charges   \n",
       "  3                          Area Code                         Area Code   \n",
       "  4                  Total Night Calls                 Total Night Calls   \n",
       "  5                              Class                             Class   \n",
       "  6                     Account Length                    Account Length   \n",
       "  7                              State                             State   \n",
       "  8                Total Night Minutes               Total Night Minutes   \n",
       "  9                       Phone Number                      Phone Number   \n",
       "  10                International Plan                International Plan   \n",
       "  11                 Total Day Minutes                 Total Day Minutes   \n",
       "  12             Total Evening Minutes             Total Evening Minutes   \n",
       "  13  Number of Customer Service Calls  Number of Customer Service Calls   \n",
       "  14                    Voicemail Plan                    Voicemail Plan   \n",
       "  15               Total Night Charges               Total Night Charges   \n",
       "  16             Total Evening Charges             Total Evening Charges   \n",
       "  \n",
       "                           y_pred_desc                 y_pred_title_desc  \n",
       "  0        Total Number of Level Calls             Total Number of Calls  \n",
       "  1                    Total Day Calls                   Total Day Calls  \n",
       "  2                  Total Day Charges                 Total Day Charges  \n",
       "  3                          Area Code                         Area Code  \n",
       "  4                  Total Night Calls                 Total Night Calls  \n",
       "  5                              Class                             Class  \n",
       "  6                     Account Length                    Account Length  \n",
       "  7                              State                             State  \n",
       "  8                Total Night Minutes               Total Night Minutes  \n",
       "  9                       Phone Number                      Phone Number  \n",
       "  10                International Plan                International Plan  \n",
       "  11                 Total Day Minutes                 Total Day Minutes  \n",
       "  12             Total Evening Minutes             Total Evening Minutes  \n",
       "  13  Number of Customer Service Calls  Number of Customer Service Calls  \n",
       "  14                    Voicemail Plan                    Voicemail Plan  \n",
       "  15               Total Night Charges               Total Night Charges  \n",
       "  16             Total Evening Charges             Total Evening Charges  ,\n",
       "  {'token_length': [120, 127, 269, 274]}],\n",
       " 'pred_df_15': [                            y                        X  \\\n",
       "  0   MAX.LENGTH_RECTANGULARITY  MAX.LENG_RECTANGULARITY   \n",
       "  1               HOLLOWS_RATIO                 HOWSRATI   \n",
       "  2                       Class                     Clas   \n",
       "  3               SCATTER_RATIO                SAERRATIO   \n",
       "  4        SKEWNESS_ABOUT_MAJOR         SKEW_ABOUT_MAJOR   \n",
       "  5       SCALED_VARIANCE_MAJOR            SCALVARIMAJOR   \n",
       "  6   SCALED_RADIUS_OF_GYRATION       SCLDRADIOFGYRATION   \n",
       "  7     MAX.LENGTH_ASPECT_RATIO      MAX.LENGASPECTRATIO   \n",
       "  8               ELONGATEDNESS                   ELONGA   \n",
       "  9        SKEWNESS_ABOUT_MINOR       SKEWNESSABOUTMINOR   \n",
       "  10      SCALED_VARIANCE_MINOR         SLEDVARIANCEMINO   \n",
       "  11               RADIUS_RATIO                 RADIRATO   \n",
       "  12       DISTANCE_CIRCULARITY           DISTANCE_CIRCU   \n",
       "  13                COMPACTNESS                    CPCTE   \n",
       "  14                CIRCULARITY                    CIRCU   \n",
       "  \n",
       "                    y_pred_baseline                   y_pred_title  \\\n",
       "  0   Maximum Length Rectangularity  Maximum Length Rectangularity   \n",
       "  1                       Hows Rati  Horizontal Width/Length Ratio   \n",
       "  2                           Class                          Class   \n",
       "  3                      Saer Ratio                 Saliency Ratio   \n",
       "  4                Skew About Major      Skewness About Major Axis   \n",
       "  5                 Scal Vari Major          Scalar Variance Major   \n",
       "  6            Scl Dradio Fgyration      Scalar Radius of Gyration   \n",
       "  7         Max Length Aspect Ratio    Maximum Length Aspect Ratio   \n",
       "  8                      Elongation                     Elongation   \n",
       "  9            Skewness About Minor      Skewness About Minor Axis   \n",
       "  10            Sled Variance Minor        Saliency Variance Minor   \n",
       "  11                      Radi Rato                   Radial Ratio   \n",
       "  12                 Distance Circu           Distance Circularity   \n",
       "  13                          Cpcte                    Compactness   \n",
       "  14                  Circumference                    Circularity   \n",
       "  \n",
       "                    y_pred_desc              y_pred_title_desc  \n",
       "  0   Max Length Rectangularity  Maximum Length Rectangularity  \n",
       "  1                   Hows Rati                  Scatter Ratio  \n",
       "  2                       Class                          Class  \n",
       "  3                   Sae Ratio                  Scatter Ratio  \n",
       "  4            Skew About Major           Skewness About Major  \n",
       "  5             Scal Vari Major          Scaled Variance Major  \n",
       "  6       Scl D Radiof Gyration      Scaled Radius of Gyration  \n",
       "  7       Max Leng Aspect Ratio        Max Length Aspect Ratio  \n",
       "  8                      Elonga                     Elongation  \n",
       "  9        Skewness About Minor           Skewness About Minor  \n",
       "  10         Sled Variance Mino          Scaled Variance Minor  \n",
       "  11                  Radi Rato                   Radius Ratio  \n",
       "  12             Distance Circu           Distance Circularity  \n",
       "  13                      Cpcte                          CPCTE  \n",
       "  14                      Circu                    Circularity  ,\n",
       "  {'token_length': [123, 130, 1828, 1833]}],\n",
       " 'pred_df_16': [                                 y                          X  \\\n",
       "  0               shift-differential          shif-differential   \n",
       "  1               statutory-holidays                stttry-Holi   \n",
       "  2                         vacation                       vctn   \n",
       "  3                         duration                       drtn   \n",
       "  4           bereavement-assistance             brvmnt-assstnc   \n",
       "  5                      standby-pay                   stan-pay   \n",
       "  6                            class                       clss   \n",
       "  7      contribution-to-dental-plan  contribution-to-dntl-plan   \n",
       "  8                          pension                       pnsn   \n",
       "  9      contribution-to-health-plan       cntrbtn-to-hlth-plan   \n",
       "  10  longterm-disability-assistance           long-Dsblty-Astn   \n",
       "  11       cost-of-living-adjustment          cost-Of-Livi-Adju   \n",
       "  12                   working-hours                   wkng-hrs   \n",
       "  13        wage-increase-third-year        wage-incs-thrd-year   \n",
       "  14             education-allowance                ectn-allwnc   \n",
       "  15        wage-increase-first-year        wage-Icrs-Frst-Year   \n",
       "  16       wage-increase-second-year       wage-Incrs-Seco-Year   \n",
       "  \n",
       "                  y_pred_baseline                 y_pred_title  \\\n",
       "  0            Shift Differential           Shift Differential   \n",
       "  1             Statutory Holiday            Statutory Holiday   \n",
       "  2                      Vacation                     Vacation   \n",
       "  3                      Duration                     Duration   \n",
       "  4        Bereavement Assistance       Bereavement Assistance   \n",
       "  5                   Standby Pay                Statutory Pay   \n",
       "  6                         Class                        Class   \n",
       "  7   Contribution to Dental Plan  Contribution to Dental Plan   \n",
       "  8                       Pension                      Pension   \n",
       "  9   Contribution to Health Plan  Contribution to Health Plan   \n",
       "  10        Long Disability Aston   Long Disability Assistance   \n",
       "  11    Cost of Living Adjustment    Cost of Living Adjustment   \n",
       "  12                Working Hours                Working Hours   \n",
       "  13    Wage Increases Third Year    Wage Increases Third Year   \n",
       "  14          Education Allowance          Education Allowance   \n",
       "  15    Wage Increases First Year    Wage Increases First Year   \n",
       "  16   Wage Increases Second Year   Wage Increases Second Year   \n",
       "  \n",
       "                          y_pred_desc                y_pred_title_desc  \n",
       "  0                Shift Differential               Shift Differential  \n",
       "  1                Statutory Holidays               Statutory Holidays  \n",
       "  2                          Vacation                         Vacation  \n",
       "  3                          Duration                         Duration  \n",
       "  4            Bereavement Assistance           Bereavement Assistance  \n",
       "  5                       Standby Pay                      Standby Pay  \n",
       "  6                             Class                            Class  \n",
       "  7       Contribution to Dental Plan      Contribution to Dental Plan  \n",
       "  8                           Pension                          Pension  \n",
       "  9       Contribution to Health Plan      Contribution to Health Plan  \n",
       "  10  Long-Term Disability Assistance  Long-Term Disability Assistance  \n",
       "  11        Cost of Living Adjustment        Cost of Living Adjustment  \n",
       "  12                    Working Hours                    Working Hours  \n",
       "  13         Wage Increase Third Year        Wage Increases Third Year  \n",
       "  14              Education Allowance              Education Allowance  \n",
       "  15         Wage Increase First Year        Wage Increases First Year  \n",
       "  16        Wage Increase Second Year       Wage Increases Second Year  ,\n",
       "  {'token_length': [138, 145, 1053, 1058]}],\n",
       " 'pred_df_17': [                 y           X      y_pred_baseline           y_pred_title  \\\n",
       "  0  exclusion_of_no  excl_of_no  Exclusion of Number     Exclusion of Nodes   \n",
       "  1    special_forms    spclfrms        Special Forms          Special Forms   \n",
       "  2   dislocation_of    dslctnof       Dislocation of   Dislocation of Nodes   \n",
       "  3  regeneration_of    rgnrtnof      Regeneration of  Regeneration of Nodes   \n",
       "  4          by_pass      bypass               Bypass                 Bypass   \n",
       "  5            class        clas       Classification         Classification   \n",
       "  \n",
       "         y_pred_desc y_pred_title_desc  \n",
       "  0  Exclusion of No   Exclusion of No  \n",
       "  1    Special Forms     Special Forms  \n",
       "  2   Dislocation of    Dislocation of  \n",
       "  3  Regeneration of   Regeneration of  \n",
       "  4           Bypass            Bypass  \n",
       "  5            Class             Class  ,\n",
       "  {'token_length': [60, 67, 1792, 1797]}],\n",
       " 'pred_df_18': [                              y                   X  \\\n",
       "  0               Wifes_education       WifsEducation   \n",
       "  1  Number_of_children_ever_born  NumbOfChlrEverBorn   \n",
       "  2     Contraceptive_method_used      Contramthdused   \n",
       "  3      Standard-of-living_index  Sndr-Of-LivingIndx   \n",
       "  4                Wifes_religion             WfsReli   \n",
       "  5           Husbands_occupation          Hsbdoccptn   \n",
       "  6            Husbands_education          Hsbndsedtn   \n",
       "  7                Media_exposure          Media_expr   \n",
       "  8                     Wifes_age              Wfsage   \n",
       "  \n",
       "                  y_pred_baseline                  y_pred_title  \\\n",
       "  0              Wife's Education              Wife's Education   \n",
       "  1  Number of Children Ever Born  Number of Children Ever Born   \n",
       "  2     Contraceptive Method Used     Contraceptive Method Used   \n",
       "  3      Standard of Living Index      Standard of Living Index   \n",
       "  4               Wife's Religion               Wife's Religion   \n",
       "  5          Husband's Occupation          Husband's Occupation   \n",
       "  6           Husband's Education           Husband's Education   \n",
       "  7                Media Exposure                Media Exposure   \n",
       "  8                    Wife's Age                    Wife's Age   \n",
       "  \n",
       "                      y_pred_desc             y_pred_title_desc  \n",
       "  0              Wife's Education              Wife's Education  \n",
       "  1  Number of Children Ever Born  Number of Children Ever Born  \n",
       "  2     Contraceptive Method Used     Contraceptive Method Used  \n",
       "  3      Standard-of-Living Index      Standard-of-Living Index  \n",
       "  4               Wife's Religion               Wife's Religion  \n",
       "  5          Husband's Occupation          Husband's Occupation  \n",
       "  6           Husband's Education           Husband's Education  \n",
       "  7                Media Exposure                Media Exposure  \n",
       "  8                    Wife's Age                    Wife's Age  ,\n",
       "  {'token_length': [88, 96, 781, 787]}],\n",
       " 'pred_df_19': [             y          X  y_pred_baseline     y_pred_title      y_pred_desc  \\\n",
       "  0        Class       Clss   Classification            Class            Class   \n",
       "  1    node-caps  node-Caps    Node Capsules    Node Capsules        Node Caps   \n",
       "  2   tumor-size  tmor-Size       Tumor Size       Tumor Size       Tumor Size   \n",
       "  3       breast       brst           Breast           Breast           Breast   \n",
       "  4    menopause       meno        Menopause        Menopause        Menopause   \n",
       "  5  breast-quad  brst-Quad  Breast Quadrant  Breast Quadrant  Breast Quadrant   \n",
       "  \n",
       "    y_pred_title_desc  \n",
       "  0             Class  \n",
       "  1         Node Caps  \n",
       "  2        Tumor Size  \n",
       "  3            Breast  \n",
       "  4         Menopause  \n",
       "  5   Breast Quadrant  ,\n",
       "  {'token_length': [58, 67, 1547, 1554]}],\n",
       " 'pred_df_20': [                 y         X  y_pred_baseline     y_pred_title  \\\n",
       "  0        Magnesium     Mgnsm        Magnesium        Magnesium   \n",
       "  1    Total_phenols  TtlPhnls    Total Phenols    Total Phenols   \n",
       "  2  Color_intensity  Clr_inst  Color Intensity  Color Intensity   \n",
       "  3          Alcohol     Alchl          Alcohol          Alcohol   \n",
       "  4          Proline      Prln          Proline          Proline   \n",
       "  5            class      clas            Class            Class   \n",
       "  \n",
       "         y_pred_desc y_pred_title_desc  \n",
       "  0        Magnesium         Magnesium  \n",
       "  1    Total Phenols     Total Phenols  \n",
       "  2  Color Intensity   Color Intensity  \n",
       "  3          Alcohol           Alcohol  \n",
       "  4  Proanthocyanins   Proanthocyanins  \n",
       "  5            Class             Class  ,\n",
       "  {'token_length': [58, 65, 982, 987]}]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the predictions in\n",
    "with open('data/complete_preds_experiment_2.pkl', 'rb') as handle:\n",
    "    pred_dct_title_desc = pickle.load(handle)\n",
    "pred_dct_title_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3cda6d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['engine location'] ['engine location', 'enginelocation']\n",
      "['stroke'] ['stroke']\n",
      "['engine type'] ['engine type', 'enginetype']\n",
      "['body style'] ['body style', 'bodystyle']\n",
      "['drive wheels', 'drive wheel'] ['drive wheels', 'drivewheels', 'drivewheel']\n",
      "['peak rpm'] ['peak rpm', 'peakrpm']\n",
      "['aspiration'] ['aspiration']\n",
      "['engine size'] ['engine size', 'enginesize']\n",
      "['width'] ['width']\n",
      "['compression ratio'] ['compression ratio', 'compressionratio']\n",
      "['fuel type'] ['fuel type', 'fueltype']\n",
      "['wheel base'] ['wheel base', 'wheelbase']\n",
      "['length'] ['length']\n",
      "['horsepower'] ['horsepower']\n",
      "['height'] ['height']\n",
      "['price'] ['price']\n",
      "['fuel system'] ['fuel system', 'fuelsystem']\n",
      "['curb weight'] ['curb weight', 'curbweight']\n",
      "['decision count'] ['decision count', 'decisioncount']\n",
      "['call pairs', 'call pair'] ['call pairs', 'callpairs', 'callpair']\n",
      "['unique operators', 'unique operator'] ['unique operands', 'uniqueoperands', 'uniqueoperand']\n",
      "['branch count'] ['branch count', 'branchcount']\n",
      "['decision factors', 'decision factor'] ['defects', 'defect']\n",
      "['decision density'] ['decision density', 'decisiondensity']\n",
      "['total operators', 'total operator'] ['total operands', 'totaloperands', 'totaloperand']\n",
      "['multiple condition count'] ['multiple condition count', 'multipleconditioncount']\n",
      "['condition count'] ['condition count', 'conditioncount']\n",
      "['design complexity'] ['design complexity', 'designcomplexity']\n",
      "['formal payments', 'formal payment'] ['formal parameters', 'formalparameters', 'formalparameter']\n",
      "['total errors', 'total error'] ['total operators', 'totaloperators', 'totaloperator']\n",
      "['unique operators', 'unique operator'] ['unique operators', 'uniqueoperators', 'uniqueoperator']\n",
      "['design density'] ['design density', 'designdensity']\n",
      "['third surgeon'] ['thyroid surgery', 'thyroidsurgery']\n",
      "['tumor'] ['tumor']\n",
      "['query hypotension'] ['query hypothyroid', 'queryhypothyroid']\n",
      "['pregnant'] ['pregnant']\n",
      "['thyroid stimulating hormone measurement', 'thyroid stimulate hormone measurement'] ['tsh measured', 'tshmeasured', 'tshmeasure']\n",
      "['glutaric acid'] ['goitre']\n",
      "['lithium'] ['lithium']\n",
      "['haplotry'] ['hypopituitary']\n",
      "['referral source'] ['referral source', 'referralsource']\n",
      "['classification'] ['class']\n",
      "['cap shape'] ['cap shape', 'capshape']\n",
      "['stalk surface below ring'] ['stalk surface below ring', 'stalksurfacebelowring', 'stalksurfacebelowre']\n",
      "['gill color'] ['gill color', 'gillcolor']\n",
      "['spore print color'] ['spore print color', 'sporeprintcolor']\n",
      "['cap color'] ['cap color', 'capcolor']\n",
      "['ring number'] ['ring number', 'ringnumber']\n",
      "['stalk root'] ['stalk root', 'stalkroot']\n",
      "['ring type'] ['ring type', 'ringtype']\n",
      "['class'] ['class']\n",
      "['stalk shape'] ['stalk shape', 'stalkshape']\n",
      "['stalk color above ring'] ['stalk color above ring', 'stalkcolorabovering', 'stalkcolorabovere']\n",
      "['population'] ['population']\n",
      "['gill size'] ['gill size', 'gillsize']\n",
      "['habitat'] ['habitat']\n",
      "['veil type'] ['veil type', 'veiltype']\n",
      "['stalk surface above ring'] ['stalk surface above ring', 'stalksurfaceabovering', 'stalksurfaceabovere']\n",
      "['gill spacing', 'gill space'] ['gill spacing', 'gillspacing', 'gillspace']\n",
      "['gill attachment'] ['gill attachment', 'gillattachment']\n",
      "['stalk color below ring'] ['stalk color below ring', 'stalkcolorbelowring', 'stalkcolorbelowre']\n",
      "['veil color'] ['veil color', 'veilcolor']\n",
      "['cap surface'] ['cap surface', 'capsurface']\n",
      "['region center color'] ['region centroid col', 'regioncentroidcol']\n",
      "['intensity mean'] ['intensity mean', 'intensitymean']\n",
      "['hue mean'] ['hue mean', 'huemean']\n",
      "['saturation mean'] ['saturation mean', 'saturationmean']\n",
      "['short line density 5'] ['short line density 5', 'shortlinedensity5']\n",
      "['edge mean'] ['hedge mean', 'hedgemean']\n",
      "['short line density 2'] ['short line density 2', 'shortlinedensity2']\n",
      "['region centered row', 'region center row'] ['region centroid row', 'regioncentroidrow']\n",
      "['class'] ['class']\n",
      "['value mean'] ['value mean', 'valuemean']\n",
      "['row blur mean'] ['rawblue mean', 'rawbluemean']\n",
      "['row green mean'] ['rawgreen mean', 'rawgreenmean']\n",
      "['region pixel count'] ['region pixel count', 'regionpixelcount']\n",
      "['classification'] ['class']\n",
      "['history'] ['histology']\n",
      "['association'] ['ascites', 'ascite']\n",
      "['airway'] ['antivirals', 'antiviral']\n",
      "['sperm'] ['spiders', 'spider']\n",
      "['blueberry'] ['bilirubin']\n",
      "['liver function'] ['liver firm', 'liverfirm']\n",
      "['maize'] ['malaise']\n",
      "['variance'] ['varices', 'varix']\n",
      "['abnormality'] ['albumin']\n",
      "['selenoprotein p like protein', 'selenoprotein plike protein'] ['spleen palpable', 'spleenpalpable']\n",
      "['fingerprinting'] ['fatigue']\n",
      "['arxid'] ['anorexia']\n",
      "['soid'] ['steroid']\n",
      "['class'] ['class']\n",
      "['bottom right square'] ['bottom right square', 'bottomrightsquare']\n",
      "['middle left square', 'middle leave square'] ['middle left square', 'middleleftsquare']\n",
      "['middle middle square'] ['middle middle square', 'middlemiddlesquare']\n",
      "['middle right square'] ['middle right square', 'middlerightsquare']\n",
      "['top middle square'] ['top middle square', 'topmiddlesquare']\n",
      "['bottom left square', 'bottom leave square'] ['bottom left square', 'bottomleftsquare']\n",
      "['top left square', 'top leave square'] ['top left square', 'topleftsquare']\n",
      "['top right square'] ['top right square', 'toprightsquare']\n",
      "['bottom middle square'] ['bottom middle square', 'bottommiddlesquare']\n",
      "['sulphates', 'sulphate'] ['sulphates', 'sulphate']\n",
      "['free sulfur dioxide'] ['free sulfur dioxide', 'freesulfurdioxide']\n",
      "['chlorides', 'chloride'] ['chlorides', 'chloride']\n",
      "['residual sugar'] ['residual sugar', 'residualsugar']\n",
      "['density'] ['density']\n",
      "['citric acid'] ['citric acid', 'citricacid']\n",
      "['alcohol'] ['alcohol']\n",
      "['volatile acidity'] ['volatile acidity', 'volatileacidity']\n",
      "['class'] ['class']\n",
      "['total sulfur dioxide'] ['total sulfur dioxide', 'totalsulfurdioxide']\n",
      "['fixed acidity', 'fix acidity'] ['fixed acidity', 'fixedacidity']\n",
      "['likes', 'like'] ['looks', 'look']\n",
      "['grade'] ['grade']\n",
      "['urbanrural'] ['urbanrural']\n",
      "['grades', 'grade'] ['grades', 'grade']\n",
      "['gender'] ['gender']\n",
      "['goals', 'goal'] ['goals', 'goal']\n",
      "['sports', 'sport'] ['sports', 'sport']\n",
      "['school'] ['school']\n",
      "['money'] ['money']\n",
      "['position'] ['position']\n",
      "['strikeouts', 'strikeout'] ['strikeouts', 'strikeout']\n",
      "['walks', 'walk'] ['walks', 'walk']\n",
      "['games played', 'game play'] ['games played', 'gamesplayed', 'gamesplaye']\n",
      "['hall of fame'] ['hall of fame', 'halloffame']\n",
      "['doubles', 'double'] ['doubles', 'double']\n",
      "['at bats', 'at bat'] ['at bats', 'atbats', 'atbat']\n",
      "['number of seasons', 'number of season'] ['number seasons', 'numberseasons', 'numberseason']\n",
      "['triples', 'triple'] ['triples', 'triple']\n",
      "['batting average'] ['batting average', 'battingaverage']\n",
      "['media', 'medium'] ['mediastinum']\n",
      "['platelet count'] ['pleura']\n",
      "['bone marrow'] ['bone marrow', 'bonemarrow']\n",
      "['brain'] ['brain']\n",
      "['liver'] ['liver']\n",
      "['abdomen'] ['abdominal']\n",
      "['peritoneum'] ['peritoneum']\n",
      "['classification'] ['class']\n",
      "['histologic type'] ['histologic type', 'histologictype']\n",
      "['attribute 1'] ['attribute 1', 'attribute1']\n",
      "['attribute 2'] ['attribute 2', 'attribute2']\n",
      "['attribute 7'] ['attribute 7', 'attribute7']\n",
      "['attribute 9'] ['attribute 9', 'attribute9']\n",
      "['attribute 8'] ['attribute 8', 'attribute8']\n",
      "['attribute 3'] ['attribute 3', 'attribute3']\n",
      "['attribute 5'] ['attribute 5', 'attribute5']\n",
      "['class'] ['class']\n",
      "['attribute 4'] ['attribute 4', 'attribute4']\n",
      "['attribute 6'] ['attribute 6', 'attribute6']\n",
      "['class'] ['class']\n",
      "['ask open'] ['ask open', 'askopen']\n",
      "['bid open'] ['bid open', 'bidopen']\n",
      "['bid high'] ['bid high', 'bidhigh']\n",
      "['ask volume'] ['ask volume', 'askvolume']\n",
      "['bid close'] ['bid close', 'bidclose']\n",
      "['bid volume'] ['bid volume', 'bidvolume']\n",
      "['ask close'] ['ask close', 'askclose']\n",
      "['ask high'] ['ask high', 'askhigh']\n",
      "['total evening calls', 'total evening call'] ['total eve calls', 'totalevecalls', 'totalevecall']\n",
      "['total day calls', 'total day call'] ['total day calls', 'totaldaycalls', 'totaldaycall']\n",
      "['total day charges', 'total day charge'] ['total day charge', 'totaldaycharge']\n",
      "['area code'] ['area code', 'areacode']\n",
      "['total night calls', 'total night call'] ['total night calls', 'totalnightcalls', 'totalnightcall']\n",
      "['class'] ['class']\n",
      "['account length'] ['account length', 'accountlength']\n",
      "['state'] ['state']\n",
      "['total night minutes', 'total night minute'] ['total night minutes', 'totalnightminutes', 'totalnightminute']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phone number'] ['phone number', 'phonenumber']\n",
      "['international plan'] ['international plan', 'internationalplan']\n",
      "['total day minutes', 'total day minute'] ['total day minutes', 'totaldayminutes', 'totaldayminute']\n",
      "['total evening minutes', 'total evening minute'] ['total eve minutes', 'totaleveminutes', 'totaleveminute']\n",
      "['number of customer service calls', 'number of customer service call'] ['number customer service calls', 'numbercustomerservicecalls', 'numbercustomerservicecall']\n",
      "['voicemail plan'] ['voice mail plan', 'voicemailplan']\n",
      "['total night charges', 'total night charge'] ['total night charge', 'totalnightcharge']\n",
      "['total evening charges', 'total evening charge'] ['total eve charge', 'totalevecharge']\n",
      "['maximum length rectangularity'] ['maxlength rectangularity', 'maxlengthrectangularity']\n",
      "['hows rati', 'how s rati'] ['hollows ratio', 'hollowsratio']\n",
      "['class'] ['class']\n",
      "['saer ratio'] ['scatter ratio', 'scatterratio']\n",
      "['skew about major'] ['skewness about major', 'skewnessaboutmajor']\n",
      "['scal vari major'] ['scaled variance major', 'scaledvariancemajor']\n",
      "['scl dradio fgyration'] ['scaled radius of gyration', 'scaledradiusofgyration']\n",
      "['max length aspect ratio'] ['maxlength aspect ratio', 'maxlengthaspectratio']\n",
      "['elongation'] ['elongatedness']\n",
      "['skewness about minor'] ['skewness about minor', 'skewnessaboutminor']\n",
      "['sled variance minor', 'sle variance minor'] ['scaled variance minor', 'scaledvarianceminor']\n",
      "['radi rato'] ['radius ratio', 'radiusratio']\n",
      "['distance circu'] ['distance circularity', 'distancecircularity']\n",
      "['cpcte'] ['compactness']\n",
      "['circumference'] ['circularity']\n",
      "['shift differential'] ['shift differential', 'shiftdifferential']\n",
      "['statutory holiday'] ['statutory holidays', 'statutoryholidays', 'statutoryholiday']\n",
      "['vacation'] ['vacation']\n",
      "['duration'] ['duration']\n",
      "['bereavement assistance'] ['bereavement assistance', 'bereavementassistance']\n",
      "['standby pay'] ['standby pay', 'standbypay']\n",
      "['class'] ['class']\n",
      "['contribution to dental plan'] ['contribution to dental plan', 'contributiontodentalplan']\n",
      "['pension'] ['pension']\n",
      "['contribution to health plan'] ['contribution to health plan', 'contributiontohealthplan']\n",
      "['long disability aston'] ['longterm disability assistance', 'longtermdisabilityassistance']\n",
      "['cost of living adjustment', 'cost of live adjustment'] ['cost of living adjustment', 'costoflivingadjustment']\n",
      "['working hours', 'work hour'] ['working hours', 'workinghours', 'workinghour']\n",
      "['wage increases third year', 'wage increase third year'] ['wage increase third year', 'wageincreasethirdyear']\n",
      "['education allowance'] ['education allowance', 'educationallowance']\n",
      "['wage increases first year', 'wage increase first year'] ['wage increase first year', 'wageincreasefirstyear']\n",
      "['wage increases second year', 'wage increase second year'] ['wage increase second year', 'wageincreasesecondyear']\n",
      "['exclusion of number'] ['exclusion of no', 'exclusionofno']\n",
      "['special forms', 'special form'] ['special forms', 'specialforms', 'specialform']\n",
      "['dislocation of'] ['dislocation of', 'dislocationof']\n",
      "['regeneration of'] ['regeneration of', 'regenerationof']\n",
      "['bypass'] ['by pass', 'bypass']\n",
      "['classification'] ['class']\n",
      "['wifes education', 'wife education'] ['wifes education', 'wifeseducation']\n",
      "['number of children ever born', 'number of child ever bear'] ['number of children ever born', 'numberofchildreneverborn']\n",
      "['contraceptive method used', 'contraceptive method use'] ['contraceptive method used', 'contraceptivemethodused', 'contraceptivemethoduse']\n",
      "['standard of living index', 'standard of live index'] ['standard of living index', 'standardoflivingindex']\n",
      "['wifes religion', 'wife religion'] ['wifes religion', 'wifesreligion']\n",
      "['husbands occupation', 'husband occupation'] ['husbands occupation', 'husbandsoccupation']\n",
      "['husbands education', 'husband education'] ['husbands education', 'husbandseducation']\n",
      "['media exposure', 'medium exposure'] ['media exposure', 'mediaexposure']\n",
      "['wifes age', 'wife age'] ['wifes age', 'wifesage']\n",
      "['classification'] ['class']\n",
      "['node capsules', 'node capsule'] ['node caps', 'nodecaps', 'nodecap']\n",
      "['tumor size'] ['tumor size', 'tumorsize']\n",
      "['breast'] ['breast']\n",
      "['menopause'] ['menopause']\n",
      "['breast quadrant'] ['breast quad', 'breastquad']\n",
      "['magnesium'] ['magnesium']\n",
      "['total phenols', 'total phenol'] ['total phenols', 'totalphenols', 'totalphenol']\n",
      "['color intensity'] ['color intensity', 'colorintensity']\n",
      "['alcohol'] ['alcohol']\n",
      "['proline'] ['proline']\n",
      "['class'] ['class']\n",
      "['engine location'] ['engine location', 'enginelocation']\n",
      "['stroke'] ['stroke']\n",
      "['engine type'] ['engine type', 'enginetype']\n",
      "['body style'] ['body style', 'bodystyle']\n",
      "['drive wheels', 'drive wheel'] ['drive wheels', 'drivewheels', 'drivewheel']\n",
      "['peak rpm'] ['peak rpm', 'peakrpm']\n",
      "['aspiration'] ['aspiration']\n",
      "['engine size'] ['engine size', 'enginesize']\n",
      "['width'] ['width']\n",
      "['compression ratio'] ['compression ratio', 'compressionratio']\n",
      "['fuel type'] ['fuel type', 'fueltype']\n",
      "['wheel base'] ['wheel base', 'wheelbase']\n",
      "['length'] ['length']\n",
      "['horsepower'] ['horsepower']\n",
      "['height'] ['height']\n",
      "['price'] ['price']\n",
      "['fuel system'] ['fuel system', 'fuelsystem']\n",
      "['curb weight'] ['curb weight', 'curbweight']\n",
      "['decision count'] ['decision count', 'decisioncount']\n",
      "['call pairs', 'call pair'] ['call pairs', 'callpairs', 'callpair']\n",
      "['unique operators', 'unique operator'] ['unique operands', 'uniqueoperands', 'uniqueoperand']\n",
      "['branch count'] ['branch count', 'branchcount']\n",
      "['decision factors', 'decision factor'] ['defects', 'defect']\n",
      "['decision density'] ['decision density', 'decisiondensity']\n",
      "['total operators', 'total operator'] ['total operands', 'totaloperands', 'totaloperand']\n",
      "['multiple condition count'] ['multiple condition count', 'multipleconditioncount']\n",
      "['condition count'] ['condition count', 'conditioncount']\n",
      "['design complexity'] ['design complexity', 'designcomplexity']\n",
      "['formal payments', 'formal payment'] ['formal parameters', 'formalparameters', 'formalparameter']\n",
      "['total errors', 'total error'] ['total operators', 'totaloperators', 'totaloperator']\n",
      "['unique operators', 'unique operator'] ['unique operators', 'uniqueoperators', 'uniqueoperator']\n",
      "['design density'] ['design density', 'designdensity']\n",
      "['third surgery'] ['thyroid surgery', 'thyroidsurgery']\n",
      "['tumor'] ['tumor']\n",
      "['query hypothesis'] ['query hypothyroid', 'queryhypothyroid']\n",
      "['pregnant'] ['pregnant']\n",
      "['tsh measurement'] ['tsh measured', 'tshmeasured', 'tshmeasure']\n",
      "['glitter'] ['goitre']\n",
      "['lithium'] ['lithium']\n",
      "['hippotherapy'] ['hypopituitary']\n",
      "['referral source'] ['referral source', 'referralsource']\n",
      "['classification'] ['class']\n",
      "['cap shape'] ['cap shape', 'capshape']\n",
      "['stalk surface below ring'] ['stalk surface below ring', 'stalksurfacebelowring', 'stalksurfacebelowre']\n",
      "['gill color'] ['gill color', 'gillcolor']\n",
      "['spore print color'] ['spore print color', 'sporeprintcolor']\n",
      "['cap color'] ['cap color', 'capcolor']\n",
      "['ring number'] ['ring number', 'ringnumber']\n",
      "['stalk root'] ['stalk root', 'stalkroot']\n",
      "['ring type'] ['ring type', 'ringtype']\n",
      "['class'] ['class']\n",
      "['stalk shape'] ['stalk shape', 'stalkshape']\n",
      "['stalk color above ring'] ['stalk color above ring', 'stalkcolorabovering', 'stalkcolorabovere']\n",
      "['population'] ['population']\n",
      "['gill size'] ['gill size', 'gillsize']\n",
      "['habitat'] ['habitat']\n",
      "['veil type'] ['veil type', 'veiltype']\n",
      "['stalk surface above ring'] ['stalk surface above ring', 'stalksurfaceabovering', 'stalksurfaceabovere']\n",
      "['gill spacing', 'gill space'] ['gill spacing', 'gillspacing', 'gillspace']\n",
      "['gill attachment'] ['gill attachment', 'gillattachment']\n",
      "['stalk color below ring'] ['stalk color below ring', 'stalkcolorbelowring', 'stalkcolorbelowre']\n",
      "['veil color'] ['veil color', 'veilcolor']\n",
      "['cap surface'] ['cap surface', 'capsurface']\n",
      "['region center color'] ['region centroid col', 'regioncentroidcol']\n",
      "['intensity mean'] ['intensity mean', 'intensitymean']\n",
      "['hue mean'] ['hue mean', 'huemean']\n",
      "['saturation mean'] ['saturation mean', 'saturationmean']\n",
      "['short line density 5'] ['short line density 5', 'shortlinedensity5']\n",
      "['edge mean'] ['hedge mean', 'hedgemean']\n",
      "['short line density 2'] ['short line density 2', 'shortlinedensity2']\n",
      "['region centered row', 'region center row'] ['region centroid row', 'regioncentroidrow']\n",
      "['class'] ['class']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['value mean'] ['value mean', 'valuemean']\n",
      "['row blur mean'] ['rawblue mean', 'rawbluemean']\n",
      "['row green mean'] ['rawgreen mean', 'rawgreenmean']\n",
      "['region pixel count'] ['region pixel count', 'regionpixelcount']\n",
      "['classification'] ['class']\n",
      "['histology'] ['histology']\n",
      "['ascites', 'ascite'] ['ascites', 'ascite']\n",
      "['alk phosphate'] ['antivirals', 'antiviral']\n",
      "['spermidine'] ['spiders', 'spider']\n",
      "['bilirubin'] ['bilirubin']\n",
      "['liver firm'] ['liver firm', 'liverfirm']\n",
      "['malaise'] ['malaise']\n",
      "['varices', 'varix'] ['varices', 'varix']\n",
      "['albumin'] ['albumin']\n",
      "['spleen palpable'] ['spleen palpable', 'spleenpalpable']\n",
      "['figer'] ['fatigue']\n",
      "['ascites', 'ascite'] ['anorexia']\n",
      "['spider naevi'] ['steroid']\n",
      "['class'] ['class']\n",
      "['bottom right square'] ['bottom right square', 'bottomrightsquare']\n",
      "['middle left square', 'middle leave square'] ['middle left square', 'middleleftsquare']\n",
      "['middle middle square'] ['middle middle square', 'middlemiddlesquare']\n",
      "['middle right square'] ['middle right square', 'middlerightsquare']\n",
      "['top middle square'] ['top middle square', 'topmiddlesquare']\n",
      "['bottom left square', 'bottom leave square'] ['bottom left square', 'bottomleftsquare']\n",
      "['top left square', 'top leave square'] ['top left square', 'topleftsquare']\n",
      "['top right square'] ['top right square', 'toprightsquare']\n",
      "['bottom middle square'] ['bottom middle square', 'bottommiddlesquare']\n",
      "['sulphates', 'sulphate'] ['sulphates', 'sulphate']\n",
      "['free sulfur dioxide'] ['free sulfur dioxide', 'freesulfurdioxide']\n",
      "['chlorides', 'chloride'] ['chlorides', 'chloride']\n",
      "['residual sugar'] ['residual sugar', 'residualsugar']\n",
      "['density'] ['density']\n",
      "['citric acid'] ['citric acid', 'citricacid']\n",
      "['alcohol'] ['alcohol']\n",
      "['volatile acidity'] ['volatile acidity', 'volatileacidity']\n",
      "['class'] ['class']\n",
      "['total sulfur dioxide'] ['total sulfur dioxide', 'totalsulfurdioxide']\n",
      "['fixed acidity', 'fix acidity'] ['fixed acidity', 'fixedacidity']\n",
      "['likes', 'like'] ['looks', 'look']\n",
      "['grade'] ['grade']\n",
      "['urbanrural'] ['urbanrural']\n",
      "['grades', 'grade'] ['grades', 'grade']\n",
      "['gender'] ['gender']\n",
      "['goals', 'goal'] ['goals', 'goal']\n",
      "['sports', 'sport'] ['sports', 'sport']\n",
      "['school'] ['school']\n",
      "['money'] ['money']\n",
      "['position'] ['position']\n",
      "['strikeouts', 'strikeout'] ['strikeouts', 'strikeout']\n",
      "['walks', 'walk'] ['walks', 'walk']\n",
      "['games played', 'game play'] ['games played', 'gamesplayed', 'gamesplaye']\n",
      "['hall of fame'] ['hall of fame', 'halloffame']\n",
      "['doubles', 'double'] ['doubles', 'double']\n",
      "['at bats', 'at bat'] ['at bats', 'atbats', 'atbat']\n",
      "['number of seasons', 'number of season'] ['number seasons', 'numberseasons', 'numberseason']\n",
      "['triples', 'triple'] ['triples', 'triple']\n",
      "['batting average'] ['batting average', 'battingaverage']\n",
      "['media', 'medium'] ['mediastinum']\n",
      "['plr'] ['pleura']\n",
      "['bone marrow'] ['bone marrow', 'bonemarrow']\n",
      "['brain'] ['brain']\n",
      "['liver'] ['liver']\n",
      "['abdomen'] ['abdominal']\n",
      "['peritoneum'] ['peritoneum']\n",
      "['classification'] ['class']\n",
      "['histologic type'] ['histologic type', 'histologictype']\n",
      "['attribute 1'] ['attribute 1', 'attribute1']\n",
      "['attribute 2'] ['attribute 2', 'attribute2']\n",
      "['attribute 7'] ['attribute 7', 'attribute7']\n",
      "['attribute 9'] ['attribute 9', 'attribute9']\n",
      "['attribute 8'] ['attribute 8', 'attribute8']\n",
      "['attribute 3'] ['attribute 3', 'attribute3']\n",
      "['attribute 5'] ['attribute 5', 'attribute5']\n",
      "['class'] ['class']\n",
      "['attribute 4'] ['attribute 4', 'attribute4']\n",
      "['attribute 6'] ['attribute 6', 'attribute6']\n",
      "['classification'] ['class']\n",
      "['ask open'] ['ask open', 'askopen']\n",
      "['bid open'] ['bid open', 'bidopen']\n",
      "['bid high'] ['bid high', 'bidhigh']\n",
      "['ask volume'] ['ask volume', 'askvolume']\n",
      "['bid close'] ['bid close', 'bidclose']\n",
      "['bid volume'] ['bid volume', 'bidvolume']\n",
      "['ask close'] ['ask close', 'askclose']\n",
      "['ask high'] ['ask high', 'askhigh']\n",
      "['total number of calls', 'total number of call'] ['total eve calls', 'totalevecalls', 'totalevecall']\n",
      "['total day calls', 'total day call'] ['total day calls', 'totaldaycalls', 'totaldaycall']\n",
      "['total day charges', 'total day charge'] ['total day charge', 'totaldaycharge']\n",
      "['area code'] ['area code', 'areacode']\n",
      "['total night calls', 'total night call'] ['total night calls', 'totalnightcalls', 'totalnightcall']\n",
      "['class'] ['class']\n",
      "['account length'] ['account length', 'accountlength']\n",
      "['state'] ['state']\n",
      "['total night minutes', 'total night minute'] ['total night minutes', 'totalnightminutes', 'totalnightminute']\n",
      "['phone number'] ['phone number', 'phonenumber']\n",
      "['international plan'] ['international plan', 'internationalplan']\n",
      "['total day minutes', 'total day minute'] ['total day minutes', 'totaldayminutes', 'totaldayminute']\n",
      "['total evening minutes', 'total evening minute'] ['total eve minutes', 'totaleveminutes', 'totaleveminute']\n",
      "['number of customer service calls', 'number of customer service call'] ['number customer service calls', 'numbercustomerservicecalls', 'numbercustomerservicecall']\n",
      "['voicemail plan'] ['voice mail plan', 'voicemailplan']\n",
      "['total night charges', 'total night charge'] ['total night charge', 'totalnightcharge']\n",
      "['total evening charges', 'total evening charge'] ['total eve charge', 'totalevecharge']\n",
      "['maximum length rectangularity'] ['maxlength rectangularity', 'maxlengthrectangularity']\n",
      "['horizontal widthlength ratio'] ['hollows ratio', 'hollowsratio']\n",
      "['class'] ['class']\n",
      "['saliency ratio'] ['scatter ratio', 'scatterratio']\n",
      "['skewness about major axis'] ['skewness about major', 'skewnessaboutmajor']\n",
      "['scalar variance major'] ['scaled variance major', 'scaledvariancemajor']\n",
      "['scalar radius of gyration'] ['scaled radius of gyration', 'scaledradiusofgyration']\n",
      "['maximum length aspect ratio'] ['maxlength aspect ratio', 'maxlengthaspectratio']\n",
      "['elongation'] ['elongatedness']\n",
      "['skewness about minor axis'] ['skewness about minor', 'skewnessaboutminor']\n",
      "['saliency variance minor'] ['scaled variance minor', 'scaledvarianceminor']\n",
      "['radial ratio'] ['radius ratio', 'radiusratio']\n",
      "['distance circularity'] ['distance circularity', 'distancecircularity']\n",
      "['compactness'] ['compactness']\n",
      "['circularity'] ['circularity']\n",
      "['shift differential'] ['shift differential', 'shiftdifferential']\n",
      "['statutory holiday'] ['statutory holidays', 'statutoryholidays', 'statutoryholiday']\n",
      "['vacation'] ['vacation']\n",
      "['duration'] ['duration']\n",
      "['bereavement assistance'] ['bereavement assistance', 'bereavementassistance']\n",
      "['statutory pay'] ['standby pay', 'standbypay']\n",
      "['class'] ['class']\n",
      "['contribution to dental plan'] ['contribution to dental plan', 'contributiontodentalplan']\n",
      "['pension'] ['pension']\n",
      "['contribution to health plan'] ['contribution to health plan', 'contributiontohealthplan']\n",
      "['long disability assistance'] ['longterm disability assistance', 'longtermdisabilityassistance']\n",
      "['cost of living adjustment', 'cost of live adjustment'] ['cost of living adjustment', 'costoflivingadjustment']\n",
      "['working hours', 'work hour'] ['working hours', 'workinghours', 'workinghour']\n",
      "['wage increases third year', 'wage increase third year'] ['wage increase third year', 'wageincreasethirdyear']\n",
      "['education allowance'] ['education allowance', 'educationallowance']\n",
      "['wage increases first year', 'wage increase first year'] ['wage increase first year', 'wageincreasefirstyear']\n",
      "['wage increases second year', 'wage increase second year'] ['wage increase second year', 'wageincreasesecondyear']\n",
      "['exclusion of nodes', 'exclusion of node'] ['exclusion of no', 'exclusionofno']\n",
      "['special forms', 'special form'] ['special forms', 'specialforms', 'specialform']\n",
      "['dislocation of nodes', 'dislocation of node'] ['dislocation of', 'dislocationof']\n",
      "['regeneration of nodes', 'regeneration of node'] ['regeneration of', 'regenerationof']\n",
      "['bypass'] ['by pass', 'bypass']\n",
      "['classification'] ['class']\n",
      "['wifes education', 'wife education'] ['wifes education', 'wifeseducation']\n",
      "['number of children ever born', 'number of child ever bear'] ['number of children ever born', 'numberofchildreneverborn']\n",
      "['contraceptive method used', 'contraceptive method use'] ['contraceptive method used', 'contraceptivemethodused', 'contraceptivemethoduse']\n",
      "['standard of living index', 'standard of live index'] ['standard of living index', 'standardoflivingindex']\n",
      "['wifes religion', 'wife religion'] ['wifes religion', 'wifesreligion']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['husbands occupation', 'husband occupation'] ['husbands occupation', 'husbandsoccupation']\n",
      "['husbands education', 'husband education'] ['husbands education', 'husbandseducation']\n",
      "['media exposure', 'medium exposure'] ['media exposure', 'mediaexposure']\n",
      "['wifes age', 'wife age'] ['wifes age', 'wifesage']\n",
      "['class'] ['class']\n",
      "['node capsules', 'node capsule'] ['node caps', 'nodecaps', 'nodecap']\n",
      "['tumor size'] ['tumor size', 'tumorsize']\n",
      "['breast'] ['breast']\n",
      "['menopause'] ['menopause']\n",
      "['breast quadrant'] ['breast quad', 'breastquad']\n",
      "['magnesium'] ['magnesium']\n",
      "['total phenols', 'total phenol'] ['total phenols', 'totalphenols', 'totalphenol']\n",
      "['color intensity'] ['color intensity', 'colorintensity']\n",
      "['alcohol'] ['alcohol']\n",
      "['proline'] ['proline']\n",
      "['class'] ['class']\n",
      "['engine location'] ['engine location', 'enginelocation']\n",
      "['stroke'] ['stroke']\n",
      "['engine type'] ['engine type', 'enginetype']\n",
      "['body style'] ['body style', 'bodystyle']\n",
      "['drive wheels', 'drive wheel'] ['drive wheels', 'drivewheels', 'drivewheel']\n",
      "['peak rpm'] ['peak rpm', 'peakrpm']\n",
      "['aspiration'] ['aspiration']\n",
      "['engine size'] ['engine size', 'enginesize']\n",
      "['width'] ['width']\n",
      "['compression ratio'] ['compression ratio', 'compressionratio']\n",
      "['fuel type'] ['fuel type', 'fueltype']\n",
      "['wheel base'] ['wheel base', 'wheelbase']\n",
      "['length'] ['length']\n",
      "['horsepower'] ['horsepower']\n",
      "['height'] ['height']\n",
      "['price'] ['price']\n",
      "['fuel system'] ['fuel system', 'fuelsystem']\n",
      "['curb weight'] ['curb weight', 'curbweight']\n",
      "['decision count'] ['decision count', 'decisioncount']\n",
      "['call pairs', 'call pair'] ['call pairs', 'callpairs', 'callpair']\n",
      "['unique operators', 'unique operator'] ['unique operands', 'uniqueoperands', 'uniqueoperand']\n",
      "['branch count'] ['branch count', 'branchcount']\n",
      "['defects', 'defect'] ['defects', 'defect']\n",
      "['decision density'] ['decision density', 'decisiondensity']\n",
      "['total operators', 'total operator'] ['total operands', 'totaloperands', 'totaloperand']\n",
      "['multiple condition count'] ['multiple condition count', 'multipleconditioncount']\n",
      "['condition count'] ['condition count', 'conditioncount']\n",
      "['design complexity'] ['design complexity', 'designcomplexity']\n",
      "['formal parameters', 'formal parameter'] ['formal parameters', 'formalparameters', 'formalparameter']\n",
      "['total operands', 'total operand'] ['total operators', 'totaloperators', 'totaloperator']\n",
      "['unique operators', 'unique operator'] ['unique operators', 'uniqueoperators', 'uniqueoperator']\n",
      "['design density'] ['design density', 'designdensity']\n",
      "['third surgery'] ['thyroid surgery', 'thyroidsurgery']\n",
      "['tumor'] ['tumor']\n",
      "['query hypothesis'] ['query hypothyroid', 'queryhypothyroid']\n",
      "['pregnant'] ['pregnant']\n",
      "['tsh measurement'] ['tsh measured', 'tshmeasured', 'tshmeasure']\n",
      "['glutaric acid'] ['goitre']\n",
      "['lithium'] ['lithium']\n",
      "['hippotherapy'] ['hypopituitary']\n",
      "['referral source'] ['referral source', 'referralsource']\n",
      "['classification'] ['class']\n",
      "['cap shape'] ['cap shape', 'capshape']\n",
      "['stalk surface below ring'] ['stalk surface below ring', 'stalksurfacebelowring', 'stalksurfacebelowre']\n",
      "['gill color'] ['gill color', 'gillcolor']\n",
      "['spore print color'] ['spore print color', 'sporeprintcolor']\n",
      "['cap color'] ['cap color', 'capcolor']\n",
      "['ring number'] ['ring number', 'ringnumber']\n",
      "['stalk root'] ['stalk root', 'stalkroot']\n",
      "['ring type'] ['ring type', 'ringtype']\n",
      "['class'] ['class']\n",
      "['stalk shape'] ['stalk shape', 'stalkshape']\n",
      "['stalk color above ring'] ['stalk color above ring', 'stalkcolorabovering', 'stalkcolorabovere']\n",
      "['population'] ['population']\n",
      "['gill size'] ['gill size', 'gillsize']\n",
      "['habitat'] ['habitat']\n",
      "['veil type'] ['veil type', 'veiltype']\n",
      "['stalk surface above ring'] ['stalk surface above ring', 'stalksurfaceabovering', 'stalksurfaceabovere']\n",
      "['gill spacing', 'gill space'] ['gill spacing', 'gillspacing', 'gillspace']\n",
      "['gill attachment'] ['gill attachment', 'gillattachment']\n",
      "['stalk color below ring'] ['stalk color below ring', 'stalkcolorbelowring', 'stalkcolorbelowre']\n",
      "['veil color'] ['veil color', 'veilcolor']\n",
      "['cap surface'] ['cap surface', 'capsurface']\n",
      "['region center column'] ['region centroid col', 'regioncentroidcol']\n",
      "['intensity mean'] ['intensity mean', 'intensitymean']\n",
      "['hue mean'] ['hue mean', 'huemean']\n",
      "['saturation mean'] ['saturation mean', 'saturationmean']\n",
      "['short line density 5'] ['short line density 5', 'shortlinedensity5']\n",
      "['hedge mean'] ['hedge mean', 'hedgemean']\n",
      "['short line density 2'] ['short line density 2', 'shortlinedensity2']\n",
      "['region center row'] ['region centroid row', 'regioncentroidrow']\n",
      "['class'] ['class']\n",
      "['value mean'] ['value mean', 'valuemean']\n",
      "['raw blue mean'] ['rawblue mean', 'rawbluemean']\n",
      "['raw green mean'] ['rawgreen mean', 'rawgreenmean']\n",
      "['region pixel count'] ['region pixel count', 'regionpixelcount']\n",
      "['class'] ['class']\n",
      "['history'] ['histology']\n",
      "['ascites', 'ascite'] ['ascites', 'ascite']\n",
      "['airborne'] ['antivirals', 'antiviral']\n",
      "['spermidine'] ['spiders', 'spider']\n",
      "['bilirubin'] ['bilirubin']\n",
      "['liver firm'] ['liver firm', 'liverfirm']\n",
      "['malignant ascites', 'malignant ascite'] ['malaise']\n",
      "['varices', 'varix'] ['varices', 'varix']\n",
      "['albumin'] ['albumin']\n",
      "['spleen palpable'] ['spleen palpable', 'spleenpalpable']\n",
      "['fibrinogen'] ['fatigue']\n",
      "['arterial blood pressure'] ['anorexia']\n",
      "['sodium iodide'] ['steroid']\n",
      "['class'] ['class']\n",
      "['bottom right square'] ['bottom right square', 'bottomrightsquare']\n",
      "['middle left square', 'middle leave square'] ['middle left square', 'middleleftsquare']\n",
      "['middle middle square'] ['middle middle square', 'middlemiddlesquare']\n",
      "['middle right square'] ['middle right square', 'middlerightsquare']\n",
      "['top middle square'] ['top middle square', 'topmiddlesquare']\n",
      "['bottom left square', 'bottom leave square'] ['bottom left square', 'bottomleftsquare']\n",
      "['top left square', 'top leave square'] ['top left square', 'topleftsquare']\n",
      "['top right square'] ['top right square', 'toprightsquare']\n",
      "['bottom middle square'] ['bottom middle square', 'bottommiddlesquare']\n",
      "['sulphates', 'sulphate'] ['sulphates', 'sulphate']\n",
      "['free sulfur dioxide'] ['free sulfur dioxide', 'freesulfurdioxide']\n",
      "['chlorides', 'chloride'] ['chlorides', 'chloride']\n",
      "['residual sugar'] ['residual sugar', 'residualsugar']\n",
      "['density'] ['density']\n",
      "['citric acid'] ['citric acid', 'citricacid']\n",
      "['alcohol'] ['alcohol']\n",
      "['volatile acidity'] ['volatile acidity', 'volatileacidity']\n",
      "['class'] ['class']\n",
      "['total sulfur dioxide'] ['total sulfur dioxide', 'totalsulfurdioxide']\n",
      "['fixed acidity', 'fix acidity'] ['fixed acidity', 'fixedacidity']\n",
      "['looks', 'look'] ['looks', 'look']\n",
      "['grade'] ['grade']\n",
      "['urbanrural'] ['urbanrural']\n",
      "['grades', 'grade'] ['grades', 'grade']\n",
      "['gender'] ['gender']\n",
      "['goals', 'goal'] ['goals', 'goal']\n",
      "['sports', 'sport'] ['sports', 'sport']\n",
      "['school'] ['school']\n",
      "['money'] ['money']\n",
      "['position'] ['position']\n",
      "['strikes', 'strike'] ['strikeouts', 'strikeout']\n",
      "['walks', 'walk'] ['walks', 'walk']\n",
      "['games played', 'game play'] ['games played', 'gamesplayed', 'gamesplaye']\n",
      "['hall of fame'] ['hall of fame', 'halloffame']\n",
      "['doubles', 'double'] ['doubles', 'double']\n",
      "['at bats', 'at bat'] ['at bats', 'atbats', 'atbat']\n",
      "['number of seasons', 'number of season'] ['number seasons', 'numberseasons', 'numberseason']\n",
      "['triples', 'triple'] ['triples', 'triple']\n",
      "['batting average'] ['batting average', 'battingaverage']\n",
      "['media', 'medium'] ['mediastinum']\n",
      "['peri'] ['pleura']\n",
      "['bone marrow'] ['bone marrow', 'bonemarrow']\n",
      "['brain'] ['brain']\n",
      "['liver'] ['liver']\n",
      "['abdomen'] ['abdominal']\n",
      "['peritoneum'] ['peritoneum']\n",
      "['class'] ['class']\n",
      "['histologic type'] ['histologic type', 'histologictype']\n",
      "['attribute 1'] ['attribute 1', 'attribute1']\n",
      "['attribute 2'] ['attribute 2', 'attribute2']\n",
      "['attribute 7'] ['attribute 7', 'attribute7']\n",
      "['attribute 9'] ['attribute 9', 'attribute9']\n",
      "['attribute 8'] ['attribute 8', 'attribute8']\n",
      "['attribute 3'] ['attribute 3', 'attribute3']\n",
      "['attribute 5'] ['attribute 5', 'attribute5']\n",
      "['classification'] ['class']\n",
      "['attribute 4'] ['attribute 4', 'attribute4']\n",
      "['attribute 6'] ['attribute 6', 'attribute6']\n",
      "['class'] ['class']\n",
      "['ask opening price'] ['ask open', 'askopen']\n",
      "['bid opening price'] ['bid open', 'bidopen']\n",
      "['bid highest price', 'bid high price'] ['bid high', 'bidhigh']\n",
      "['ask volume'] ['ask volume', 'askvolume']\n",
      "['bid closing price'] ['bid close', 'bidclose']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bid volume'] ['bid volume', 'bidvolume']\n",
      "['ask closing price'] ['ask close', 'askclose']\n",
      "['ask highest price', 'ask high price'] ['ask high', 'askhigh']\n",
      "['total number of level calls', 'total number of level call'] ['total eve calls', 'totalevecalls', 'totalevecall']\n",
      "['total day calls', 'total day call'] ['total day calls', 'totaldaycalls', 'totaldaycall']\n",
      "['total day charges', 'total day charge'] ['total day charge', 'totaldaycharge']\n",
      "['area code'] ['area code', 'areacode']\n",
      "['total night calls', 'total night call'] ['total night calls', 'totalnightcalls', 'totalnightcall']\n",
      "['class'] ['class']\n",
      "['account length'] ['account length', 'accountlength']\n",
      "['state'] ['state']\n",
      "['total night minutes', 'total night minute'] ['total night minutes', 'totalnightminutes', 'totalnightminute']\n",
      "['phone number'] ['phone number', 'phonenumber']\n",
      "['international plan'] ['international plan', 'internationalplan']\n",
      "['total day minutes', 'total day minute'] ['total day minutes', 'totaldayminutes', 'totaldayminute']\n",
      "['total evening minutes', 'total evening minute'] ['total eve minutes', 'totaleveminutes', 'totaleveminute']\n",
      "['number of customer service calls', 'number of customer service call'] ['number customer service calls', 'numbercustomerservicecalls', 'numbercustomerservicecall']\n",
      "['voicemail plan'] ['voice mail plan', 'voicemailplan']\n",
      "['total night charges', 'total night charge'] ['total night charge', 'totalnightcharge']\n",
      "['total evening charges', 'total evening charge'] ['total eve charge', 'totalevecharge']\n",
      "['max length rectangularity'] ['maxlength rectangularity', 'maxlengthrectangularity']\n",
      "['hows rati', 'how s rati'] ['hollows ratio', 'hollowsratio']\n",
      "['class'] ['class']\n",
      "['sae ratio'] ['scatter ratio', 'scatterratio']\n",
      "['skew about major'] ['skewness about major', 'skewnessaboutmajor']\n",
      "['scal vari major'] ['scaled variance major', 'scaledvariancemajor']\n",
      "['scl d radiof gyration'] ['scaled radius of gyration', 'scaledradiusofgyration']\n",
      "['max leng aspect ratio'] ['maxlength aspect ratio', 'maxlengthaspectratio']\n",
      "['elonga'] ['elongatedness']\n",
      "['skewness about minor'] ['skewness about minor', 'skewnessaboutminor']\n",
      "['sled variance mino', 'sle variance mino'] ['scaled variance minor', 'scaledvarianceminor']\n",
      "['radi rato'] ['radius ratio', 'radiusratio']\n",
      "['distance circu'] ['distance circularity', 'distancecircularity']\n",
      "['cpcte'] ['compactness']\n",
      "['circu'] ['circularity']\n",
      "['shift differential'] ['shift differential', 'shiftdifferential']\n",
      "['statutory holidays', 'statutory holiday'] ['statutory holidays', 'statutoryholidays', 'statutoryholiday']\n",
      "['vacation'] ['vacation']\n",
      "['duration'] ['duration']\n",
      "['bereavement assistance'] ['bereavement assistance', 'bereavementassistance']\n",
      "['standby pay'] ['standby pay', 'standbypay']\n",
      "['class'] ['class']\n",
      "['contribution to dental plan'] ['contribution to dental plan', 'contributiontodentalplan']\n",
      "['pension'] ['pension']\n",
      "['contribution to health plan'] ['contribution to health plan', 'contributiontohealthplan']\n",
      "['long term disability assistance', 'longterm disability assistance'] ['longterm disability assistance', 'longtermdisabilityassistance']\n",
      "['cost of living adjustment', 'cost of live adjustment'] ['cost of living adjustment', 'costoflivingadjustment']\n",
      "['working hours', 'work hour'] ['working hours', 'workinghours', 'workinghour']\n",
      "['wage increase third year'] ['wage increase third year', 'wageincreasethirdyear']\n",
      "['education allowance'] ['education allowance', 'educationallowance']\n",
      "['wage increase first year'] ['wage increase first year', 'wageincreasefirstyear']\n",
      "['wage increase second year'] ['wage increase second year', 'wageincreasesecondyear']\n",
      "['exclusion of no'] ['exclusion of no', 'exclusionofno']\n",
      "['special forms', 'special form'] ['special forms', 'specialforms', 'specialform']\n",
      "['dislocation of'] ['dislocation of', 'dislocationof']\n",
      "['regeneration of'] ['regeneration of', 'regenerationof']\n",
      "['bypass'] ['by pass', 'bypass']\n",
      "['class'] ['class']\n",
      "['wifes education', 'wife education'] ['wifes education', 'wifeseducation']\n",
      "['number of children ever born', 'number of child ever bear'] ['number of children ever born', 'numberofchildreneverborn']\n",
      "['contraceptive method used', 'contraceptive method use'] ['contraceptive method used', 'contraceptivemethodused', 'contraceptivemethoduse']\n",
      "['standard of living index', 'standardofliving index', 'standardoflive index'] ['standard of living index', 'standardoflivingindex']\n",
      "['wifes religion', 'wife religion'] ['wifes religion', 'wifesreligion']\n",
      "['husbands occupation', 'husband occupation'] ['husbands occupation', 'husbandsoccupation']\n",
      "['husbands education', 'husband education'] ['husbands education', 'husbandseducation']\n",
      "['media exposure', 'medium exposure'] ['media exposure', 'mediaexposure']\n",
      "['wifes age', 'wife age'] ['wifes age', 'wifesage']\n",
      "['class'] ['class']\n",
      "['node caps', 'node cap'] ['node caps', 'nodecaps', 'nodecap']\n",
      "['tumor size'] ['tumor size', 'tumorsize']\n",
      "['breast'] ['breast']\n",
      "['menopause'] ['menopause']\n",
      "['breast quadrant'] ['breast quad', 'breastquad']\n",
      "['magnesium'] ['magnesium']\n",
      "['total phenols', 'total phenol'] ['total phenols', 'totalphenols', 'totalphenol']\n",
      "['color intensity'] ['color intensity', 'colorintensity']\n",
      "['alcohol'] ['alcohol']\n",
      "['proanthocyanins', 'proanthocyanin'] ['proline']\n",
      "['class'] ['class']\n",
      "['engine location'] ['engine location', 'enginelocation']\n",
      "['stroke'] ['stroke']\n",
      "['engine type'] ['engine type', 'enginetype']\n",
      "['body style'] ['body style', 'bodystyle']\n",
      "['drive wheels', 'drive wheel'] ['drive wheels', 'drivewheels', 'drivewheel']\n",
      "['peak rpm'] ['peak rpm', 'peakrpm']\n",
      "['aspiration'] ['aspiration']\n",
      "['engine size'] ['engine size', 'enginesize']\n",
      "['width'] ['width']\n",
      "['compression ratio'] ['compression ratio', 'compressionratio']\n",
      "['fuel type'] ['fuel type', 'fueltype']\n",
      "['wheel base'] ['wheel base', 'wheelbase']\n",
      "['length'] ['length']\n",
      "['horsepower'] ['horsepower']\n",
      "['height'] ['height']\n",
      "['price'] ['price']\n",
      "['fuel system'] ['fuel system', 'fuelsystem']\n",
      "['curb weight'] ['curb weight', 'curbweight']\n",
      "['decision count'] ['decision count', 'decisioncount']\n",
      "['call pairs', 'call pair'] ['call pairs', 'callpairs', 'callpair']\n",
      "['unique operators', 'unique operator'] ['unique operands', 'uniqueoperands', 'uniqueoperand']\n",
      "['branch count'] ['branch count', 'branchcount']\n",
      "['defects', 'defect'] ['defects', 'defect']\n",
      "['decision density'] ['decision density', 'decisiondensity']\n",
      "['total operators', 'total operator'] ['total operands', 'totaloperands', 'totaloperand']\n",
      "['multiple condition count'] ['multiple condition count', 'multipleconditioncount']\n",
      "['condition count'] ['condition count', 'conditioncount']\n",
      "['design complexity'] ['design complexity', 'designcomplexity']\n",
      "['formal parameters', 'formal parameter'] ['formal parameters', 'formalparameters', 'formalparameter']\n",
      "['total operands', 'total operand'] ['total operators', 'totaloperators', 'totaloperator']\n",
      "['unique operators', 'unique operator'] ['unique operators', 'uniqueoperators', 'uniqueoperator']\n",
      "['design density'] ['design density', 'designdensity']\n",
      "['third surgeon'] ['thyroid surgery', 'thyroidsurgery']\n",
      "['tumor'] ['tumor']\n",
      "['query hypothesis'] ['query hypothyroid', 'queryhypothyroid']\n",
      "['pregnant'] ['pregnant']\n",
      "['tsh measurement'] ['tsh measured', 'tshmeasured', 'tshmeasure']\n",
      "['glitter'] ['goitre']\n",
      "['lithium'] ['lithium']\n",
      "['hypertrophy'] ['hypopituitary']\n",
      "['referral source'] ['referral source', 'referralsource']\n",
      "['classification'] ['class']\n",
      "['cap shape'] ['cap shape', 'capshape']\n",
      "['stalk surface below ring'] ['stalk surface below ring', 'stalksurfacebelowring', 'stalksurfacebelowre']\n",
      "['gill color'] ['gill color', 'gillcolor']\n",
      "['spore print color'] ['spore print color', 'sporeprintcolor']\n",
      "['cap color'] ['cap color', 'capcolor']\n",
      "['ring number'] ['ring number', 'ringnumber']\n",
      "['stalk root'] ['stalk root', 'stalkroot']\n",
      "['ring type'] ['ring type', 'ringtype']\n",
      "['class'] ['class']\n",
      "['stalk shape'] ['stalk shape', 'stalkshape']\n",
      "['stalk color above ring'] ['stalk color above ring', 'stalkcolorabovering', 'stalkcolorabovere']\n",
      "['population'] ['population']\n",
      "['gill size'] ['gill size', 'gillsize']\n",
      "['habitat'] ['habitat']\n",
      "['veil type'] ['veil type', 'veiltype']\n",
      "['stalk surface above ring'] ['stalk surface above ring', 'stalksurfaceabovering', 'stalksurfaceabovere']\n",
      "['gill spacing', 'gill space'] ['gill spacing', 'gillspacing', 'gillspace']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gill attachment'] ['gill attachment', 'gillattachment']\n",
      "['stalk color below ring'] ['stalk color below ring', 'stalkcolorbelowring', 'stalkcolorbelowre']\n",
      "['veil color'] ['veil color', 'veilcolor']\n",
      "['cap surface'] ['cap surface', 'capsurface']\n",
      "['region center column'] ['region centroid col', 'regioncentroidcol']\n",
      "['intensity mean'] ['intensity mean', 'intensitymean']\n",
      "['hue mean'] ['hue mean', 'huemean']\n",
      "['saturation mean'] ['saturation mean', 'saturationmean']\n",
      "['short line density 5'] ['short line density 5', 'shortlinedensity5']\n",
      "['hedge mean'] ['hedge mean', 'hedgemean']\n",
      "['short line density 2'] ['short line density 2', 'shortlinedensity2']\n",
      "['region center row'] ['region centroid row', 'regioncentroidrow']\n",
      "['class'] ['class']\n",
      "['value mean'] ['value mean', 'valuemean']\n",
      "['raw blue mean'] ['rawblue mean', 'rawbluemean']\n",
      "['raw green mean'] ['rawgreen mean', 'rawgreenmean']\n",
      "['region pixel count'] ['region pixel count', 'regionpixelcount']\n",
      "['class'] ['class']\n",
      "['history'] ['histology']\n",
      "['ascites', 'ascite'] ['ascites', 'ascite']\n",
      "['airborne'] ['antivirals', 'antiviral']\n",
      "['spermidine'] ['spiders', 'spider']\n",
      "['bilirubin'] ['bilirubin']\n",
      "['liver firm'] ['liver firm', 'liverfirm']\n",
      "['malignant ascites', 'malignant ascite'] ['malaise']\n",
      "['varices', 'varix'] ['varices', 'varix']\n",
      "['albumin'] ['albumin']\n",
      "['spleen palpable'] ['spleen palpable', 'spleenpalpable']\n",
      "['fibrinogen'] ['fatigue']\n",
      "['arterial oxygen'] ['anorexia']\n",
      "['sodium index'] ['steroid']\n",
      "['class'] ['class']\n",
      "['bottom right square'] ['bottom right square', 'bottomrightsquare']\n",
      "['middle left square', 'middle leave square'] ['middle left square', 'middleleftsquare']\n",
      "['middle middle square'] ['middle middle square', 'middlemiddlesquare']\n",
      "['middle right square'] ['middle right square', 'middlerightsquare']\n",
      "['top middle square'] ['top middle square', 'topmiddlesquare']\n",
      "['bottom left square', 'bottom leave square'] ['bottom left square', 'bottomleftsquare']\n",
      "['top left square', 'top leave square'] ['top left square', 'topleftsquare']\n",
      "['top right square'] ['top right square', 'toprightsquare']\n",
      "['bottom middle square'] ['bottom middle square', 'bottommiddlesquare']\n",
      "['sulphates', 'sulphate'] ['sulphates', 'sulphate']\n",
      "['free sulfur dioxide'] ['free sulfur dioxide', 'freesulfurdioxide']\n",
      "['chlorides', 'chloride'] ['chlorides', 'chloride']\n",
      "['residual sugar'] ['residual sugar', 'residualsugar']\n",
      "['density'] ['density']\n",
      "['citric acid'] ['citric acid', 'citricacid']\n",
      "['alcohol'] ['alcohol']\n",
      "['volatile acidity'] ['volatile acidity', 'volatileacidity']\n",
      "['class'] ['class']\n",
      "['total sulfur dioxide'] ['total sulfur dioxide', 'totalsulfurdioxide']\n",
      "['fixed acidity', 'fix acidity'] ['fixed acidity', 'fixedacidity']\n",
      "['looks', 'look'] ['looks', 'look']\n",
      "['grade'] ['grade']\n",
      "['urbanrural'] ['urbanrural']\n",
      "['grades', 'grade'] ['grades', 'grade']\n",
      "['gender'] ['gender']\n",
      "['goals', 'goal'] ['goals', 'goal']\n",
      "['sports', 'sport'] ['sports', 'sport']\n",
      "['school'] ['school']\n",
      "['money'] ['money']\n",
      "['position'] ['position']\n",
      "['strikes', 'strike'] ['strikeouts', 'strikeout']\n",
      "['walks', 'walk'] ['walks', 'walk']\n",
      "['games played', 'game play'] ['games played', 'gamesplayed', 'gamesplaye']\n",
      "['hall of fame'] ['hall of fame', 'halloffame']\n",
      "['doubles', 'double'] ['doubles', 'double']\n",
      "['at bats', 'at bat'] ['at bats', 'atbats', 'atbat']\n",
      "['number of seasons', 'number of season'] ['number seasons', 'numberseasons', 'numberseason']\n",
      "['triples', 'triple'] ['triples', 'triple']\n",
      "['batting average'] ['batting average', 'battingaverage']\n",
      "['media', 'medium'] ['mediastinum']\n",
      "['peri'] ['pleura']\n",
      "['bone marrow'] ['bone marrow', 'bonemarrow']\n",
      "['brain'] ['brain']\n",
      "['liver'] ['liver']\n",
      "['abdomen'] ['abdominal']\n",
      "['peritoneum'] ['peritoneum']\n",
      "['class'] ['class']\n",
      "['histologic type'] ['histologic type', 'histologictype']\n",
      "['attribute 1'] ['attribute 1', 'attribute1']\n",
      "['attribute 2'] ['attribute 2', 'attribute2']\n",
      "['attribute 7'] ['attribute 7', 'attribute7']\n",
      "['attribute 9'] ['attribute 9', 'attribute9']\n",
      "['attribute 8'] ['attribute 8', 'attribute8']\n",
      "['attribute 3'] ['attribute 3', 'attribute3']\n",
      "['attribute 5'] ['attribute 5', 'attribute5']\n",
      "['classification'] ['class']\n",
      "['attribute 4'] ['attribute 4', 'attribute4']\n",
      "['attribute 6'] ['attribute 6', 'attribute6']\n",
      "['class'] ['class']\n",
      "['ask opening price'] ['ask open', 'askopen']\n",
      "['bid opening price'] ['bid open', 'bidopen']\n",
      "['bid high price'] ['bid high', 'bidhigh']\n",
      "['ask volume'] ['ask volume', 'askvolume']\n",
      "['bid closing price'] ['bid close', 'bidclose']\n",
      "['bid volume'] ['bid volume', 'bidvolume']\n",
      "['ask closing price'] ['ask close', 'askclose']\n",
      "['ask high price'] ['ask high', 'askhigh']\n",
      "['total number of calls', 'total number of call'] ['total eve calls', 'totalevecalls', 'totalevecall']\n",
      "['total day calls', 'total day call'] ['total day calls', 'totaldaycalls', 'totaldaycall']\n",
      "['total day charges', 'total day charge'] ['total day charge', 'totaldaycharge']\n",
      "['area code'] ['area code', 'areacode']\n",
      "['total night calls', 'total night call'] ['total night calls', 'totalnightcalls', 'totalnightcall']\n",
      "['class'] ['class']\n",
      "['account length'] ['account length', 'accountlength']\n",
      "['state'] ['state']\n",
      "['total night minutes', 'total night minute'] ['total night minutes', 'totalnightminutes', 'totalnightminute']\n",
      "['phone number'] ['phone number', 'phonenumber']\n",
      "['international plan'] ['international plan', 'internationalplan']\n",
      "['total day minutes', 'total day minute'] ['total day minutes', 'totaldayminutes', 'totaldayminute']\n",
      "['total evening minutes', 'total evening minute'] ['total eve minutes', 'totaleveminutes', 'totaleveminute']\n",
      "['number of customer service calls', 'number of customer service call'] ['number customer service calls', 'numbercustomerservicecalls', 'numbercustomerservicecall']\n",
      "['voicemail plan'] ['voice mail plan', 'voicemailplan']\n",
      "['total night charges', 'total night charge'] ['total night charge', 'totalnightcharge']\n",
      "['total evening charges', 'total evening charge'] ['total eve charge', 'totalevecharge']\n",
      "['maximum length rectangularity'] ['maxlength rectangularity', 'maxlengthrectangularity']\n",
      "['scatter ratio'] ['hollows ratio', 'hollowsratio']\n",
      "['class'] ['class']\n",
      "['scatter ratio'] ['scatter ratio', 'scatterratio']\n",
      "['skewness about major'] ['skewness about major', 'skewnessaboutmajor']\n",
      "['scaled variance major', 'scale variance major'] ['scaled variance major', 'scaledvariancemajor']\n",
      "['scaled radius of gyration', 'scale radius of gyration'] ['scaled radius of gyration', 'scaledradiusofgyration']\n",
      "['max length aspect ratio'] ['maxlength aspect ratio', 'maxlengthaspectratio']\n",
      "['elongation'] ['elongatedness']\n",
      "['skewness about minor'] ['skewness about minor', 'skewnessaboutminor']\n",
      "['scaled variance minor', 'scale variance minor'] ['scaled variance minor', 'scaledvarianceminor']\n",
      "['radius ratio'] ['radius ratio', 'radiusratio']\n",
      "['distance circularity'] ['distance circularity', 'distancecircularity']\n",
      "['cpcte'] ['compactness']\n",
      "['circularity'] ['circularity']\n",
      "['shift differential'] ['shift differential', 'shiftdifferential']\n",
      "['statutory holidays', 'statutory holiday'] ['statutory holidays', 'statutoryholidays', 'statutoryholiday']\n",
      "['vacation'] ['vacation']\n",
      "['duration'] ['duration']\n",
      "['bereavement assistance'] ['bereavement assistance', 'bereavementassistance']\n",
      "['standby pay'] ['standby pay', 'standbypay']\n",
      "['class'] ['class']\n",
      "['contribution to dental plan'] ['contribution to dental plan', 'contributiontodentalplan']\n",
      "['pension'] ['pension']\n",
      "['contribution to health plan'] ['contribution to health plan', 'contributiontohealthplan']\n",
      "['long term disability assistance', 'longterm disability assistance'] ['longterm disability assistance', 'longtermdisabilityassistance']\n",
      "['cost of living adjustment', 'cost of live adjustment'] ['cost of living adjustment', 'costoflivingadjustment']\n",
      "['working hours', 'work hour'] ['working hours', 'workinghours', 'workinghour']\n",
      "['wage increases third year', 'wage increase third year'] ['wage increase third year', 'wageincreasethirdyear']\n",
      "['education allowance'] ['education allowance', 'educationallowance']\n",
      "['wage increases first year', 'wage increase first year'] ['wage increase first year', 'wageincreasefirstyear']\n",
      "['wage increases second year', 'wage increase second year'] ['wage increase second year', 'wageincreasesecondyear']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exclusion of no'] ['exclusion of no', 'exclusionofno']\n",
      "['special forms', 'special form'] ['special forms', 'specialforms', 'specialform']\n",
      "['dislocation of'] ['dislocation of', 'dislocationof']\n",
      "['regeneration of'] ['regeneration of', 'regenerationof']\n",
      "['bypass'] ['by pass', 'bypass']\n",
      "['class'] ['class']\n",
      "['wifes education', 'wife education'] ['wifes education', 'wifeseducation']\n",
      "['number of children ever born', 'number of child ever bear'] ['number of children ever born', 'numberofchildreneverborn']\n",
      "['contraceptive method used', 'contraceptive method use'] ['contraceptive method used', 'contraceptivemethodused', 'contraceptivemethoduse']\n",
      "['standard of living index', 'standardofliving index', 'standardoflive index'] ['standard of living index', 'standardoflivingindex']\n",
      "['wifes religion', 'wife religion'] ['wifes religion', 'wifesreligion']\n",
      "['husbands occupation', 'husband occupation'] ['husbands occupation', 'husbandsoccupation']\n",
      "['husbands education', 'husband education'] ['husbands education', 'husbandseducation']\n",
      "['media exposure', 'medium exposure'] ['media exposure', 'mediaexposure']\n",
      "['wifes age', 'wife age'] ['wifes age', 'wifesage']\n",
      "['class'] ['class']\n",
      "['node caps', 'node cap'] ['node caps', 'nodecaps', 'nodecap']\n",
      "['tumor size'] ['tumor size', 'tumorsize']\n",
      "['breast'] ['breast']\n",
      "['menopause'] ['menopause']\n",
      "['breast quadrant'] ['breast quad', 'breastquad']\n",
      "['magnesium'] ['magnesium']\n",
      "['total phenols', 'total phenol'] ['total phenols', 'totalphenols', 'totalphenol']\n",
      "['color intensity'] ['color intensity', 'colorintensity']\n",
      "['alcohol'] ['alcohol']\n",
      "['proanthocyanins', 'proanthocyanin'] ['proline']\n",
      "['class'] ['class']\n"
     ]
    }
   ],
   "source": [
    "def f1_scores_per_type(pred_dct, typ):\n",
    "    ''' Function to generate f1 and em scores per query type'''\n",
    "    f1_scores = []\n",
    "    em_scores = []\n",
    "    for key, value in pred_dct.items():\n",
    "        predictions_1_truth = value[0]['y']\n",
    "        predictions_1_0 = value[0][f'y_pred_{typ}']\n",
    "        for references, predictions in zip(predictions_1_truth, predictions_1_0):\n",
    "            refs = normalize_answer(references)\n",
    "            preds = normalize_answer(predictions)\n",
    "            f1_sub_scores = []\n",
    "            em_sub_scores = []\n",
    "            print(preds, refs)\n",
    "            for pred in preds:\n",
    "                for ref in refs:\n",
    "                    f1_sub_scores.append(bertscore.compute(predictions=[pred],references=[ref],model_type=\"distilbert-base-uncased\")['f1'][0])\n",
    "                    if pred == ref:\n",
    "                        em_sub_scores.append(1)\n",
    "                    else:\n",
    "                        em_sub_scores.append(0)\n",
    "            f1_scores.append(max(f1_sub_scores))\n",
    "            em_scores.append(max(em_sub_scores))\n",
    "    return f1_scores, em_scores\n",
    "f1_scores_baseline, em_scores_bs = f1_scores_per_type(pred_dct_title_desc, 'baseline')\n",
    "f1_scores_title, em_scores_tit = f1_scores_per_type(pred_dct_title_desc, 'title')\n",
    "f1_scores_desc, em_scores_desc = f1_scores_per_type(pred_dct_title_desc, 'desc')\n",
    "f1_scores_title_desc, em_scores_tit_desc = f1_scores_per_type(pred_dct_title_desc, 'title_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f3cd7446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9478237529595693, 0.962423782572787, 0.9572087732645181, 0.9645706275080004]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average f1 scores\n",
    "average_baseline = mean(f1_scores_baseline)\n",
    "average_title = mean(f1_scores_title)\n",
    "average_desc = mean(f1_scores_desc)\n",
    "average_title_desc = mean(f1_scores_title_desc)\n",
    "f1_scores = [average_baseline, average_title, average_desc, average_title_desc]\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ba7df2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89.65, 97.8, 878.1, 884.25]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average token lengths\n",
    "token_len_bs = []\n",
    "token_len_tit = []\n",
    "token_len_desc = []\n",
    "token_len_tit_desc = []\n",
    "\n",
    "for df,tokens in pred_dct_title_desc.values():\n",
    "    token_len_bs.append(tokens['token_length'][0])\n",
    "    token_len_tit.append(tokens['token_length'][1])\n",
    "    token_len_desc.append(tokens['token_length'][2])\n",
    "    token_len_tit_desc.append(tokens['token_length'][3])\n",
    "avg_tokens_bs = mean(token_len_bs)\n",
    "avg_tokens_tit = mean(token_len_tit)\n",
    "avg_tokens_desc = mean(token_len_desc)\n",
    "avg_tokens_tit_desc = mean(token_len_tit_desc)\n",
    "avg_tokens = [avg_tokens_bs, avg_tokens_tit, avg_tokens_desc, avg_tokens_tit_desc]\n",
    "avg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e34a1bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7393162393162394, 0.7692307692307693, 0.7777777777777778, 0.811965811965812]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average em scores\n",
    "em_scores = [mean(em_scores_bs), mean(em_scores_tit), mean(em_scores_desc), mean(em_scores_tit_desc)]\n",
    "em_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce44f0e",
   "metadata": {},
   "source": [
    "## Normalized F1 scores for experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5cdb8bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010572490272834013\n",
      "0.009840733973136883\n",
      "0.0010900908475851476\n",
      "0.0010908347497970035\n"
     ]
    }
   ],
   "source": [
    "for f1, token in zip(f1_scores, avg_tokens):\n",
    "    print(f1 / token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee51b2",
   "metadata": {},
   "source": [
    "## Normalized EM scores for experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86caa024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00824669536325978\n",
      "0.007865345288658173\n",
      "0.000885750800339116\n",
      "0.0009182536748270421\n"
     ]
    }
   ],
   "source": [
    "for em, token in zip(em_scores, avg_tokens):\n",
    "    print(em / token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
